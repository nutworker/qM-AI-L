{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nutworker/qM-AI-L/blob/L_test/RoBERTa_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repository_url = 'https://github.com/nutworker/qM-AI-L'\n",
        "!git clone {repository_url}"
      ],
      "metadata": {
        "id": "n0UfO9kv4-nq",
        "outputId": "c2c9f99f-0dbf-46ab-fbc5-17f4e86d2053",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'qM-AI-L'...\n",
            "remote: Enumerating objects: 175, done.\u001b[K\n",
            "remote: Counting objects: 100% (175/175), done.\u001b[K\n",
            "remote: Compressing objects: 100% (145/145), done.\u001b[K\n",
            "remote: Total 175 (delta 79), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (175/175), 7.94 MiB | 3.94 MiB/s, done.\n",
            "Resolving deltas: 100% (79/79), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ID0koCUtX33Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a169973-5d36-47d5-ce82-6bd980a85b27",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.4)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.15.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=2.0.0->evaluate)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Collecting requests>=2.19.0 (from evaluate)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
            "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.6.1\n",
            "    Uninstalling fsspec-2024.6.1:\n",
            "      Successfully uninstalled fsspec-2024.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.1+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.1+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "gcsfs 2024.6.1 requires fsspec==2024.6.1, but you have fsspec 2024.5.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 evaluate-0.4.2 fsspec-2024.5.0 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.4)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=01b60872b17c4468d106cbdb85c3bcddc18f812eb643d38389a07e10a6e92a24\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.5.15)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.25.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.10.1 sacrebleu-2.4.2\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install rouge-score # Installing rouge-score library (https://pypi.org/project/rouge-score/)\n",
        "\n",
        "# !pip install accelerate -U\n",
        "\n",
        "# !pip install transformers[torch]\n",
        "!pip install sacrebleu\n",
        "\n",
        "!pip install datasets nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd # Data Handling\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import seaborn as sns\n",
        "\n",
        "from datasets import load_metric\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "xuOoOaSEDLqg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "from transformers import pipeline                                         # Pipeline\n",
        "from transformers import DataCollatorForSeq2Seq                           # DataCollator to batch the data\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "tbXvEcauCHyq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "from nltk.translate.meteor_score import meteor_score"
      ],
      "metadata": {
        "id": "T7T6r-6fLsBo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**"
      ],
      "metadata": {
        "id": "As3cC5xSngQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df = pd.read_csv('/content/qM-AI-L/email-subject/model-tuning/train_emails.csv')"
      ],
      "metadata": {
        "id": "4nBEREYUQPjQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df"
      ],
      "metadata": {
        "id": "L9skosJrQc0t",
        "outputId": "015f9e0a-0514-45bf-d408-752755adfbd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    source                                               body  \\\n",
              "0         allen-p_inbox_20  Greg/Phillip,  Attached is the Grande Communic...   \n",
              "1         allen-p_inbox_28  Phillip & Keith  Attached is the first draw re...   \n",
              "2         allen-p_inbox_63  Your Internet Banking accounts are now setup a...   \n",
              "3         allen-p_inbox_64  To our IBS Customers that are still hanging in...   \n",
              "4         allen-p_inbox_65  Phillip Good Morning!\\nI hope you had a wonder...   \n",
              "...                    ...                                                ...   \n",
              "14431  zufferli-j_inbox_43  This email is acknowledgement from the Power P...   \n",
              "14432  zufferli-j_inbox_44  This email is acknowledgement from the Power P...   \n",
              "14433  zufferli-j_inbox_46  John,  Further to the voice message that I lef...   \n",
              "14434   zufferli-j_inbox_8  Make sure that all curves are downloaded by th...   \n",
              "14435   zufferli-j_inbox_9  John:  Do you need Accumap day one?\\nCarmen sa...   \n",
              "\n",
              "                       subject  ann0  ann1  ann2  body_wcount  subj_wcount  \\\n",
              "0            Service Agreement   NaN   NaN   NaN           65            2   \n",
              "1               Bishops Corner   NaN   NaN   NaN          145            2   \n",
              "2             Internet Banking   NaN   NaN   NaN          250            2   \n",
              "3             Internet Banking   NaN   NaN   NaN          458            2   \n",
              "4      SMEs for expert stories   NaN   NaN   NaN           68            4   \n",
              "...                        ...   ...   ...   ...          ...          ...   \n",
              "14431               Power Pool   NaN   NaN   NaN          227            2   \n",
              "14432    Power Pool of Alberta   NaN   NaN   NaN          277            4   \n",
              "14433           Enron Security   NaN   NaN   NaN          148            2   \n",
              "14434        Simulation Curves   NaN   NaN   NaN           66            2   \n",
              "14435              IHS Accumap   NaN   NaN   NaN           35            2   \n",
              "\n",
              "                                          cleaned_emails  \n",
              "0      gregphillip attached is the grande communicati...  \n",
              "1      phillip keith attached is the first draw reque...  \n",
              "2      your internet banking accounts are now setup a...  \n",
              "3      to our ibs customers that are still hanging in...  \n",
              "4      phillip good morning i hope you had a wonderfu...  \n",
              "...                                                  ...  \n",
              "14431  this email is acknowledgement from the power p...  \n",
              "14432  this email is acknowledgement from the power p...  \n",
              "14433  john further to the voice message that i left ...  \n",
              "14434  make sure that all curves are downloaded by th...  \n",
              "14435  john do you need accumap day one carmen said t...  \n",
              "\n",
              "[14436 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-234cd924-53e3-40de-a9fb-fa5c74e10960\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>body</th>\n",
              "      <th>subject</th>\n",
              "      <th>ann0</th>\n",
              "      <th>ann1</th>\n",
              "      <th>ann2</th>\n",
              "      <th>body_wcount</th>\n",
              "      <th>subj_wcount</th>\n",
              "      <th>cleaned_emails</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>allen-p_inbox_20</td>\n",
              "      <td>Greg/Phillip,  Attached is the Grande Communic...</td>\n",
              "      <td>Service Agreement</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>65</td>\n",
              "      <td>2</td>\n",
              "      <td>gregphillip attached is the grande communicati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>allen-p_inbox_28</td>\n",
              "      <td>Phillip &amp; Keith  Attached is the first draw re...</td>\n",
              "      <td>Bishops Corner</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>145</td>\n",
              "      <td>2</td>\n",
              "      <td>phillip keith attached is the first draw reque...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>allen-p_inbox_63</td>\n",
              "      <td>Your Internet Banking accounts are now setup a...</td>\n",
              "      <td>Internet Banking</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>250</td>\n",
              "      <td>2</td>\n",
              "      <td>your internet banking accounts are now setup a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>allen-p_inbox_64</td>\n",
              "      <td>To our IBS Customers that are still hanging in...</td>\n",
              "      <td>Internet Banking</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>458</td>\n",
              "      <td>2</td>\n",
              "      <td>to our ibs customers that are still hanging in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>allen-p_inbox_65</td>\n",
              "      <td>Phillip Good Morning!\\nI hope you had a wonder...</td>\n",
              "      <td>SMEs for expert stories</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>68</td>\n",
              "      <td>4</td>\n",
              "      <td>phillip good morning i hope you had a wonderfu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14431</th>\n",
              "      <td>zufferli-j_inbox_43</td>\n",
              "      <td>This email is acknowledgement from the Power P...</td>\n",
              "      <td>Power Pool</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>227</td>\n",
              "      <td>2</td>\n",
              "      <td>this email is acknowledgement from the power p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14432</th>\n",
              "      <td>zufferli-j_inbox_44</td>\n",
              "      <td>This email is acknowledgement from the Power P...</td>\n",
              "      <td>Power Pool of Alberta</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>277</td>\n",
              "      <td>4</td>\n",
              "      <td>this email is acknowledgement from the power p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14433</th>\n",
              "      <td>zufferli-j_inbox_46</td>\n",
              "      <td>John,  Further to the voice message that I lef...</td>\n",
              "      <td>Enron Security</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>148</td>\n",
              "      <td>2</td>\n",
              "      <td>john further to the voice message that i left ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14434</th>\n",
              "      <td>zufferli-j_inbox_8</td>\n",
              "      <td>Make sure that all curves are downloaded by th...</td>\n",
              "      <td>Simulation Curves</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>66</td>\n",
              "      <td>2</td>\n",
              "      <td>make sure that all curves are downloaded by th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14435</th>\n",
              "      <td>zufferli-j_inbox_9</td>\n",
              "      <td>John:  Do you need Accumap day one?\\nCarmen sa...</td>\n",
              "      <td>IHS Accumap</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35</td>\n",
              "      <td>2</td>\n",
              "      <td>john do you need accumap day one carmen said t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14436 rows × 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-234cd924-53e3-40de-a9fb-fa5c74e10960')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-234cd924-53e3-40de-a9fb-fa5c74e10960 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-234cd924-53e3-40de-a9fb-fa5c74e10960');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f3a13e01-2aef-4139-847c-62c89503357f\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f3a13e01-2aef-4139-847c-62c89503357f')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f3a13e01-2aef-4139-847c-62c89503357f button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_7b70f381-6970-4ed1-9ce9-9e39688637bc\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dataset_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7b70f381-6970-4ed1-9ce9-9e39688637bc button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dataset_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset_df",
              "summary": "{\n  \"name\": \"dataset_df\",\n  \"rows\": 14436,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14436,\n        \"samples\": [\n          \"sanders-r_sent_2329\",\n          \"buy-r_inbox_344\",\n          \"dorland-c_sent_301\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"body\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13523,\n        \"samples\": [\n          \"Sara: Here are drafts of the Certificates and Sight Drafts for Niagara Mohawk, El Paso electric, LG&E Energy Marketing and Energy Production Corp. (just in case it wasn't renewed).\\nI have put together files for each of these that include copies of the LC, the underlying trading docs and any correspondence that has been sent.\\nExcept for Niagara Mohawk which requires copies of the invoices and that the certificate be on \\\"letterhead\\\" there don't appear to be any other special requirements.\\nLeslie Reeves will be faxing the invoices to us.\\nI will be in tomorrow until around 11.\\nThanks for your help on this.\\nAll of these docs are in a file called Enron Restructuring.\",\n          \"Good afternoon,  When you get a chance, please let me know if you would like for me to order you a 2002 calendar.\\nIf so, which kind?\\nYou may want more than one type.\\nJust let me know and I will be more than happy to order what you want.\\nThanks!\",\n          \"Hey Hunter, A quick update.\\nKevin Brady is up to speed on working with Colin and Chris on consolidating critical and noncritical notices for all the regions.\\nI have also passed on the idea of an operations page on the website to the other logistics managers and the central desk schedulers to get their ideas and insights on how to make this page beneficial to both the traders and schedulers.\\nA couple of ideas that have already been passed my way are:  \\t1) Incorporating the Gas Daily and Index historicals against production/storage by pipe so that we could project cash outs (Mark Schrab) \\t2) A transportation rate matrix on the web site (Cora Pendergrass) \\t3) Enron operations personnel contact page with phone numbers and pictures.\\nMuch like the weather guys.\\nI laughed at first but Victor LaMadrid   \\t\\twhom suggested this said that the East Desk Traders don't even know all of his schedulers.\\n4) An easy access to the pipelines meters and dunns numbers.\\n(Kevin Brady) \\t5) Morning sheets updated with contract, constraint, and imbalance information from Sitara and Unify (Victor LaMadrid)  I have asked Kevin to consolidate all the ideas and then we can get together and decide which ones you want to place into production.\\nLisa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12032,\n        \"samples\": [\n          \"OMLX Application\",\n          \"Draft OF CA\",\n          \"Wharton trip\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ann0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ann1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ann2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"body_wcount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 148,\n        \"min\": 25,\n        \"max\": 3136,\n        \"num_unique_values\": 683,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subj_wcount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 15,\n        \"num_unique_values\": 15,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_emails\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13498,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test Train Split**"
      ],
      "metadata": {
        "id": "L6sRPzc6oU02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset_df = dataset_df.sample(frac=0.3, random_state=42)\n",
        "subset_df.shape"
      ],
      "metadata": {
        "id": "1lnDOKzv4QZb",
        "outputId": "73f673f2-f019-465a-9463-841574d63a52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4331, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Train_df, Test_df = train_test_split(subset_df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "guCjiJJyRLzM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_df.shape, Test_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgDx01BreHk",
        "outputId": "cee73b55-e8b9-4f7a-f55f-c75f5811f595"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3464, 9), (867, 9))"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Train_dataset = Dataset.from_pandas(Train_df)\n",
        "Test_dataset = Dataset.from_pandas(Test_df)"
      ],
      "metadata": {
        "id": "ZHaFrU6-oYQ5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_dataset, Test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfLahVPWoa1X",
        "outputId": "95180cf9-2b49-4def-dae2-23e760f6cb89"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['source', 'body', 'subject', 'ann0', 'ann1', 'ann2', 'body_wcount', 'subj_wcount', 'cleaned_emails', '__index_level_0__'],\n",
              "     num_rows: 3464\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['source', 'body', 'subject', 'ann0', 'ann1', 'ann2', 'body_wcount', 'subj_wcount', 'cleaned_emails', '__index_level_0__'],\n",
              "     num_rows: 867\n",
              " }))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a subset of Dataset**"
      ],
      "metadata": {
        "id": "ofsPKKg_nxk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #create a subset of dataset\n",
        "# train_dataset = Train_dataset.filter(lambda example, index: index % 2 == 0, with_indices=True)\n",
        "# test_dataset = Test_dataset.filter(lambda example, index: index % 2 == 0, with_indices=True)\n",
        "\n",
        "Train_dataset.shape, Test_dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iDTgW2UtjIT",
        "outputId": "1a741866-bbf6-4287-a420-b9a70df5c5d8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3464, 10), (867, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uWXvaTRtnE_",
        "outputId": "313fd621-6692-49d0-c553-04ebf35497e1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['source', 'body', 'subject', 'ann0', 'ann1', 'ann2', 'body_wcount', 'subj_wcount', 'cleaned_emails', '__index_level_0__'],\n",
              "    num_rows: 3464\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Train_dataset['cleaned_emails'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "nghT9hdrfoYJ",
        "outputId": "57f63c59-47c6-4c7a-b0ce-3e678e5756aa"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the following reports have been waiting for your approval for more than 4 days. please review. owner mark e koenig report name miscellaneous ir expenses boston trip days in mgr. queue 4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the pre-trained FLAN-T5 models and its tokenizer directly from HuggingFace. FLAN-T5 was released in the paper Scaling Instruction-Finetuned Language Models - it is an enhanced version of T5 that has been finetuned in a mixture of tasks. Setting torch_dtype=torch.bfloat16 specifies the memory type to be used by this model.**"
      ],
      "metadata": {
        "id": "qUwUyn2SowRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph_xA-Hrt83i",
        "outputId": "e666c7f0-890c-4a4d-c69e-fa124a0521f2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess function\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples['body']\n",
        "    labels = examples['subject']\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
        "\n",
        "    # Encode labels\n",
        "    labels = tokenizer(labels, max_length=128, truncation=True, padding='max_length')\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Tokenize the dataset\n",
        "tokenized_train_dataset = Train_dataset.map(preprocess_function, batched=True, remove_columns=Train_dataset.column_names)\n",
        "tokenized_eval_dataset = Test_dataset.map(preprocess_function, batched=True, remove_columns=Test_dataset.column_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "4c72c4faf85f445ba9d953ef921e89d1",
            "3408f36d59364a15b2168ca381788477",
            "7d415e0fadb54450baa7b145ad546fc8",
            "17fffb7161954a7a80c2a6e976cbd37f",
            "17a84654a0804cd19b35f7c87b9d2d3a",
            "73309f927c1f488fbfb1c7083fead481",
            "dbcbe7b9fb994b23b5bbeddf46adb63f",
            "452fdc2565984701923cf24635c2ad78",
            "d0c44351a15e42c5a90712ac27b4e05a",
            "ad06fa7e06dd43bf8d5f177df9d3adc9",
            "2b2331464a2e47fd99a8883a8bf6b7a0",
            "853b267b7d04441387c35afce7d48d83",
            "4a454ded720e4024a603a17ca75c4f07",
            "4f9f4e3314894567bf576d0a96b5fb3b",
            "62fcfe28d8de4633a48aab08286edb0a",
            "404696b41bea444596aace330a2abe70",
            "e7036af3d10c4667921b5fe539fd5ab1",
            "5c6040cddec14d78b90369762771cc98",
            "b13a9bbf5c544fefbb281b3e7194798d",
            "1dcfecbb12994f4b8550103f11e6a329",
            "e15eb89663d74eff925345ab2873aa7b",
            "0654c21295f3435f902218e858603e67"
          ]
        },
        "id": "NQNV0Hb6zDB8",
        "outputId": "d8deb876-5592-4395-c228-3d7b4ca72fb3"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3464 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c72c4faf85f445ba9d953ef921e89d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/867 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "853b267b7d04441387c35afce7d48d83"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_eval_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd3YRezAwJO9",
        "outputId": "f932ceda-063d-4492-fcdb-4790e8c8812e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 867\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir='./results',\n",
        "#     num_train_epochs=3,\n",
        "#     evaluation_strategy='epoch',\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=4,\n",
        "#     per_device_eval_batch_size=4,\n",
        "\n",
        "#     weight_decay=0.01,\n",
        "#     save_total_limit=3,\n",
        "#     remove_unused_columns=False,\n",
        "#     fp16=True\n",
        "# )\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    remove_unused_columns=False,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "x1tdeCuJtv--",
        "outputId": "bd97bda4-942c-4d96-f661-e6628aafb90b"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected input batch_size (4) to match target batch_size (512).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-5c32fcad76a7>\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1931\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1932\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1933\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1934\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2267\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3306\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3307\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3309\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3336\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3337\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3338\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3339\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3340\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1227\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"single_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproblem_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"multi_label_classification\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m                 \u001b[0mloss_fct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1186\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3084\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3086\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (4) to match target batch_size (512)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assuming you have input_tensor and target_tensor\n",
        "\n",
        "# Check shapes\n",
        "print(input_tensor.shape)  # Output: torch.Size([4, 128])\n",
        "print(target_tensor.shape)  # Output: torch.Size([512])"
      ],
      "metadata": {
        "id": "KdhN8En21Xdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abx"
      ],
      "metadata": {
        "id": "6kEoEz9xxarj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained('./roberta-email-subject-generator')\n",
        "tokenizer.save_pretrained('./roberta-email-subject-generator')"
      ],
      "metadata": {
        "id": "3-yGBPH_tv0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('./roberta-email-subject-generator')\n",
        "model = RobertaForSequenceClassification.from_pretrained('./roberta-email-subject-generator')\n",
        "\n",
        "# Create a text generation pipeline (This is an illustrative example, as RoBERTa is not typically used for generation)\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "def generate_subject(email):\n",
        "    inputs = tokenizer(email, return_tensors='pt', max_length=512, truncation=True)\n",
        "    outputs = generator(email, max_length=128, num_return_sequences=1)\n",
        "    subject = outputs[0]['generated_text']\n",
        "    return subject\n",
        "\n",
        "# Example usage\n",
        "email = \"Your email content here...\"\n",
        "subject = generate_subject(email)\n",
        "print(f\"Generated Subject: {subject}\")"
      ],
      "metadata": {
        "id": "Gv5JmM9puSIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get size of models\n",
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"Trainable model parameters: {trainable_model_params}\\nAll model parameters: {all_model_params}\\nPercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n"
      ],
      "metadata": {
        "id": "0fsHEX6Bm3an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Flan T5 Base Model by freezing starting layers**"
      ],
      "metadata": {
        "id": "GKeNgh0NpSaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Freeze the first 6 layers of the encoder and decoder\n",
        "for param in model.encoder.block[:6].parameters(): # Use 'block' instead of 'layers'\n",
        "    param.requires_grad = False\n",
        "for param in model.decoder.block[:6].parameters(): # Use 'block' instead of 'layers'\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(\"For Model : FLAN-T5-Base \\n\",print_number_of_trainable_model_parameters(model))"
      ],
      "metadata": {
        "id": "Xkz4NmaW0KPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\", device_map=\"auto\")\n",
        "\n",
        "print(\"For Model : FLAN-T5-Base \\n\",print_number_of_trainable_model_parameters(model))"
      ],
      "metadata": {
        "id": "tlWuYPwITCaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    inputs = [\"generate subject line: \" + example for example in examples[\"body\"]] # Access the 'body' column\n",
        "    targets = [example for example in examples[\"subject\"]] # Access the 'subject' column\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "2qUVhlTf07dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset = Train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_eval_dataset = Test_dataset.map(tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "id": "UPg_4Oau1Cem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_total_limit=2,\n",
        ")"
      ],
      "metadata": {
        "id": "qehbaqIMWE85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
        ")"
      ],
      "metadata": {
        "id": "mD7DzIFoWPSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "JWZkdpfEWovI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_bkl3jp3Uvvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For Model : FLAN-T5-Base \\n\",print_number_of_trainable_model_parameters(model))"
      ],
      "metadata": {
        "id": "qhDSbG6W2BZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset, tokenized_eval_dataset"
      ],
      "metadata": {
        "id": "yFSryaOrEKQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
        "\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "8prqmLk74DC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model_save_path = \"fine_tuned_flan_t5_LP\"\n",
        "model.save_pretrained(model_save_path)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(model_save_path)"
      ],
      "metadata": {
        "id": "FgN3Xw66_dmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and tokenizer\n",
        "Flan_T5_FT_LP_model = T5ForConditionalGeneration.from_pretrained(model_save_path)\n",
        "\n",
        "Flan_T5_model_FT_tokenizer = T5Tokenizer.from_pretrained(model_save_path)\n"
      ],
      "metadata": {
        "id": "Jg1XJBOZAbma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emails_for_score_df = Test_df.sample(n=20, random_state=32)\n",
        "emails_for_score_df"
      ],
      "metadata": {
        "id": "wtlJg_-YB2Fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_subjects = []\n",
        "flan_T5_subjects = []\n",
        "finetuned_flan_T5_subjects = []\n",
        "\n",
        "\n",
        "for index, row in emails_for_score_df.iterrows():\n",
        "    email = row['body']\n",
        "    original_subject = row['subject']\n",
        "    prompt = f\"\"\"\n",
        "    Generate the subject line for the following email.\n",
        "\n",
        "    Email:\n",
        "    {email}\n",
        "\n",
        "    Subject:\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the device (GPU if available, otherwise CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move both models to the same device\n",
        "    model.to(device)\n",
        "    Flan_T5_FT_LP_model.to(device)\n",
        "\n",
        "    original_model_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    finetuned_model_input_ids = Flan_T5_model_FT_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "\n",
        "    original_model_outputs = model.generate(input_ids=original_model_input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "    flan_T5_subjects.append(original_model_text_output)\n",
        "\n",
        "    instruct_model_outputs = Flan_T5_FT_LP_model.generate(input_ids=finetuned_model_input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    instruct_model_text_output = Flan_T5_model_FT_tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "    finetuned_flan_T5_subjects.append(instruct_model_text_output)\n",
        "\n",
        "    original_subjects.append(original_subject)\n",
        "\n",
        "zipped_subjects = list(zip(original_subjects, flan_T5_subjects, finetuned_flan_T5_subjects))\n",
        "\n",
        "Flan_T5_LP_df = pd.DataFrame(zipped_subjects, columns = ['original_subjects', 'T5_subjects', 'finetuned_flan_T5_subjects'])\n",
        "Flan_T5_LP_df"
      ],
      "metadata": {
        "id": "m45B3VOkBAvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=flan_T5_subjects,\n",
        "    references=original_subjects,\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "finetuned_model_results = rouge.compute(\n",
        "    predictions=finetuned_flan_T5_subjects,\n",
        "    references=original_subjects,\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('FLAN T5 Base Model:')\n",
        "print(original_model_results)\n",
        "print('\\nFINETUNED FLAN T5 BASE MODEL:')\n",
        "print(finetuned_model_results)"
      ],
      "metadata": {
        "id": "Lufz_SVBDS5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**flan_t5_base_original_model**"
      ],
      "metadata": {
        "id": "OnDeQ_KnAWai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flan_t5_base_original_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)\n",
        "flan_t5_base_tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "print(\"For Model : FLAN-T5-Base \\n\",print_number_of_trainable_model_parameters(flan_t5_base_original_model))"
      ],
      "metadata": {
        "id": "6ufBICyAosbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flan_t5_base_tokenizer.special_tokens_map"
      ],
      "metadata": {
        "id": "2-a_H6f06elE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Flan T5 Small Model**"
      ],
      "metadata": {
        "id": "JU6tvjt2pbJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flan_t5_small_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\", torch_dtype=torch.bfloat16)\n",
        "# flan_t5_small_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "print(\"For Model : Flan-t5-small \\n\",print_number_of_trainable_model_parameters(flan_t5_small_model))"
      ],
      "metadata": {
        "id": "SpTzBk9jEv_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flan-t5-base vs. flan-t5-small parameters\n",
        "247577856-76961152"
      ],
      "metadata": {
        "id": "iLGrsgJ2E35P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test the Model with Zero Shot Inferencing**"
      ],
      "metadata": {
        "id": "ilaYbjowrNVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using a pre-trained model from Hugging Face/sources - FLAN-T5-Base .**"
      ],
      "metadata": {
        "id": "IV3wSp9HRSxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 9\n",
        "\n",
        "email = Train_dataset['cleaned_emails'][index]\n",
        "subject = Train_dataset['subject'][index]\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Generate the subject line for the following email.\n",
        "\n",
        "Email:\n",
        "{email}\n",
        "\n",
        "Subject:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BqP21DaAMaiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inputs = flan_t5_base_tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "# Move input tensors to the same device as the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inputs = {k: v.to(flan_t5_base_original_model.device) for k, v in inputs.items()}\n",
        "\n",
        "\n",
        "output = flan_t5_base_tokenizer.decode(\n",
        "    flan_t5_base_original_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE ORIGINAL HUMAN ANNOTED SUBJECT LINE :\\n{subject}\\n')\n",
        "print(dash_line)\n",
        "print(f'FLAN-T5 MODEL GENERATION - ZERO SHOT SUBJECT LINE:\\n{output}')"
      ],
      "metadata": {
        "id": "YpNTTyqsvQS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using a pre-trained model from Hugging Face etc- FLAN-T5-SMALL .**"
      ],
      "metadata": {
        "id": "slm6KkVEF_xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = flan_t5_base_tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "# Move input tensors to the same device as the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "\n",
        "output = flan_t5_base_tokenizer.decode(\n",
        "    flan_t5_small_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE ORIGINAL HUMAN ANNOTED SUBJECT LINE :\\n{subject}\\n')\n",
        "print(dash_line)\n",
        "print(f'FLAN-T5-SMALL MODEL GENERATION - ZERO SHOT SUBJECT LINE:\\n{output}')"
      ],
      "metadata": {
        "id": "LYLYJUzZG_pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**By testing with various models with the zero shot inferencing, we can see that the model struggles to extract the same subject line compared to the baseline subject, but it does pull out some important information from the email which indicates the models can be fine-tuned to the task at hand.**"
      ],
      "metadata": {
        "id": "C7S2UVFV8ZND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Performing Full Fine-Tuning using FLAN-T5 BASE MODEL**"
      ],
      "metadata": {
        "id": "3kTvPTm2wFVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "# # # Define special tokens\n",
        "# # special_tokens = {\n",
        "# #     \"additional_special_tokens\": [\"<EMAIL>\", \"</EMAIL>\"]\n",
        "# # }\n",
        "\n",
        "# # # Add special tokens to the tokenizer\n",
        "# # tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "\n",
        "# # Load the model\n",
        "# model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "# Resize the model's embeddings to accommodate the new tokens\n",
        "flan_t5_base_original_model.resize_token_embeddings(len(flan_t5_base_tokenizer))\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    start_prompt = 'Generate subject line for the email.\\n\\nEmail:'\n",
        "    end_prompt = '\\nSubject:\\n'\n",
        "    inputs = [start_prompt + email + end_prompt for email in examples['cleaned_emails']]\n",
        "    model_inputs = flan_t5_base_tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
        "\n",
        "    labels = flan_t5_base_tokenizer(examples['subject'], max_length=15, truncation=True, padding='max_length')\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = Train_dataset.map(tokenize_function, batched=True, remove_columns=Train_dataset.column_names)\n"
      ],
      "metadata": {
        "id": "bKNmBevDVfmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset = Dataset.from_pandas(dataset_df)"
      ],
      "metadata": {
        "id": "Dhgp_1YuGFLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_full_dataset = full_dataset.map(tokenize_function, batched=True, remove_columns=full_dataset.column_names)"
      ],
      "metadata": {
        "id": "3_-CBdF6GavI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_full_dataset"
      ],
      "metadata": {
        "id": "t4gF6D6Qq0Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and test\n",
        "split_dataset = tokenized_full_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = split_dataset['train']\n",
        "test_dataset = split_dataset['test']"
      ],
      "metadata": {
        "id": "DCfVV097WbZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "2pDOF9rv8eRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=0.0001,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        ")"
      ],
      "metadata": {
        "id": "ckExw6k-Wq50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=flan_t5_base_original_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=flan_t5_base_tokenizer,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
        "\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Uo6VC5P6Wv1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model and tokenizer\n",
        "save_directory = \"./finetuned_flan_t5_base\"\n",
        "flan_t5_base_original_model.save_pretrained(save_directory)\n",
        "flan_t5_base_tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "# Zip the saved model files\n",
        "zip_filename = \"finetuned_flan_t5_base.zip\"\n",
        "shutil.make_archive(base_name=\"finetuned_flan_t5_base\", format=\"zip\", root_dir=save_directory)"
      ],
      "metadata": {
        "id": "S6gF7_J8uzEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the zipped checkpoint\n",
        "files.download(zip_filename)"
      ],
      "metadata": {
        "id": "qWhDAJVuCFmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Unzip model checkpoint as it is zipped\n",
        "# # !unzip finetuned_flan_t5_base.zip -d ./finetuned_flan_t5_base\n",
        "\n",
        "# # Load the tokenizer\n",
        "# tokenizer = T5Tokenizer.from_pretrained('t5-base', legacy=False)\n",
        "\n",
        "# # Add special tokens to the tokenizer\n",
        "# num_added_tokens = tokenizer.add_special_tokens(special_tokens_map)\n",
        "\n",
        "\n",
        "# model_path = \"./finetuned_flan_t5_base\"\n",
        "# finetuned_tokenizer = T5Tokenizer.from_pretrained(model_path,  legacy=False)\n",
        "# finetuned_model = T5ForConditionalGeneration.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "XJd20rDqvIM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./finetuned_flan_t5_base\"\n",
        "finetuned_tokenizer = T5Tokenizer.from_pretrained(model_path,  legacy=False)\n",
        "finetuned_model = T5ForConditionalGeneration.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "j0l1kvYvGKIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For Finetuned Model : Flan-t5 \\n\",print_number_of_trainable_model_parameters(finetuned_model))"
      ],
      "metadata": {
        "id": "qOAE40aKGtR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load special tokens from JSON file\n",
        "with open('/content/finetuned_flan_t5_base/special_tokens_map.json', 'r') as file:\n",
        "    special_tokens_map = json.load(file)\n",
        "\n",
        "# Print special tokens to verify\n",
        "print(special_tokens_map)"
      ],
      "metadata": {
        "id": "mjIfz8qkD7vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate the Flan T5-Base Model Quantitatively (with ROUGE Metric)**\n",
        "The ROUGE metric helps quantify the validity of subject lines produced by models. It compares subjects to a \"Annoted baseline\" subject which is usually created by a human. While not perfect, it does indicate the overall increase in subject line generatiion effectiveness that we have accomplished by fine-tuning."
      ],
      "metadata": {
        "id": "os-472avsQlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the Saved Finetuned Model and Tokenizer**"
      ],
      "metadata": {
        "id": "xxDoUanxvRov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emails_for_score_df = Train_df.sample(n=10, random_state=32)\n",
        "emails_for_score_df"
      ],
      "metadata": {
        "id": "pMYOOca77uGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emails = emails_for_score_df['body']\n",
        "original_subjects = emails_for_score_df['subject']\n",
        "original_subjects\n"
      ],
      "metadata": {
        "id": "a6FaMiCNtOgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "id": "Mn8SNYNUBLbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_subjects = []\n",
        "flan_T5_subjects = []\n",
        "finetuned_flan_T5_subjects = []\n",
        "\n",
        "\n",
        "for index, row in emails_for_score_df.iterrows():\n",
        "    email = row['body']\n",
        "    original_subject = row['subject']\n",
        "    prompt = f\"\"\"\n",
        "    Generate the subject line for the following email.\n",
        "\n",
        "    Email:\n",
        "    {email}\n",
        "\n",
        "    Subject:\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the device (GPU if available, otherwise CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move both models to the same device\n",
        "    flan_t5_base_original_model.to(device)\n",
        "    finetuned_model.to(device)\n",
        "\n",
        "    original_model_input_ids = flan_t5_base_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    finetuned_model_input_ids = finetuned_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "\n",
        "    original_model_outputs = flan_t5_base_original_model.generate(input_ids=original_model_input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    original_model_text_output = flan_t5_base_tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "    flan_T5_subjects.append(original_model_text_output)\n",
        "\n",
        "    instruct_model_outputs = finetuned_model.generate(input_ids=finetuned_model_input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    instruct_model_text_output = finetuned_tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "    finetuned_flan_T5_subjects.append(instruct_model_text_output)\n",
        "\n",
        "    original_subjects.append(original_subject)\n",
        "\n",
        "zipped_subjects = list(zip(original_subjects, flan_T5_subjects, finetuned_flan_T5_subjects))\n",
        "\n",
        "Flan_T5_df = pd.DataFrame(zipped_subjects, columns = ['original_subjects', 'T5_subjects', 'finetuned_flan_T5_subjects'])\n",
        "Flan_T5_df"
      ],
      "metadata": {
        "id": "0SUG4spPuGPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=flan_T5_subjects,\n",
        "    references=original_subjects,\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "finetuned_model_results = rouge.compute(\n",
        "    predictions=finetuned_flan_T5_subjects,\n",
        "    references=original_subjects,\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('FLAN T5 Base Model:')\n",
        "print(original_model_results)\n",
        "print('\\nFINETUNED FLAN T5 BASE MODEL:')\n",
        "print(finetuned_model_results)"
      ],
      "metadata": {
        "id": "LnuLg7e_1_Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Finetuning results should show  improvement in all ROUGE metrics:**\n",
        "\n"
      ],
      "metadata": {
        "id": "-2t11zBHN8nM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Absolute percentage improvement of FINETUNED MODEL over HUMAN BASELINE\")\n",
        "\n",
        "improvement = (np.array(list(finetuned_model_results.values())) - np.array(list(original_model_results.values())))\n",
        "for key, value in zip(finetuned_model_results.keys(), improvement):\n",
        "    print(f'{key}: {value*100:.2f}%')"
      ],
      "metadata": {
        "id": "cKtHUDqyNf4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Perform Parameter Efficient Fine-Tuning (PEFT)**\n",
        "Parameter Efficient Fine-Tuning (PEFT), which is more efficient than full fine-tuning and yields comparable results. PEFT, often referring to Low-Rank Adaptation (LoRA), enables fine-tuning with fewer compute resources, often a single GPU.\n",
        "\n",
        "LoRA produces a small adapter (a few MBs) while keeping the original LLM unchanged. During inference, this adapter is combined with the original LLM, allowing multiple adapters to reuse the same LLM and reducing memory requirements for various tasks."
      ],
      "metadata": {
        "id": "FoDFsTogOMTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup the PEFT/LoRA model for Fine-Tuning**"
      ],
      "metadata": {
        "id": "O0TgYeKuO-4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \\\n",
        "    loralib==0.1.1 \\\n",
        "    peft==0.3.0 --quiet"
      ],
      "metadata": {
        "id": "Od5iuL0YPKPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32, # Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
        ")"
      ],
      "metadata": {
        "id": "eDAkW_4VO6qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(flan_t5_base_original_model, lora_config)\n",
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ],
      "metadata": {
        "id": "esd41UNmO6il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "uHDODVO9Q4Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset"
      ],
      "metadata": {
        "id": "QPI-afIzahgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "# # Preprocess function\n",
        "# def preprocess_function(examples):\n",
        "#     inputs = tokenizer(examples['body'], max_length=512, truncation=True, padding=\"max_length\")\n",
        "#     targets = tokenizer(examples['lable'], max_length=128, truncation=True, padding=\"max_length\")\n",
        "#     return {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'labels': targets['input_ids']} # Added attention_mask to the output\n",
        "\n",
        "# # Apply preprocessing\n",
        "# train_dataset = split_dataset['train'].map(preprocess_function, batched=True)\n",
        "# val_dataset = split_dataset['validation'].map(preprocess_function, batched=True)\n",
        "\n",
        "# Load base model\n",
        "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "# Define LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "FIVCe35mZtlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "UwNKgwWScPvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kill -9 <pid>"
      ],
      "metadata": {
        "id": "hbedsL4PcaRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f'./peft-subjectline-training-{str(int(time.time()))}'\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=1,\n",
        "    max_steps=1,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "xMBzuOTjiaiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "id": "m-IZqqfui1FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_trainer.train()"
      ],
      "metadata": {
        "id": "gNK4EmpSinor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    gradient_accumulation_steps=8,  # Accumulate gradients over multiple steps\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=200,\n",
        "    fp16=True,  # Enable mixed precision training\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "PS2BCdU1YxUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained('path/to/save/finetuned_model')\n",
        "tokenizer.save_pretrained('path/to/save/finetuned_model')"
      ],
      "metadata": {
        "id": "rByaLl7oZZBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f'./peft-subjectline-training-{str(int(time.time()))}'\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    # learning_rate=1e-4, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    # logging_steps=10,\n",
        "    # max_steps=1\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=split_dataset['test'],\n",
        ")\n",
        "\n",
        "\n",
        "peft_trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "Td3BbnnYPrhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_path=\"./peft-subjectline-checkpoint-local\"\n",
        "\n",
        "peft_trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)"
      ],
      "metadata": {
        "id": "mQIvcDX7YaEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(peft_model_base,\n",
        "                                       './peft-dialogue-summary-checkpoint-from-s3/',\n",
        "                                       torch_dtype=torch.bfloat16,\n",
        "                                       is_trainable=False)"
      ],
      "metadata": {
        "id": "swBbx2xffokn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wz0o7_vWSgON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the 'punkt' resource\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('wordnet') # Download the WordNet corpus"
      ],
      "metadata": {
        "id": "vTP9c9DuNXhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "# Evaluate the generated subject lines\n",
        "def evaluate_metrics(true_subject, generated_subject):\n",
        "    # ROUGE-L\n",
        "    rougeL = scorer.score(true_subject, generated_subject)['rougeL'].fmeasure\n",
        "\n",
        "    # METEOR\n",
        "    meteor = meteor_score([word_tokenize(true_subject)], word_tokenize(generated_subject))\n",
        "\n",
        "    # SACREBLEU\n",
        "    sacre_bleu = sacrebleu.corpus_bleu([generated_subject], [[true_subject]]).score\n",
        "\n",
        "    return rougeL, meteor, sacre_bleu\n",
        "\n",
        "Flan_T5_df['rougeL'], Flan_T5_df['meteor'], Flan_T5_df['sacrebleu'] = zip(*Flan_T5_df.apply(\n",
        "    lambda row: evaluate_metrics(row['original_subjects'], row['T5_subjects']), axis=1))\n",
        "\n",
        "# Calculate average scores\n",
        "average_rougeL = Flan_T5_df['rougeL'].mean()\n",
        "average_meteor = Flan_T5_df['meteor'].mean()\n",
        "average_sacrebleu = Flan_T5_df['sacrebleu'].mean()\n",
        "\n",
        "print(f'Average ROUGE-L: {average_rougeL:.4f}')\n",
        "print(f'Average METEOR: {average_meteor:.4f}')\n",
        "print(f'Average SACREBLEU: {average_sacrebleu:.4f}')"
      ],
      "metadata": {
        "id": "xFBuePhiLDcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1_dWxlbZQCTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#T5 Flan Base\n",
        "# Load the ROUGE metric\n",
        "rouge_metric = load_metric('rouge')\n",
        "\n",
        "def calculate_rouge_scores(predictions, references):\n",
        "    rouge_metric.add_batch(predictions=predictions, references=references)\n",
        "    result = rouge_metric.compute()\n",
        "    return {\n",
        "        'rouge1': result['rouge1'].mid.fmeasure * 100,\n",
        "        'rouge2': result['rouge2'].mid.fmeasure * 100,\n",
        "        'rougeL': result['rougeL'].mid.fmeasure * 100\n",
        "    }\n",
        "\n",
        "# Generate predictions and calculate ROUGE scores\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "for example in test_dataset.shuffle(seed=42).select(range(20)):\n",
        "    input_ids = example['input_ids']\n",
        "    input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)  # Add batch dimension\n",
        "    outputs = finetuned_model.generate(input_ids)\n",
        "    pred = finetuned_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    ref = finetuned_tokenizer.decode(example['labels'], skip_special_tokens=True)\n",
        "\n",
        "    predictions.append(pred)\n",
        "    references.append(ref)\n",
        "\n",
        "# Calculate and print ROUGE scores for each test sample\n",
        "rouge_scores = []\n",
        "for pred, ref in zip(predictions, references):\n",
        "    score = calculate_rouge_scores([pred], [ref])\n",
        "    rouge_scores.append(score)\n",
        "    print(f\"Prediction: {pred}\\nReference: {ref}\\nROUGE Scores: {score}\\n\")\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "rouge_df = pd.DataFrame(rouge_scores)\n",
        "rouge_df"
      ],
      "metadata": {
        "id": "vQ-w7TxGZEV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)"
      ],
      "metadata": {
        "id": "rubayi3ZdWSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_test_df = test_df.sample(n=10, random_state=42)"
      ],
      "metadata": {
        "id": "SHpw9jHsefXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_test_df"
      ],
      "metadata": {
        "id": "258hkqQlevCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_subject_line(email_body):\n",
        "    inputs = finetuned_tokenizer(\"Generate a subject line for the following email.: \" + email_body, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    # Move inputs to the same device as the model\n",
        "    inputs = {k: v.to(finetuned_model.device) for k, v in inputs.items()}\n",
        "    # Access input_ids as a key in the dictionary\n",
        "    outputs = finetuned_model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n",
        "    return finetuned_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "Hw-_IgKYdlkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_test_df['generated_subject_line'] = small_test_df['cleaned_emails'].apply(generate_subject_line)\n",
        "small_test_df"
      ],
      "metadata": {
        "id": "HPLoCJgffW7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "\n",
        "# Evaluate the generated subject lines\n",
        "def evaluate_metrics(true_subject, generated_subject):\n",
        "    # Tokenize the input for METEOR\n",
        "    true_subject_tokens = nltk.word_tokenize(true_subject)\n",
        "    generated_subject_tokens = nltk.word_tokenize(generated_subject)\n",
        "\n",
        "    # ROUGE-L\n",
        "    rougeL = scorer.score(true_subject, generated_subject)['rougeL'].fmeasure\n",
        "\n",
        "    # METEOR\n",
        "    meteor = meteor_score([true_subject_tokens], generated_subject_tokens) # Pass tokenized input\n",
        "\n",
        "    # SACREBLEU - Use import sacrebleu directly\n",
        "    sacre_bleu_score = sacrebleu.corpus_bleu([generated_subject], [[true_subject]]).score\n",
        "\n",
        "    return rougeL, meteor, sacre_bleu_score # Return the score, not the module\n",
        "\n",
        "small_test_df['rougeL'], small_test_df['meteor'], small_test_df['sacrebleu'] = zip(*small_test_df.apply(\n",
        "    lambda row: evaluate_metrics(row['subject'], row['generated_subject_line']), axis=1))"
      ],
      "metadata": {
        "id": "fjUdAhycdLvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_test_df"
      ],
      "metadata": {
        "id": "E4bo2fd6NQQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")"
      ],
      "metadata": {
        "id": "tAUYMLeiWoQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_prompt = 'Generate the subject line for the following email.\\n\\nEmail:'\n",
        "end_prompt = '\\nSubject:\\n'\n",
        "start_prompt + email + end_prompt\n",
        "print(start_prompt)\n",
        "print(email)\n",
        "print(end_prompt)"
      ],
      "metadata": {
        "id": "hcU_iws1LLSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization for FLAN-T5 model\n",
        "def flan_t5_base_tokenize_function(example):\n",
        "    start_prompt = 'Generate the subject line for the following email.\\n\\nEmail:'\n",
        "    end_prompt = '\\nSubject:\\n'\n",
        "    prompt = [start_prompt + email + end_prompt for email in example[\"body\"]]\n",
        "    example['input_ids'] = flan_t5_base_tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    example['labels'] = flan_t5_base_tokenizer(example[\"subject\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    return example\n",
        "\n",
        "# The dataset actually contains 3 diff splits: train, validation, test.\n",
        "# The tokenize_function code is handling all data across all splits in batches.\n",
        "tokenized_train_datasets = train_dataset.map(flan_t5_base_tokenize_function, batched=True)\n",
        "tokenized_test_datasets = test_dataset.map(flan_t5_base_tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "id": "PNZhgan9fJ6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_train_datasets = tokenized_train_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
        "# tokenized_test_datasets = tokenized_test_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
      ],
      "metadata": {
        "id": "EkFTC1Mvr81h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_datasets, tokenized_test_datasets"
      ],
      "metadata": {
        "id": "lV8NjE0YrWaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FLAN-T5 BASE - Fine-Tune the Model with the Preprocessed Tokenised Dataset utiliing the built-in Hugging Face Trainer class. Then compare it with reference to the original model.**\n",
        "\n",
        "flan_t5_base_original_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)\n",
        "flan_t5_base_tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')"
      ],
      "metadata": {
        "id": "Q0tc44JYPNW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    start_prompt = 'Generate the subject line for the following email.\\n\\nEmail:'\n",
        "    end_prompt = '\\nSubject:\\n'\n",
        "    inputs = [start_prompt + email + end_prompt for email in examples['email_body']]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
        "\n",
        "    labels = tokenizer(examples['subject'], max_length=50, truncation=True, padding='max_length')\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n"
      ],
      "metadata": {
        "id": "H1GVZxyPU56L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Tokenization\n",
        "# def preprocess_data(examples):\n",
        "#     inputs = [\"generate subject line: \" + email for email in examples['cleaned_emails']]\n",
        "#     model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "#     with tokenizer.as_target_tokenizer():\n",
        "#         labels = tokenizer(examples['subject'], max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "#     return model_inputs\n",
        "\n",
        "# train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
        "# test_dataset = test_dataset.map(preprocess_data, batched=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=flan_t5_base_original_model ,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_datasets,\n",
        "    eval_dataset=tokenized_test_datasets,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "RV5TsUrfR1zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FLAN-T5 SMALL - Fine-Tune the Model with the Preprocessed Tokenised Dataset utiliing the built-in Hugging Face Trainer class. Then compare it with reference to the original model.\n",
        "\n",
        "flan_t5_small_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\", torch_dtype=torch.bfloat16)\n",
        "flan_t5_small_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")"
      ],
      "metadata": {
        "id": "YrHuhrEfsmdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization for FLAN-T5 SMALL model\n",
        "def flan_t5_small_tokenize_function(example):\n",
        "    start_prompt = 'Generate the subject line for the following email.\\n\\nEmail:'\n",
        "    end_prompt = '\\nSubject:\\n'\n",
        "    prompt = [start_prompt + email + end_prompt for email in example[\"cleaned_emails\"]]\n",
        "    example['input_ids'] = flan_t5_small_tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    example['labels'] = flan_t5_small_tokenizer(example[\"subject\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    return example\n",
        "\n",
        "# The dataset actually contains 3 diff splits: train, validation, test.\n",
        "# The tokenize_function code is handling all data across all splits in batches.\n",
        "tokenized_train_datasets = train_dataset.map(flan_t5_small_tokenize_function, batched=True)\n",
        "tokenized_test_datasets = test_dataset.map(flan_t5_small_tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "n15RWBkAXxzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=flan_t5_small_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_datasets,\n",
        "    eval_dataset=tokenized_test_datasets,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "GjJMAmUcYSh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model checkpoint\n",
        "flan_t5_small_model.save_pretrained('./results/Flan_T5_small_model')\n",
        "flan_t5_small_tokenizer.save_pretrained('./results/Flan_T5_small_model')\n",
        "\n",
        "# Define and save the generation configuration\n",
        "generation_config = GenerationConfig(\n",
        "    early_stopping=True,\n",
        "    num_beams=4,\n",
        "    no_repeat_ngram_size=3,\n",
        "    forced_bos_token_id=0,\n",
        "    forced_eos_token_id=2\n",
        ")\n",
        "\n",
        "generation_config.save_pretrained('./results/Flan_T5_small_model')"
      ],
      "metadata": {
        "id": "CcOfhXaGdmyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AutoModelForSeq2SeqLM\n",
        "AutoTokenizer"
      ],
      "metadata": {
        "id": "tESux8htfc_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained Flan T5 Small fine tuned model\n",
        "flan_T5_small_model_directory = '/content/results/Flan_T5_small_model'\n",
        "flan_T5_small_tokenizer = AutoTokenizer.from_pretrained(flan_T5_small_model_directory)\n",
        "flan_T5_small_model_finetuned = AutoModelForSeq2SeqLM.from_pretrained(flan_T5_small_model_directory)"
      ],
      "metadata": {
        "id": "PM4itxVveS3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emails_for_score = tokenized_test_datasets[0:10]['cleaned_emails']\n",
        "original_subjects = tokenized_test_datasets[0:10]['subject']"
      ],
      "metadata": {
        "id": "b9GGjgy6b0cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flan_T5small_subjects = []\n",
        "finetuned_flan_T5small_subjects = []\n",
        "\n",
        "for _, cleaned_emails in enumerate(emails_for_score):\n",
        "    prompt = f\"\"\"\n",
        "    Generate the subject line for the following email.\n",
        "\n",
        "    Email:\n",
        "    {email}\n",
        "\n",
        "    Subject:\n",
        "    \"\"\"\n",
        "    # Tokenize the input and move to the same device as the model\n",
        "    inputs = flan_t5_small_tokenizer(prompt, return_tensors='pt').to(flan_T5_small_model_finetuned.device) # Move inputs to the same device as the fine-tuned model\n",
        "\n",
        "    # Add the decoder_start_token_id to the generation configuration\n",
        "    generation_config = GenerationConfig(max_new_tokens=200,\n",
        "        decoder_start_token_id=flan_T5_small_tokenizer.bos_token_id if flan_T5_small_tokenizer.bos_token_id is not None else flan_T5_small_tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Generate subject using the original model\n",
        "    # Make sure flan_t5_small_model is on the same device as flan_T5_small_model_finetuned\n",
        "    flan_t5_small_model = flan_t5_small_model.to(flan_T5_small_model_finetuned.device)\n",
        "    original_model_outputs = flan_t5_small_model.generate(**inputs, generation_config=generation_config)\n",
        "    original_model_subject_output = flan_t5_small_tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "    flan_T5small_subjects.append(original_model_subject_output)\n",
        "\n",
        "    # Generate subject using the finetuned model\n",
        "    trained_flan_t5_outputs = flan_T5_small_model_finetuned.generate(**inputs, generation_config=generation_config)\n",
        "    instruct_model_text_output = flan_T5_small_tokenizer.decode(trained_flan_t5_outputs[0], skip_special_tokens=True)\n",
        "    finetuned_flan_T5small_subjects.append(instruct_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(original_subjects, flan_T5small_subjects, finetuned_flan_T5small_subjects))\n",
        "\n",
        "Flan_T5_Small_Subjects_df = pd.DataFrame(zipped_summaries, columns = ['Original_subjects', 'Flan_T5s_subjects', 'Finetuned_Flan_T5s_subjects'])\n",
        "Flan_T5_Small_Subjects_df"
      ],
      "metadata": {
        "id": "LXGOGtXUbh5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "# from datasets import Dataset\n",
        "\n",
        "\n",
        "# # Initialize tokenizer and model\n",
        "# tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "# T5_model = T5ForConditionalGeneration.from_pretrained('t5-small')"
      ],
      "metadata": {
        "id": "exqW5Kv0mzyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    start_prompt = 'Generate the subject line for the following email.\\n\\n'\n",
        "    end_prompt = '\\n\\Subject: '\n",
        "    prompt = [start_prompt + email + end_prompt for email in example[\"cleaned_emails\"]]\n",
        "    # Tokenize the input_ids - do not flatten\n",
        "    example['input_ids'] = [tokenizer(p, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids[0] for p in prompt]\n",
        "    # Tokenize the labels - do not flatten\n",
        "    example['labels'] = [tokenizer(s, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids[0] for s in example[\"subject\"]]\n",
        "\n",
        "    return example"
      ],
      "metadata": {
        "id": "aJmS7TCD4cQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output_dir = f'./email-Subject-training-{str(int(time.time()))}'\n",
        "\n",
        "# # Define training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=output_dir,\n",
        "#     learning_rate=1e-5,\n",
        "#     num_train_epochs=3,\n",
        "#     weight_decay=0.01,\n",
        "#     logging_steps=30,\n",
        "#     evaluation_strategy='epoch',\n",
        "#     per_device_train_batch_size=4,  # Reduced batch size\n",
        "#     # max_steps=1\n",
        "# )\n",
        "\n",
        "# # training_args = TrainingArguments(\n",
        "# #     output_dir='./results',\n",
        "# #     evaluation_strategy='epoch',\n",
        "# #     learning_rate=1e-5,\n",
        "# #     per_device_train_batch_size=4,\n",
        "# #     per_device_eval_batch_size=4,\n",
        "# #     num_train_epochs=3,\n",
        "# #     weight_decay=0.01,\n",
        "# # )\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=flan_t5_base_original_model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=tokenized_train_datasets,\n",
        "#     eval_dataset=tokenized_test_datasets\n",
        "# )\n",
        "\n",
        "# trainer.train()"
      ],
      "metadata": {
        "id": "ymjhoRaoshwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Generate subject lines for the test set\n",
        "# def generate_subject_line(email_body):\n",
        "#     inputs = tokenizer(\"generate subject line: \" + email_body, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "#     # Move inputs to the same device as the model\n",
        "#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "#     outputs = model.generate(inputs.input_ids, max_length=128, num_beams=4, early_stopping=True)\n",
        "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# test_df['generated_subject_line'] = test_df['cleaned_emails'].apply(generate_subject_line)\n",
        "\n",
        "# Generate subject lines for the test set\n",
        "def generate_subject_line(email_body):\n",
        "    inputs = tokenizer(\"generate subject line: \" + email_body, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    # Move inputs to the same device as the model\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    # Access input_ids as a key in the dictionary\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "test_df['generated_subject_line'] = test_df['cleaned_emails'].apply(generate_subject_line)"
      ],
      "metadata": {
        "id": "H00VaamVakII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **t5-small Evaluation**"
      ],
      "metadata": {
        "id": "cQ4bUC74cjcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)"
      ],
      "metadata": {
        "id": "XJ3D8o9yegTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the 'punkt' resource\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet') # Download WordNet\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "\n",
        "# Evaluate the generated subject lines\n",
        "def evaluate_metrics(true_subject, generated_subject):\n",
        "    # Tokenize the input for METEOR\n",
        "    true_subject_tokens = nltk.word_tokenize(true_subject)\n",
        "    generated_subject_tokens = nltk.word_tokenize(generated_subject)\n",
        "\n",
        "    # ROUGE-L\n",
        "    rougeL = scorer.score(true_subject, generated_subject)['rougeL'].fmeasure\n",
        "\n",
        "    # METEOR\n",
        "    meteor = meteor_score([true_subject_tokens], generated_subject_tokens) # Pass tokenized input\n",
        "\n",
        "    # SACREBLEU - Use import sacrebleu directly\n",
        "    sacre_bleu_score = sacrebleu.corpus_bleu([generated_subject], [[true_subject]]).score\n",
        "\n",
        "    return rougeL, meteor, sacre_bleu_score # Return the score, not the module\n",
        "\n",
        "test_df['rougeL'], test_df['meteor'], test_df['sacrebleu'] = zip(*test_df.apply(\n",
        "    lambda row: evaluate_metrics(row['subject'], row['generated_subject_line']), axis=1))\n",
        "\n",
        "# Calculate average scores\n",
        "average_rougeL = test_df['rougeL'].mean()\n",
        "average_meteor = test_df['meteor'].mean()\n",
        "average_sacrebleu = test_df['sacrebleu'].mean() # Now you should be able to calculate the mean\n",
        "\n",
        "print(f'Average ROUGE-L: {average_rougeL:.4f}')\n",
        "print(f'Average METEOR: {average_meteor:.4f}')\n",
        "print(f'Average SACREBLEU: {average_sacrebleu:.4f}')"
      ],
      "metadata": {
        "id": "t5C7gSX3ceB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pretrained('facebook/bart-base')**"
      ],
      "metadata": {
        "id": "nP7-BBpHezyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "K5lsH75xikZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert DataFrame to Dataset\n",
        "# train_dataset = Dataset.from_pandas(train_df)\n",
        "# test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "facebook_bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "facebook_bart_base_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
      ],
      "metadata": {
        "id": "W_UUmQ-O5tso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For Model : facebook_bart_base_model \\n\",print_number_of_trainable_model_parameters(facebook_bart_base_model))"
      ],
      "metadata": {
        "id": "HGVkLIIf52zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FB Bart doesnot require a prompt\n",
        "# prompt = f\"\"\"\n",
        "# Generate a subject line for the following email.\n",
        "\n",
        "# Email:\n",
        "# {email}\n",
        "\n",
        "# Subject:\n",
        "# \"\"\"\n",
        "\n",
        "inputs = facebook_bart_tokenizer(email, return_tensors='pt')\n",
        "\n",
        "# Move input tensors to the same device as the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inputs = {k: v.to(facebook_bart_base_model.device) for k, v in inputs.items()}\n",
        "\n",
        "outputs = facebook_bart_base_model.generate(inputs['input_ids'], max_length=10, num_beams=4, early_stopping=True)\n",
        "\n",
        "# Decode the output to get the subject line as text\n",
        "generated_subject_line = facebook_bart_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE ORIGINAL HUMAN ANNOTED SUBJECT LINE :\\n{subject}\\n')\n",
        "print(dash_line)\n",
        "print(f'FB BART MODEL GENERATION - ZERO SHOT SUBJECT LINE:\\n{generated_subject_line}')"
      ],
      "metadata": {
        "id": "3p1txC8YLwi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# Load BART model and tokenizer\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
        "\n",
        "def get_subject(text, max_length=20):\n",
        "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(inputs, max_length=max_length, min_length=5, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "text = email\n",
        "subject = get_subject(text, max_length=30)\n",
        "print(\"Subject:\", subject)"
      ],
      "metadata": {
        "id": "3rQkXNPXQmO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "model = DistilBertForTokenClassification.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Load pipeline for NER\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "text = email\n",
        "entities = nlp(text)\n",
        "\n",
        "# Extract important entities\n",
        "print(\"Entities:\", entities)"
      ],
      "metadata": {
        "id": "vGbgjujXRXWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenization\n",
        "def Tokenization_preprocess_data(examples):\n",
        "    inputs = [prompt + email for email in examples['cleaned_emails']]\n",
        "    model_inputs = facebook_bart_tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Use facebook_bart_tokenizer as the target tokenizer\n",
        "    with facebook_bart_tokenizer.as_target_tokenizer():\n",
        "        labels = facebook_bart_tokenizer(examples['subject'], max_length=10, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "train_dataset = train_dataset.map(Tokenization_preprocess_data, batched=True)\n",
        "test_dataset = test_dataset.map(Tokenization_preprocess_data, batched=True)\n",
        "\n",
        "# # Define training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir='./results',\n",
        "#     evaluation_strategy='epoch',\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=4,\n",
        "#     per_device_eval_batch_size=4,\n",
        "#     num_train_epochs=3,\n",
        "#     weight_decay=0.01,\n",
        "# )\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',        # Directory to save the model checkpoints\n",
        "    evaluation_strategy='steps',\n",
        "    save_steps=500,               # Save checkpoint every 1000 steps\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=200,\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "facebook_bart_base_model_trainer = Trainer(\n",
        "    model=facebook_bart_base_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "facebook_bart_base_model_trainer.train()\n"
      ],
      "metadata": {
        "id": "UHfFS01Feyu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final model checkpoint\n",
        "facebook_bart_base_model.save_pretrained('./results/FB_Bart_model')\n",
        "facebook_bart_tokenizer.save_pretrained('./results/FB_Bart_model')\n",
        "\n",
        "# Define and save the generation configuration\n",
        "generation_config = GenerationConfig(\n",
        "    early_stopping=True,\n",
        "    num_beams=4,\n",
        "    no_repeat_ngram_size=3,\n",
        "    forced_bos_token_id=0,\n",
        "    forced_eos_token_id=2\n",
        ")\n",
        "\n",
        "generation_config.save_pretrained('./results/FB_Bart_model')"
      ],
      "metadata": {
        "id": "8Pj219LPCVmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content"
      ],
      "metadata": {
        "id": "wfBgXge85Fxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "logs_folder = '/content/logs'  # Adjust to your source folder\n",
        "logs_git_folder = './content/qM-AI-L/email-subject/model-tuning/logs'  # Adjust to your repository folder\n",
        "\n",
        "# Copy the folder\n",
        "shutil.copytree(logs_folder, logs_git_folder)"
      ],
      "metadata": {
        "id": "62ihDbDA_Dgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "results_folder = '/content/results'  # Adjust to your source folder\n",
        "results_git_folder = './content/qM-AI-L/email-subject/model-tuning/results'  # Adjust to your repository folder\n",
        "\n",
        "# Copy the folder\n",
        "shutil.copytree(results_folder, results_git_folder)"
      ],
      "metadata": {
        "id": "KA6-ZTV1AoJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to the cloned repository\n",
        "os.chdir('./content/qM-AI-L')  # Adjust to your repository folder\n",
        "\n",
        "# Configure Git (only needed once)\n",
        "!git config --global user.email \"mlreddy3082@gmail.com\"\n",
        "!git config --global user.name \"Mlr9459\"\n",
        "\n",
        "# Add, commit, and push changes\n",
        "!git add /content/qM-AI-L/email-subject/\n",
        "!git commit -m \"Add training outputs: model checkpoints, tokenizer, and generation configuration\"\n",
        "!git push origin master"
      ],
      "metadata": {
        "id": "qdXDrq7SCPm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the Model Quantitatively (with ROUGE Metric)\n",
        "The ROUGE metric) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning.**"
      ],
      "metadata": {
        "id": "BhjyD6Wyzxhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_datasets, tokenized_test_datasets"
      ],
      "metadata": {
        "id": "lKqFN4OX1yaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_datasets[0:10]['cleaned_emails']"
      ],
      "metadata": {
        "id": "m1tWpGFv2EcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emails_for_score = tokenized_test_datasets[0:10]['cleaned_emails']\n",
        "human_annoted_subjects = tokenized_test_datasets[0:10]['subject']\n"
      ],
      "metadata": {
        "id": "pqox7th015Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "facebook_bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "facebook_bart_base_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
      ],
      "metadata": {
        "id": "W1-e0Dgg4s71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_directory = '/content/results/FB_Bart_model'\n",
        "facebook_bart_tokenizer_trained = BartTokenizer.from_pretrained(model_directory)\n",
        "facebook_bart_base_model_trained = BartForConditionalGeneration.from_pretrained(model_directory)"
      ],
      "metadata": {
        "id": "gXnio11hELAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "facebook_bart_subjects = []\n",
        "trained_facebook_bart_subjects = []\n",
        "\n",
        "for _, email in enumerate(emails_for_score):\n",
        "    prompt = f\"\"\"\n",
        "    Generate the subject line for the following email.\n",
        "\n",
        "    Email:\n",
        "    {email}\n",
        "\n",
        "    Subject:\n",
        "    \"\"\"\n",
        "    # Tokenize the input and move to the same device as the model\n",
        "    inputs = facebook_bart_tokenizer(prompt, return_tensors='pt').to(facebook_bart_base_model.device)\n",
        "\n",
        "    # Add the decoder_start_token_id to the generation configuration\n",
        "    generation_config = GenerationConfig(\n",
        "    max_new_tokens=200,\n",
        "    decoder_start_token_id=facebook_bart_tokenizer.bos_token_id  # Add this line\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Generate subject using the original model\n",
        "    original_model_outputs = facebook_bart_base_model.generate(**inputs, generation_config=generation_config)\n",
        "    original_model_subject_output = facebook_bart_tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "    facebook_bart_subjects.append(original_model_subject_output)\n",
        "\n",
        "    trained_facebook_bart_outputs = facebook_bart_base_model_trained.generate(**inputs, generation_config=generation_config)\n",
        "    instruct_model_text_output = facebook_bart_tokenizer_trained.decode(trained_facebook_bart_outputs[0], skip_special_tokens=True)\n",
        "    trained_facebook_bart_subjects.append(instruct_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(human_annoted_subjects, facebook_bart_subjects, trained_facebook_bart_subjects))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns = ['human_annoted_subjects', 'facebook_bart_subjects', 'trained_facebook_bart_subjects'])\n",
        "df"
      ],
      "metadata": {
        "id": "Yy9Bz2uV1c-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "from rouge_score.scoring import BootstrapAggregator\n",
        "\n"
      ],
      "metadata": {
        "id": "eMHM2fPzsAxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate ROUGE scores for each pair of prediction and reference\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "aggregator = BootstrapAggregator() # Initialize aggregator\n",
        "\n",
        "for prediction, reference in zip(facebook_bart_subjects, human_annoted_subjects[0:len(facebook_bart_subjects)]):\n",
        "    score = scorer.score(reference, prediction)\n",
        "    aggregator.add_scores(score) # Add scores to aggregator\n",
        "\n",
        "# Aggregate the scores\n",
        "original_model_results = aggregator.aggregate()\n",
        "\n",
        "print(original_model_results)"
      ],
      "metadata": {
        "id": "eAzGwd9qrS0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model_results = rouge.compute(\n",
        "    predictions=facebook_bart_subjects,\n",
        "    references=human_annoted_subjects[0:len(facebook_bart_subjects)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=trained_facebook_bart_subjects,\n",
        "    references=human_annoted_subjects[0:len(trained_facebook_bart_subjects)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('INSTRUCT MODEL:')\n",
        "print(instruct_model_results)"
      ],
      "metadata": {
        "id": "4N2v_M6SJUBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model locally\n",
        "model_save_path = \"./facebook_bart_base\"\n",
        "facebook_bart_base_model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "WCpjkEa9rWxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate subject lines for the test set\n",
        "def generate_subject_line(email_body):\n",
        "    inputs = tokenizer(\"generate subject line: \" + email_body, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    # Move inputs to the GPU if available\n",
        "    inputs = {k: v.to(facebook_bart_base_model.device) for k, v in inputs.items()}\n",
        "    # Access input_ids from the dictionary\n",
        "    outputs = facebook_bart_base_model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "test_df['generated_subject_line'] = test_df['body'].apply(generate_subject_line)"
      ],
      "metadata": {
        "id": "HM0tVaa5i8n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "n5PT2dfT2OEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "from nltk.translate.meteor_score import meteor_score"
      ],
      "metadata": {
        "id": "T6KRD7Oo2Muk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "\n",
        "# Evaluate the generated subject lines\n",
        "def evaluate_metrics(true_subject, generated_subject):\n",
        "    # Tokenize the input for METEOR\n",
        "    true_subject_tokens = nltk.word_tokenize(true_subject)\n",
        "    generated_subject_tokens = nltk.word_tokenize(generated_subject)\n",
        "\n",
        "    # ROUGE-L\n",
        "    rougeL = scorer.score(true_subject, generated_subject)['rougeL'].fmeasure\n",
        "\n",
        "    # METEOR\n",
        "    meteor = meteor_score([true_subject_tokens], generated_subject_tokens) # Pass tokenized input\n",
        "\n",
        "    # SACREBLEU - Use import sacrebleu directly\n",
        "    sacre_bleu_score = sacrebleu.corpus_bleu([generated_subject], [[true_subject]]).score\n",
        "\n",
        "    return rougeL, meteor, sacre_bleu_score # Return the score, not the module\n",
        "\n",
        "test_df['rougeL'], test_df['meteor'], test_df['sacrebleu'] = zip(*test_df.apply(\n",
        "    lambda row: evaluate_metrics(row['subject'], row['generated_subject_line']), axis=1))"
      ],
      "metadata": {
        "id": "g5swqKxsfrQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate average scores\n",
        "average_rougeL = test_df['rougeL'].mean()\n",
        "average_meteor = test_df['meteor'].mean()\n",
        "average_sacrebleu = test_df['sacrebleu'].mean() # Now you should be able to calculate the mean\n",
        "\n",
        "print(f'Average facebook_bart_base_model ROUGE-L: {average_rougeL:.4f}')\n",
        "print(f'Average facebook_bart_base_model METEOR: {average_meteor:.4f}')\n",
        "print(f'Average facebook_bart_base_model SACREBLEU: {average_sacrebleu:.4f}')"
      ],
      "metadata": {
        "id": "TcCHg_OU29ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Replace 'directory_name' with the name of your directory\n",
        "shutil.make_archive('/content/facebook_bart_base_model', 'zip', 'facebook_bart_base_model')"
      ],
      "metadata": {
        "id": "Lzt-7qci4Niq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# files.download('facebook_bart_base_model.zip')"
      ],
      "metadata": {
        "id": "pY7QEHAw4bsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **t5-base**"
      ],
      "metadata": {
        "id": "Dx76AiWz5CEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokenizer and model t5-base\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n"
      ],
      "metadata": {
        "id": "qlr5gRXjGGw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset.from_pandas(pivot_df)"
      ],
      "metadata": {
        "id": "zd2OYT0bXhMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "_dQE0OytX7D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(pivot_df):\n",
        "    inputs = pivot_df['cleaned_emails']\n",
        "    targets = pivot_df['subject']\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "j2MxJu3bGWxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "id": "K0aZkei_T0V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Replace 'directory_name' with the name of your directory\n",
        "# shutil.make_archive('/content/fine_tuned_t5', 'zip', 'fine_tuned_t5')\n",
        "\n",
        "# Download the zipped directory\n"
      ],
      "metadata": {
        "id": "uGocveE7q2_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.make_archive('/content/results', 'zip', 'FB_Bart_model_results')\n",
        "shutil.make_archive('/content/logs', 'zip', 'FB_Bart_model_logs')\n"
      ],
      "metadata": {
        "id": "eT_AJo9PEjxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# files.download('fine_tuned_t5.zip')"
      ],
      "metadata": {
        "id": "ZO2hx72nrVx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "BDnsANB7Iz3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_dataset['attention_mask']\n",
        "# tokenized_dataset['input_ids']"
      ],
      "metadata": {
        "id": "6Mo5GbMDH8Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and test sets\n",
        "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "# Data collator for padding\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
      ],
      "metadata": {
        "id": "0KMgDPdkU753"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    # predict_with_generate=True,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "19V3PPTgVBQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('fine_tuned_t5')\n",
        "tokenizer.save_pretrained('fine_tuned_t5')"
      ],
      "metadata": {
        "id": "uvY_nd2_bise"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available. \\nUsing GPU\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"GPU is not available. \\nUsing CPU\")\n",
        "    device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "9shJp4Spb3Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(text):\n",
        "    inputs = tokenizer.encode_plus(text, return_tensors='pt', max_length=512, truncation=True)\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    summary_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=150, num_beams=2, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\n",
        "\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n"
      ],
      "metadata": {
        "id": "ohAtd3YQbn1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example inference\n",
        "example_text = pivot_df['body'][3]\n",
        "print(\"Email Body:\", example_text)\n",
        "print(\"\\n\\n Generated Subject:\", generate_summary(example_text))\n",
        "print(\"\\n\\n Actual Subject:\", pivot_df['subject'][3])"
      ],
      "metadata": {
        "id": "V_MzfdBqcJZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "# Evaluate the generated subject lines\n",
        "def evaluate_metrics(true_subject, generated_subject):\n",
        "    # ROUGE-L\n",
        "    rougeL = scorer.score(true_subject, generated_subject)['rougeL'].fmeasure\n",
        "\n",
        "    # METEOR\n",
        "    meteor = meteor_score([true_subject], generated_subject)\n",
        "\n",
        "    # SACREBLEU\n",
        "    sacre_bleu = sacrebleu.corpus_bleu([generated_subject], [[true_subject]]).score\n",
        "\n",
        "    return rougeL, meteor, sacre_bleu\n",
        "\n",
        "test_df['rougeL'], test_df['meteor'], test_df['sacrebleu'] = zip(*test_df.apply(\n",
        "    lambda row: evaluate_metrics(row['subject_line'], row['generated_subject_line']), axis=1))\n",
        "\n",
        "# Calculate average scores\n",
        "average_rougeL = test_df['rougeL'].mean()\n",
        "average_meteor = test_df['meteor'].mean()\n",
        "average_sacrebleu = test_df['sacrebleu'].mean()\n",
        "\n",
        "print(f'Average ROUGE-L: {average_rougeL:.4f}')\n",
        "print(f'Average METEOR: {average_meteor:.4f}')\n",
        "print(f'Average SACREBLEU: {average_sacrebleu:.4f}')"
      ],
      "metadata": {
        "id": "UvJijyqfN5k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_sentences(text):\n",
        "  \"\"\"\n",
        "  This function extracts sentences from text without introducing control characters.\n",
        "\n",
        "  Args:\n",
        "      text (str): The text to be processed.\n",
        "\n",
        "  Returns:\n",
        "      list: A list of extracted sentences.\n",
        "  \"\"\"\n",
        "  # Split text based on sentence boundaries (adjust pattern as needed)\n",
        "  sentences = re.split(r\"(?<!\\w)\\.(?!\\w)\", text)\n",
        "\n",
        "  # Remove empty elements and leading/trailing whitespace\n",
        "  sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "\n",
        "  return sentences\n",
        "\n",
        "# Example usage\n",
        "text = \"This is an example text. It contains multiple sentences.  \\nThere are also newlines and extra spaces.\"\n",
        "extracted_sentences = extract_sentences(pivot_df['body'][0])\n",
        "print(extracted_sentences)\n"
      ],
      "metadata": {
        "id": "R_UuPM3bLEEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df['body'][0]"
      ],
      "metadata": {
        "id": "gW-fbCCleshA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "summary = summarizer(pivot_df['body'][0], max_length=15, min_length=2, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "id": "8hmYeCH2flFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df['body'][0]"
      ],
      "metadata": {
        "id": "QSg88TJak4t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Chirayu/subject-generator-t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Chirayu/subject-generator-t5-base\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "def get_subject(content, num_beams=5,max_length=512, repetition_penalty=2.5, length_penalty=1, early_stopping=True,top_p=.95, top_k=50, num_return_sequences=3):\n",
        "\n",
        "  text =  \"title: \" + content + \" </s>\"\n",
        "\n",
        "  input_ids = tokenizer.encode(\n",
        "    text, return_tensors=\"pt\", add_special_tokens=True\n",
        "  )\n",
        "\n",
        "  input_ids = input_ids.to(device)\n",
        "  generated_ids = model.generate(\n",
        "      input_ids=input_ids,\n",
        "\n",
        "      num_beams=num_beams,\n",
        "      max_length=max_length,\n",
        "      repetition_penalty=repetition_penalty,\n",
        "      length_penalty=length_penalty,\n",
        "      early_stopping=early_stopping,\n",
        "      top_p=top_p,\n",
        "      top_k=top_k,\n",
        "      num_return_sequences=num_return_sequences,\n",
        "  )\n",
        "  subjects = [tokenizer.decode(generated_id,skip_special_tokens=True,clean_up_tokenization_spaces=True,) for generated_id in generated_ids]\n",
        "  return subjects"
      ],
      "metadata": {
        "id": "eI-YnG00leqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = get_subject(pivot_df['body'][0])\n",
        "x"
      ],
      "metadata": {
        "id": "JhMrjNf3mKOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df['body'][0]"
      ],
      "metadata": {
        "id": "fTcDIbDHq1wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df['body'][1]"
      ],
      "metadata": {
        "id": "X3-wKWGkmgXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap"
      ],
      "metadata": {
        "id": "XOWH5k3TjZqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_summarizer(email_text):\n",
        "\n",
        "    model_name = \"facebook/bart-large-cnn\"\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    inputs = tokenizer.encode(\"summarize: \" + email_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(inputs, max_length=100, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    formatted_summary = \"\\n\".join(textwrap.wrap(summary, width=80))\n",
        "    return formatted_summary\n",
        "\n",
        "summary = text_summarizer(pivot_df['body'][0])\n",
        "summary"
      ],
      "metadata": {
        "id": "Y85goaEkcOYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sentences(text):\n",
        "    \"\"\"\n",
        "    Extract sentences from the text and remove newline characters.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The text to process.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of sentences without newline characters.\n",
        "    \"\"\"\n",
        "    # Replace newline characters with spaces\n",
        "    text = text.replace('\\n', '')\n",
        "    text = text.replace('!', '.').replace('?', '.').replace(';', '.')\n",
        "\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "# Example usage\n",
        "sample_text = \"Hello, world!\\nThis is a test.\\nLet's replace, commas, and newlines.\"\n",
        "sentences = extract_sentences(sample_text)\n",
        "for sentence in sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "id": "zGazV9EgQkxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = text_summarizer(pivot_df['body'][0])\n",
        "summary"
      ],
      "metadata": {
        "id": "jezqQnA9Lck-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df['body'][0]"
      ],
      "metadata": {
        "id": "7ZrJIfk2MFn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_len = [len(pivot_df['dialogue'].split()) for x in samsum['train']]\n",
        "sub_len = [len(pivot_df['summary'].split()) for x in samsum['train']]\n",
        "\n",
        "dialogue_len = [len(x['dialogue'].split()) for x in samsum['train']]\n",
        "summary_len = [len(x['summary'].split()) for x in samsum['train']]\n",
        "\n"
      ],
      "metadata": {
        "id": "_AM_aMgKSjST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "describe_df(pivot_df)"
      ],
      "metadata": {
        "id": "0N8EfTwkRqF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_rows"
      ],
      "metadata": {
        "id": "FrRN8if0dyTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.preprocessing import Tokenizer, Lowercaser, PunctuationRemover, StopwordRemover\n",
        "from langchain.stemming import Stemmer\n",
        "from langchain import Pipeline"
      ],
      "metadata": {
        "id": "oKUEMAee8aSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(email_docs)*3"
      ],
      "metadata": {
        "id": "REA79efkSNQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emails_df['content'][0]"
      ],
      "metadata": {
        "id": "pwhf-gfhMfWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emails_df['source'][0]"
      ],
      "metadata": {
        "id": "FLk0SU2hTPr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_file_name(emails_df['source'][0])"
      ],
      "metadata": {
        "id": "XTKL_ao_T6jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8h-_DelG2pb9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4c72c4faf85f445ba9d953ef921e89d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3408f36d59364a15b2168ca381788477",
              "IPY_MODEL_7d415e0fadb54450baa7b145ad546fc8",
              "IPY_MODEL_17fffb7161954a7a80c2a6e976cbd37f"
            ],
            "layout": "IPY_MODEL_17a84654a0804cd19b35f7c87b9d2d3a"
          }
        },
        "3408f36d59364a15b2168ca381788477": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73309f927c1f488fbfb1c7083fead481",
            "placeholder": "​",
            "style": "IPY_MODEL_dbcbe7b9fb994b23b5bbeddf46adb63f",
            "value": "Map: 100%"
          }
        },
        "7d415e0fadb54450baa7b145ad546fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_452fdc2565984701923cf24635c2ad78",
            "max": 3464,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0c44351a15e42c5a90712ac27b4e05a",
            "value": 3464
          }
        },
        "17fffb7161954a7a80c2a6e976cbd37f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad06fa7e06dd43bf8d5f177df9d3adc9",
            "placeholder": "​",
            "style": "IPY_MODEL_2b2331464a2e47fd99a8883a8bf6b7a0",
            "value": " 3464/3464 [00:09&lt;00:00, 388.49 examples/s]"
          }
        },
        "17a84654a0804cd19b35f7c87b9d2d3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73309f927c1f488fbfb1c7083fead481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbcbe7b9fb994b23b5bbeddf46adb63f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "452fdc2565984701923cf24635c2ad78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0c44351a15e42c5a90712ac27b4e05a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad06fa7e06dd43bf8d5f177df9d3adc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b2331464a2e47fd99a8883a8bf6b7a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "853b267b7d04441387c35afce7d48d83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a454ded720e4024a603a17ca75c4f07",
              "IPY_MODEL_4f9f4e3314894567bf576d0a96b5fb3b",
              "IPY_MODEL_62fcfe28d8de4633a48aab08286edb0a"
            ],
            "layout": "IPY_MODEL_404696b41bea444596aace330a2abe70"
          }
        },
        "4a454ded720e4024a603a17ca75c4f07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7036af3d10c4667921b5fe539fd5ab1",
            "placeholder": "​",
            "style": "IPY_MODEL_5c6040cddec14d78b90369762771cc98",
            "value": "Map: 100%"
          }
        },
        "4f9f4e3314894567bf576d0a96b5fb3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b13a9bbf5c544fefbb281b3e7194798d",
            "max": 867,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1dcfecbb12994f4b8550103f11e6a329",
            "value": 867
          }
        },
        "62fcfe28d8de4633a48aab08286edb0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e15eb89663d74eff925345ab2873aa7b",
            "placeholder": "​",
            "style": "IPY_MODEL_0654c21295f3435f902218e858603e67",
            "value": " 867/867 [00:02&lt;00:00, 383.33 examples/s]"
          }
        },
        "404696b41bea444596aace330a2abe70": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7036af3d10c4667921b5fe539fd5ab1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c6040cddec14d78b90369762771cc98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b13a9bbf5c544fefbb281b3e7194798d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dcfecbb12994f4b8550103f11e6a329": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e15eb89663d74eff925345ab2873aa7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0654c21295f3435f902218e858603e67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}