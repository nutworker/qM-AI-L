{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tJ5HpKCvcx8I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "8205a850-54db-4984-82fc-ffc502dbe127"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#sample huggingface model\\nhttps://huggingface.co/docs/transformers/model_doc/gpt2\\n#data for training is at git location\\nhttps://github.com/ryanzhumich/AESLC\\n#Some models to read throught o use -- mentor provided\\nhttps://arxiv.org/abs/2310.06825\\nhttps://arxiv.org/abs/2307.09288\\nhttps://arxiv.org/abs/2309.16609 '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "'''\n",
        "#sample huggingface model\n",
        "https://huggingface.co/docs/transformers/model_doc/gpt2\n",
        "#data for training is at git location\n",
        "https://github.com/ryanzhumich/AESLC\n",
        "#Some models to read throught o use -- mentor provided\n",
        "https://arxiv.org/abs/2310.06825\n",
        "https://arxiv.org/abs/2307.09288\n",
        "https://arxiv.org/abs/2309.16609 '''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers[torch]\n",
        "!pip install -U huggingface_hub\n",
        "! pip install evaluate\n",
        "!pip install sacrebleu\n",
        "!pip install rouge_score\n",
        "!pip install -U nltk\n",
        "#!pip install bitsandbytes\n",
        "!pip install tqdm\n"
      ],
      "metadata": {
        "id": "_V-i-lxqQSs5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc1bf3e2-468d-4daf-b56c-49a00e562bd3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.32.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.1+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.23.5)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.24.0-py3-none-any.whl (419 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.0/419.0 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.7.4)\n",
            "Installing collected packages: huggingface_hub\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.23.5\n",
            "    Uninstalling huggingface-hub-0.23.5:\n",
            "      Successfully uninstalled huggingface-hub-0.23.5\n",
            "Successfully installed huggingface_hub-0.24.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub"
                ]
              },
              "id": "0eb37a5944684acd880ded464276b776"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.4)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.15.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=2.0.0->evaluate)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Collecting requests>=2.19.0 (from evaluate)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 evaluate-0.4.2 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyarrow",
                  "requests"
                ]
              },
              "id": "1b9721c5a368458f9947b104f8fb5aea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/106.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.5.15)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.25.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.10.1 sacrebleu-2.4.2\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.4)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=5a11be3a29c9587f0e15c68841fe43974adb7cc66850feed2f1c3f2826563bea\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#clone the data from git\n",
        "!git clone https://github.com/ryanzhumich/AESLC.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y8vyq-TdE4M",
        "outputId": "7d72b77c-c90c-4cf8-e4cf-5695bb87e09f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'AESLC' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import shutil"
      ],
      "metadata": {
        "id": "EfJG4fTj3b9A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, random_split, DataLoader\n",
        "from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel, DataCollatorForLanguageModeling #Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import accelerate\n",
        "import evaluate\n",
        "#import bitsandbytes\n",
        "from torch import nn\n",
        "from transformers.trainer_pt_utils import get_parameter_names\n",
        "import re\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CXmsLOhNrYVA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "16ff027d-e139-4a91-8502-3334d79dc188"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'pyarrow.lib' has no attribute 'ListViewType'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-5b03d47d13a0>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataCollatorForLanguageModeling\u001b[0m \u001b[0;31m#Seq2SeqTrainingArguments, Seq2SeqTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccelerate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#import bitsandbytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mevaluation_suite\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluationSuite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m from .evaluator import (\n\u001b[1;32m     31\u001b[0m     \u001b[0mAudioClassificationEvaluator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/evaluate/evaluation_suite/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDownloadConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.20.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptimizedTypedSequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msanitize_patterns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_reader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthread_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/parquet/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# flake8: noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parquet\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     raise ImportError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/_parquet.pyx\u001b[0m in \u001b[0;36minit pyarrow._parquet\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pyarrow.lib' has no attribute 'ListViewType'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#define a function for subset data copying\n",
        "def subsetFileCopy(sourceDir, destDir, countToCopy):\n",
        "  #remove and recreate destDir directory\n",
        "  if os.path.exists(destDir):\n",
        "    shutil.rmtree(destDir)\n",
        "  os.makedirs(destDir)\n",
        "  #Read Source directory to randomly select few files and copy\n",
        "  for dirname, _, filenames in os.walk(sourceDir):\n",
        "      #count how many files we are writing\n",
        "      count = 0\n",
        "      for count in range(countToCopy):\n",
        "        filename = random.choice(filenames)\n",
        "        source = os.path.join(dirname, filename)\n",
        "        destination = os.path.join(destDir,filename)\n",
        "        shutil.copy(source, destination)\n",
        "        count += 1"
      ],
      "metadata": {
        "id": "X1wXPlCYzQ3Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy subset train data to train subset directory\n",
        "train_dir        = '/content/AESLC/enron_subject_line/train'\n",
        "train_subset_dir = '/content/AESLC/enron_subject_line/train_subset'\n",
        "\n",
        "val_dir          = '/content/AESLC/enron_subject_line/dev'\n",
        "val_subset_dir   = '/content/AESLC/enron_subject_line/dev_subset'\n",
        "\n",
        "test_dir         = '/content/AESLC/enron_subject_line/test'\n",
        "test_subset_dir  = '/content/AESLC/enron_subject_line/test_subset'\n",
        "\n",
        "base_dir = '/content/AESLC/enron_subject_line/'"
      ],
      "metadata": {
        "collapsed": true,
        "id": "S8EBxKBBrkJR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mergeEmailFilesIntoCSV(TargetCSVFilename, sourceDirPath):\n",
        "    filename = TargetCSVFilename\n",
        "    dirpath = sourceDirPath\n",
        "    output = base_dir+filename\n",
        "    #Remove if csv already exists\n",
        "    if os.path.exists(output):\n",
        "        os.remove(output)\n",
        "\n",
        "    csvout_lst = []\n",
        "    files = [os.path.join(dirpath, fname) for fname in os.listdir(dirpath)]\n",
        "\n",
        "    for filename in sorted(files):\n",
        "        #open each email file and process to seperate email body and subject with delimiter @subject\n",
        "        #data = pd.read_csv(filename, sep=\"@subject\\n\", header=None)\n",
        "        with open(filename, 'r') as fin:\n",
        "          lines = [line.strip('\\n')  for line in fin]\n",
        "          #reset to use for next file\n",
        "          subIndex = 0\n",
        "          email=''\n",
        "          subject=''\n",
        "          row_num = 0\n",
        "          #find the delimiting line index\n",
        "          for i, t in enumerate(lines):\n",
        "            if t =='@subject':\n",
        "              subIndex = i\n",
        "          #write to df columns based on delimiter index\n",
        "          for i, text in enumerate(lines):\n",
        "            if i<subIndex :\n",
        "              email = email+text+' '\n",
        "            elif i>subIndex:\n",
        "              subject = subject+text+' '\n",
        "          new_row =  pd.DataFrame({'Row_Num': [row_num],'Email':[email], 'Subject':[subject]})\n",
        "          row_num += 1\n",
        "          table = pd.pivot_table(new_row, index =['Row_Num','Email', 'Subject'])\n",
        "          csvout_lst.append(table)\n",
        "    pd.concat(csvout_lst).to_csv(output)"
      ],
      "metadata": {
        "id": "fYtjlz80HGV4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mergeEmailFilesIntoCSV_alongWithAnnotations(TargetCSVFilename, sourceDirPath):\n",
        "    filename = TargetCSVFilename\n",
        "    dirpath = sourceDirPath\n",
        "    output = base_dir+filename\n",
        "    #Remove if csv already exists\n",
        "    if os.path.exists(output):\n",
        "        os.remove(output)\n",
        "\n",
        "    csvout_lst = []\n",
        "    files = [os.path.join(dirpath, fname) for fname in os.listdir(dirpath)]\n",
        "\n",
        "    for filename in sorted(files):\n",
        "        #open each email file and process to seperate email body and subject with delimiter @subject\n",
        "        #data = pd.read_csv(filename, sep=\"@subject\\n\", header=None)\n",
        "        with open(filename, 'r') as fin:\n",
        "          lines = [line.strip('\\n')  for line in fin]\n",
        "          #reset to use for next file\n",
        "          email=''\n",
        "          subject=''\n",
        "          ann0=''\n",
        "          ann1=''\n",
        "          ann2=''\n",
        "          subIndex = 0\n",
        "          ann0Index = 0\n",
        "          ann1Index = 0\n",
        "          ann2Index = 0\n",
        "          row_num = 0\n",
        "\n",
        "          #find the delimiting line index\n",
        "          for i, t in enumerate(lines):\n",
        "            if t =='@subject':\n",
        "              subIndex = i\n",
        "            elif t == '@ann0':\n",
        "              ann0Index = i\n",
        "            elif t == '@ann1':\n",
        "              ann1Index = i\n",
        "            elif t == '@ann2':\n",
        "              ann2Index = i\n",
        "\n",
        "          #write to df columns based on delimiter index\n",
        "          for i, text in enumerate(lines):\n",
        "            if i<subIndex :\n",
        "              email = email+text+' '\n",
        "            elif i>subIndex and i<ann0Index :\n",
        "              subject = subject+text+' '\n",
        "            elif i>ann0Index and i<ann1Index:\n",
        "              ann0 = ann0+text+' '\n",
        "            elif i>ann1Index and i<ann2Index:\n",
        "              ann1 = ann1+text+' '\n",
        "            elif i>ann2Index:\n",
        "              ann2 = ann2+text+' '\n",
        "          new_row =  pd.DataFrame({'Row_Num': [row_num], 'Email':[email], 'Subject':[subject], 'Ann0':[ann0], 'Ann1':[ann1], 'Ann2':[ann2]})\n",
        "          row_num += 1\n",
        "          table = pd.pivot_table(new_row, index =['Row_Num', 'Email', 'Subject', 'Ann0', 'Ann1', 'Ann2'])\n",
        "          csvout_lst.append(table)\n",
        "    pd.concat(csvout_lst).to_csv(output)"
      ],
      "metadata": {
        "id": "FMyGKXS7mMhh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#=====================SUBSET FILES====================================#\n",
        "#call function to copy subset of data\n",
        "subsetFileCopy(train_dir, train_subset_dir, 1000)\n",
        "subsetFileCopy(val_dir, val_subset_dir, 10)\n",
        "subsetFileCopy(test_dir, test_subset_dir, 10)\n",
        "\n",
        "#call function to make csv from subset Train email files\n",
        "mergeEmailFilesIntoCSV('train_subset.csv', train_subset_dir)\n",
        "mergeEmailFilesIntoCSV_alongWithAnnotations('val_subset.csv', val_subset_dir)\n",
        "mergeEmailFilesIntoCSV_alongWithAnnotations('test_subset.csv', test_subset_dir)\n",
        "\n",
        "#This is train on subset data folders\n",
        "df = pd.read_csv(base_dir+'train_subset.csv')\n",
        "df_val = pd.read_csv(base_dir+'val_subset.csv')\n",
        "df_test = pd.read_csv(base_dir+'test_subset.csv')"
      ],
      "metadata": {
        "id": "G5JE48J8hZti"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#========================================FULL FILES=================================#\n",
        "\n",
        "#call function to make csv from subset Train email files\n",
        "mergeEmailFilesIntoCSV('train.csv', train_dir)\n",
        "mergeEmailFilesIntoCSV_alongWithAnnotations('val.csv', val_dir)\n",
        "mergeEmailFilesIntoCSV_alongWithAnnotations('test.csv', test_dir)\n",
        "\n",
        "#This is train on whole dataset folders\n",
        "df = pd.read_csv(base_dir+'train.csv')\n",
        "df_val = pd.read_csv(base_dir+'val.csv')\n",
        "df_test = pd.read_csv(base_dir+'test.csv')"
      ],
      "metadata": {
        "id": "y0NyEzpdkOzh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/"
      ],
      "metadata": {
        "id": "pFjtnLvq_cqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5436379f-264b-4ba0-cc93-e3bd277a1527"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /usr/share/nltk_data/corpora/wordnet.zip, /usr/share/nltk_data/corpora/wordnet.zip.zip or /usr/share/nltk_data/corpora/wordnet.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sacrebleu = evaluate.load(\"sacrebleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "meteor = evaluate.load(\"meteor\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "Wtgd2EjAlb0p",
        "outputId": "0ce165e8-64a8-4a69-bcc2-22b31ae92d0a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'evaluate' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d0b48bc02f14>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msacrebleu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sacrebleu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrouge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rouge\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmeteor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"meteor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n"
      ],
      "metadata": {
        "id": "BIXZwzA_lh4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make a directory\n",
        "if not os.path.exists('/content/working/output'):\n",
        "    os.makedirs('/content/working/output')"
      ],
      "metadata": {
        "id": "TVrXcoKB1rJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq,Trainer, TrainingArguments\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "81B6kkCRsXVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2', bos_token='<|startoftext|>',\n",
        "                                          eos_token='<|endoftext|>', pad_token='<|pad|>', sep_token='<|sep|>')\n",
        "model = GPT2LMHeadModel.from_pretrained('distilgpt2').cuda()\n",
        "model.resize_token_embeddings(len(tokenizer))'''"
      ],
      "metadata": {
        "id": "5GgrvZQAlixL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datacollator = DataCollatorForSeq2Seq(tokenizer = tokenizer, model=model)\n"
      ],
      "metadata": {
        "id": "G7TcmqMGlm3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datacollator"
      ],
      "metadata": {
        "id": "lXrGwF6FofrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmailSubjectDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.labels = []\n",
        "        for index, row in data.iterrows():\n",
        "            encodings_dict = tokenizer('<|startoftext|>' + row[\"Email\"] + '<|sep|>' + row[\"Subject\"] + '<|endoftext|>', truncation=True, max_length=250, padding=\"max_length\", return_tensors='pt')\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "       return self.input_ids[idx]"
      ],
      "metadata": {
        "id": "qMk3Sq5flxSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ValEmailSubjectDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.labels = []\n",
        "        for index, row in data.iterrows():\n",
        "          encodings_dict = tokenizer('<|startoftext|>' + row[\"Email\"] + '<|sep|>', truncation=True, max_length=250, padding=\"max_length\", return_tensors='pt')\n",
        "          if((encodings_dict['input_ids'][0][249] != torch.Tensor(np.array([50259]))) and (encodings_dict['input_ids'][0][249] != torch.Tensor(np.array([50258])))):\n",
        "            encodings_dict['input_ids'][0] = torch.cat((encodings_dict['input_ids'][0][np.r_[:249]],torch.Tensor(np.array([50258]))),0)\n",
        "          self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "          self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx]"
      ],
      "metadata": {
        "id": "l9-sF-00l3QK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestEmailSubjectDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.labels = []\n",
        "        for index, row in data.iterrows():\n",
        "          encodings_dict = tokenizer('<|startoftext|>' + row[\"Email\"] + '<|sep|>', truncation=True, max_length=250, return_tensors='pt')\n",
        "          self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "          self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx]"
      ],
      "metadata": {
        "id": "YZNKPLQml8_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = EmailSubjectDataset(df, tokenizer)\n",
        "val_dataset = ValEmailSubjectDataset(df_val,tokenizer)\n",
        "test_dataset = TestEmailSubjectDataset(df_test,tokenizer)"
      ],
      "metadata": {
        "id": "ucXIJxcHl-eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_dataset.attn_masks"
      ],
      "metadata": {
        "id": "9Caf3r-Do699"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/Content/working/output\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=15,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=8,\n",
        "    #eval_steps = 2,\n",
        "    eval_steps = 400,\n",
        "    save_steps=800,\n",
        "    warmup_steps=500,\n",
        "    load_best_model_at_end=True,\n",
        "    #evaluation_strategy=\"steps\",\n",
        "    #save_strategy = \"steps\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    weight_decay=0.01,\n",
        "    #metric_for_best_model = \"rougeL\",\n",
        "    gradient_accumulation_steps=4,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True\n",
        "    )\n"
      ],
      "metadata": {
        "id": "W4BHWl40mBdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "metadata": {
        "id": "7oDP6Ca_NmSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from tqdm import tqdm # Import the tqdm function\n",
        "\n",
        "def compute_metrics(eval_pred, eval_dataset, df):\n",
        "    decoded_preds = []\n",
        "    references = [df['Subject'], df['Ann0'], df['Ann1'], df['Ann2']]\n",
        "    refs = []\n",
        "\n",
        "\n",
        "    '''for i in range(len(eval_dataset)):\n",
        "        sample = eval_dataset[i]\n",
        "        # Assuming sample is a dictionary-like object with 'Row_Num' key\n",
        "        Row_Num = sample.get('Row_Num', None)\n",
        "        if Row_Num is None:\n",
        "            print(f\"Warning: Sample at index {i} does not have 'input_ids' key. Skipping.\")\n",
        "            continue\n",
        "        # Convert Row_Num to a tensor for comparison\n",
        "        Row_Num_tensor = torch.tensor(Row_Num)\n",
        "        # Filter Row_Num_tensor based on the length of eval_dataset\n",
        "        temp_input = Row_Num_tensor[Row_Num_tensor != len(eval_dataset)]\n",
        "\n",
        "        temp_input = temp_input[None, :]\n",
        "        metric_outputs = model.generate(temp_input.cuda(), attention_mask=eval_dataset.attn_masks,min_new_tokens = 4, max_new_tokens = 12, num_beams=5, early_stopping=True, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
        "        decoded_preds.append(tokenizer.decode(metric_outputs[0]))\n",
        "'''\n",
        "    for idx, pred in tqdm(enumerate(eval_pred.predictions)):\n",
        "        temp_input = torch.tensor(pred).long()\n",
        "        # Access attention mask for the current sample\n",
        "        attention_mask = eval_dataset[idx]['attention_mask']\n",
        "        temp_input = temp_input[None, :]\n",
        "        metric_outputs = model.generate(torch.tensor(temp_input).cuda(),\n",
        "                                        attention_mask=torch.tensor(attention_mask).cuda(),\n",
        "                                        min_new_tokens = 4,\n",
        "                                        max_new_tokens = 12, num_beams=5,\n",
        "                                        early_stopping=True, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
        "        decoded_preds.append(tokenizer.decode(metric_outputs[0]))\n",
        "\n",
        "    final_preds =[]\n",
        "    for j in range(len(decoded_preds)):\n",
        "        lst = decoded_preds[j].split('<|sep|>')\n",
        "        if (len(lst) >= 2):\n",
        "            final_preds.append(lst[1].replace(\"<|endoftext|>\",\"\"))\n",
        "        temp_refs = []\n",
        "        for k in range(len(references)):\n",
        "            temp_refs.append(references[k][j])\n",
        "        refs.append(temp_refs)\n",
        "\n",
        "    results_sacrebleu = sacrebleu.compute(predictions=final_preds, references=refs, lowercase = True)\n",
        "\n",
        "    results_rouge = rouge.compute(predictions=final_preds, references=refs)\n",
        "\n",
        "    results_meteor = meteor.compute(predictions=final_preds, references=refs)\n",
        "\n",
        "    return {'bleu': results_sacrebleu['score'], 'rouge1' : results_rouge['rouge1'], 'rouge2' : results_rouge['rouge2'], 'rougeL' : results_rouge['rougeL'], 'meteor' : results_meteor['meteor']}"
      ],
      "metadata": {
        "id": "uzfGg_pYGvU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''def compute_metrics(eval_pred, eval_dataset, df):\n",
        "    decoded_preds = []\n",
        "    # Assuming 'eval_pred.predictions' is the model output\n",
        "    for sample_input in eval_pred.predictions:\n",
        "        temp_input = torch.tensor(sample_input) # Convert to tensor if it's not already\n",
        "        temp_input = temp_input[temp_input != tokenizer.pad_token_id] # Remove padding tokens\n",
        "        if temp_input.numel() > 0: # Check if tensor is not empty\n",
        "            temp_input = temp_input[None, :]  # Add a batch dimension if necessary\n",
        "            metric_outputs = model.generate(temp_input.cuda(), min_new_tokens=4, max_new_tokens=12,\n",
        "                                           num_beams=5, early_stopping=True, num_return_sequences=1,\n",
        "                                           pad_token_id=tokenizer.eos_token_id)\n",
        "            decoded_preds.append(tokenizer.decode(metric_outputs[0]))\n",
        "        else:\n",
        "            decoded_preds.append(\"\") # Handle empty tensors\n",
        "\n",
        "    final_preds =[]\n",
        "    for j in range(len(decoded_preds)):\n",
        "        lst = decoded_preds[j].split('<|sep|>')\n",
        "        if (len(lst) >= 2):\n",
        "            final_preds.append(lst[1].replace(\"<|endoftext|>\",\"\"))\n",
        "        temp_refs = []\n",
        "        for k in range(len(references)):\n",
        "            temp_refs.append(references[k][j])\n",
        "        refs.append(temp_refs)\n",
        "\n",
        "    results_sacrebleu = sacrebleu.compute(predictions=final_preds, references=refs, lowercase = True)\n",
        "\n",
        "    results_rouge = rouge.compute(predictions=final_preds, references=refs)\n",
        "\n",
        "    results_meteor = meteor.compute(predictions=final_preds, references=refs)\n",
        "\n",
        "    return {'bleu': results_sacrebleu['score'], 'rouge1' : results_rouge['rouge1'], 'rouge2' : results_rouge['rouge2'], 'rougeL' : results_rouge['rougeL'], 'meteor' : results_meteor['meteor']}\n",
        "'''"
      ],
      "metadata": {
        "id": "p1ptf1tfmJzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''def compute_metrics(eval_pred, eval_dataset, df):\n",
        "    logits, labels = eval_pred\n",
        "    # Handle the case where logits is a tuple (e.g., from evaluation_loop)\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]  # Extract the logits tensor from the tuple\n",
        "\n",
        "    pred_ids = torch.argmax(torch.Tensor(logits), dim=-1)  # Get predicted token IDs\n",
        "    # Ensure the shape of pred_ids and labels is correct for the T5 model\n",
        "    pred_ids = pred_ids.view(-1, pred_ids.size(-1))\n",
        "    # Check if labels is a tensor before reshaping\n",
        "    if isinstance(labels, torch.Tensor):\n",
        "        labels = labels.view(-1, labels.size(-1))  # Flatten labels as well to maintain consistency\n",
        "    else:\n",
        "        print(\"Warning: labels is not a torch.Tensor. Skipping reshaping.\")\n",
        "\n",
        "    # Convert predictions and labels to lists\n",
        "    pred_ids = pred_ids.tolist()\n",
        "    labels = labels.tolist()\n",
        "\n",
        "    # ... rest of your compute_metrics function ...\n",
        "\n",
        "    non_padded_preds = []\n",
        "    for pred in pred_ids:\n",
        "        temp_input = torch.tensor([i for i in pred if i != -100]) # Filter out padding tokens\n",
        "        if temp_input.numel() > 0:\n",
        "            temp_input = temp_input.unsqueeze(0).cuda()\n",
        "            decoded_pred = tokenizer.decode(temp_input[0], skip_special_tokens=True) # Decode token IDs to text\n",
        "            non_padded_preds.append(decoded_pred)\n",
        "        else:\n",
        "            non_padded_preds.append(\"\") # Handle empty predictions\n",
        "\n",
        "    # Replace decoded_preds with non-padded versions\n",
        "    decoded_preds = non_padded_preds\n",
        "\n",
        "    final_preds =[]\n",
        "    for j in range(len(decoded_preds)):\n",
        "        lst = decoded_preds[j].split('<|sep|>')\n",
        "        if (len(lst) >= 2):\n",
        "            final_preds.append(lst[1].replace(\"<|endoftext|>\",\"\"))\n",
        "        temp_refs = []\n",
        "        for k in range(len(references)):\n",
        "            temp_refs.append(references[k][j])\n",
        "        refs.append(temp_refs)\n",
        "\n",
        "    results_sacrebleu = sacrebleu.compute(predictions=final_preds, references=refs, lowercase = True)\n",
        "\n",
        "    results_rouge = rouge.compute(predictions=final_preds, references=refs)\n",
        "\n",
        "    results_meteor = meteor.compute(predictions=final_preds, references=refs)\n",
        "\n",
        "    return {'bleu': results_sacrebleu['score'], 'rouge1' : results_rouge['rouge1'], 'rouge2' : results_rouge['rouge2'], 'rougeL' : results_rouge['rougeL'], 'meteor' : results_meteor['meteor']}\n",
        "\n",
        "\n",
        " result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract the Rouge2 score\n",
        "    rouge2_score = result['rouge2'].mid.fmeasure\n",
        "    return {\"rouge2\": rouge2_score}'''"
      ],
      "metadata": {
        "id": "SPsPEaxYO1Es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# Tokenization\n",
        "from datasets import Dataset\n",
        "# Assuming 'df' contains your training data\n",
        "train_dataset = Dataset.from_pandas(df)\n",
        "# Assuming 'df_val' contains your validation data\n",
        "val_dataset = Dataset.from_pandas(df_val)\n",
        "#assuming 'df_test' contains your test data\n",
        "test_dataset = Dataset.from_pandas(df_test)\n",
        "\n",
        "def preprocess_data(examples):\n",
        "    inputs = [\"generate subject line: \" + email for email in examples['Email']]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples['Subject'], max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
        "test_dataset = test_dataset.map(preprocess_data, batched=True)\n",
        "val_dataset = val_dataset.map(preprocess_data, batched=True) '''"
      ],
      "metadata": {
        "id": "TwHVsvrlkrml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    \"\"\"\n",
        "    Original Trainer may have a memory leak.\n",
        "    This is a workaround to avoid storing too many tensors that are not needed.\n",
        "    \"\"\"\n",
        "    # Handle the case where logits is a tuple (e.g., from evaluation_loop)\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]  # Extract the logits tensor from the tuple\n",
        "\n",
        "    pred_ids = torch.argmax(logits, dim=-1)\n",
        "    # Ensure the shape of pred_ids and labels is correct for the T5 model\n",
        "    pred_ids = pred_ids.view(-1, pred_ids.size(-1))\n",
        "    labels = labels.view(-1, labels.size(-1))\n",
        "    return pred_ids, labels\n",
        "    '''"
      ],
      "metadata": {
        "id": "daC15SagmK-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    \"\"\"\n",
        "    Original Trainer may have a memory leak.\n",
        "    This is a workaround to avoid storing too many tensors that are not needed.\n",
        "    \"\"\"\n",
        "    # Handle the case where logits is a tuple (e.g., from evaluation_loop)\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = logits[0]  # Extract the logits tensor from the tuple\n",
        "\n",
        "    pred_ids = torch.argmax(logits, dim=-1)\n",
        "    # Ensure the shape of pred_ids and labels is correct for the T5 model\n",
        "    pred_ids = pred_ids.view(-1, pred_ids.size(-1))\n",
        "    labels = labels.view(-1, labels.size(-1)) # Flatten labels as well to maintain consistency\n",
        "    return pred_ids, labels  # Return predicted token IDs and labels"
      ],
      "metadata": {
        "id": "mykX5KG1HZeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def preprocess_logits_for_metrics(logits, labels):\n",
        "    \"\"\"\n",
        "    Original Trainer may have a memory leak.\n",
        "    This is a workaround to avoid storing too many tensors that are not needed.\n",
        "    \"\"\"\n",
        "    # Handle the case where logits is a tuple (e.g., from evaluation_loop)\n",
        "    if isinstance(logits, tuple):\n",
        "        logits = torch.Tensor(logits[0])  # Extract the logits tensor from the tuple\n",
        "\n",
        "    pred_ids = torch.argmax(logits, dim=-1)  # Get predicted token IDs\n",
        "    # Ensure the shape of pred_ids and labels is correct for the T5 model\n",
        "    pred_ids = pred_ids.view(-1, pred_ids.size(-1))\n",
        "    labels = labels.view(-1, labels.size(-1)) # Flatten labels as well to maintain consistency\n",
        "    return pred_ids, labels  # Return predicted token IDs and labels\n",
        "    '''"
      ],
      "metadata": {
        "id": "ALXAIdRzWWd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check for available CUDA devices\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"CUDA is available. Using GPU.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA is not available. Using CPU.\")\n",
        "\n",
        "# Set the environment variable to enable more verbose CUDA error reporting\n",
        "!export CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "# Commenting out the line below to see if the error persists\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model.to(device),  # Move the model to the appropriate device\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=datacollator,\n",
        "    compute_metrics=lambda pred: compute_metrics(pred, val_dataset, df_val),\n",
        "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "fcq2xamwmT4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import wandb\n",
        "# wandb.login(key = \"{your token here}\")"
      ],
      "metadata": {
        "id": "1ASO2t1qma9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "RN1ND4bcmgXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4D40kK2131iI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model()\n"
      ],
      "metadata": {
        "id": "H27iM8CtmkuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trainedmodel = GPT2LMHeadModel.from_pretrained(\"/Content/input/trainedmodel\").cuda()\n",
        "#trainedmodel.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "2NGEGhQkmttY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_dataset = TestEmailSubjectDataset(df_test,tokenizer)\n",
        "#len(test_dataset)"
      ],
      "metadata": {
        "id": "CgkNFMTFmxKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_output = []\n",
        "for i, sample_input in enumerate(test_dataset):\n",
        "    if(len(sample_input[0]) == 250 and sample_input[0][249] != torch.Tensor(np.array([50258]))):\n",
        "        sample_input[0] = torch.cat((sample_input[0][np.r_[:249]],torch.Tensor(np.array([50258]))),0)\n",
        "    sample_output = model.generate(sample_input.cuda(), min_new_tokens = 4, max_new_tokens = 12, num_beams=5, early_stopping=True, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
        "    temp_output.append(tokenizer.decode(sample_output[0]))\n",
        "final_output =[]\n",
        "for j in range(len(temp_output)):\n",
        "    lst = temp_output[j].split('<|sep|>')\n",
        "    if (len(lst) >= 2):\n",
        "        final_output.append(lst[1].replace(\"<|endoftext|>\",\"\"))\n",
        "    else:\n",
        "        final_output.append(\"\")"
      ],
      "metadata": {
        "id": "cfIijXy9mzqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test[\"Generated\"] = final_output\n",
        "df_test.to_csv('/Content/working/Generated.csv')"
      ],
      "metadata": {
        "id": "pB-OcC1Um3IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_test = pd.read_csv(\"/Content/input/generatedoutput/Generated.csv\")\n"
      ],
      "metadata": {
        "id": "AlO1nN0Sm5ra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = df_test['Generated']\n",
        "ref = [df_test['Subject'], df_test['Ann0'], df_test['Ann1'], df_test['Ann2']]"
      ],
      "metadata": {
        "id": "T6Fwdnhgm9Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def score_evaluate(predictions, references):\n",
        "    preds = []\n",
        "    refs = []\n",
        "    for i in range(len(predictions)):\n",
        "        preds.append(predictions[i])\n",
        "        temp_refs = []\n",
        "        for j in range(len(references)):\n",
        "            temp_refs.append(references[j][i])\n",
        "        refs.append(temp_refs)\n",
        "    results_sacrebleu = sacrebleu.compute(predictions=preds, references=refs, lowercase = True)\n",
        "    print(\"Bleu Score : \" + str(results_sacrebleu['score']))\n",
        "\n",
        "    results_rouge = rouge.compute(predictions=preds, references=refs)\n",
        "    print(\"Rouge1 Score : \" + str(results_rouge['rouge1']))\n",
        "    print(\"Rouge2 Score : \" + str(results_rouge['rouge2']))\n",
        "    print(\"RougeL Score : \" + str(results_rouge['rougeL']))\n",
        "\n",
        "    results_meteor = meteor.compute(predictions=preds, references=refs)\n",
        "    print(\"Meteor Score : \" + str(results_meteor['meteor']))"
      ],
      "metadata": {
        "id": "qDf7Wf4PnCcM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_evaluate(pred,ref)"
      ],
      "metadata": {
        "id": "i3pUjjZmnG5W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}