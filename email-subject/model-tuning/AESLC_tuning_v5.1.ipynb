{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nutworker/qM-AI-L/blob/L_test/email-subject/model-tuning/AESLC_tuning_v5.1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repository_url = 'https://github.com/nutworker/qM-AI-L'\n",
        "!git clone {repository_url}"
      ],
      "metadata": {
        "id": "n0UfO9kv4-nq",
        "outputId": "9f468162-f89b-4b20-e5c5-6831fbbe9f80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'qM-AI-L'...\n",
            "remote: Enumerating objects: 101, done.\u001b[K\n",
            "remote: Counting objects: 100% (101/101), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 101 (delta 39), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (101/101), 7.77 MiB | 5.21 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ID0koCUtX33Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f5e3119-3a33-4070-bc35-7c2ebd97c198",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting datasets>=2.0.0 (from evaluate)\n",
            "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Collecting dill (from evaluate)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.4)\n",
            "Collecting xxhash (from evaluate)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.15.4)\n",
            "Collecting pyarrow>=15.0.0 (from datasets>=2.0.0->evaluate)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Collecting requests>=2.19.0 (from evaluate)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, requests, pyarrow, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.31.0\n",
            "    Uninstalling requests-2.31.0:\n",
            "      Successfully uninstalled requests-2.31.0\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\n",
            "ibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-2.20.0 dill-0.3.8 evaluate-0.4.2 multiprocess-0.70.16 pyarrow-17.0.0 requests-2.32.3 xxhash-3.4.1\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.4)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c8c4f9dfa3d8aba01914ebd012ee8107b08f4d324f2346387dafdac014871187\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.32.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.1+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[torch])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[torch])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.5.15)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.25.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.10.1 sacrebleu-2.4.2\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install rouge-score # Installing rouge-score library (https://pypi.org/project/rouge-score/)\n",
        "\n",
        "# !pip install accelerate -U\n",
        "\n",
        "!pip install transformers[torch]\n",
        "!pip install sacrebleu\n",
        "\n",
        "!pip install datasets nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd # Data Handling\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import seaborn as sns\n",
        "\n",
        "from datasets import load_metric\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "xuOoOaSEDLqg"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformers\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration      # BERT Tokenizer and architecture\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments         # These will help us to fine-tune our model\n",
        "from transformers import pipeline                                         # Pipeline\n",
        "from transformers import DataCollatorForSeq2Seq                           # DataCollator to batch the data\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "tbXvEcauCHyq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "from nltk.translate.meteor_score import meteor_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7T6r-6fLsBo",
        "outputId": "a97b17ba-e8f7-44a5-86d9-4383867b75fc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**"
      ],
      "metadata": {
        "id": "As3cC5xSngQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df = pd.read_csv('/content/qM-AI-L/email-subject/model-tuning/train_emails.csv')"
      ],
      "metadata": {
        "id": "4nBEREYUQPjQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df"
      ],
      "metadata": {
        "id": "L9skosJrQc0t",
        "outputId": "25d03865-2f0f-47bf-c698-5e74a96d8e13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    source                                               body  \\\n",
              "0         allen-p_inbox_20  Greg/Phillip,  Attached is the Grande Communic...   \n",
              "1         allen-p_inbox_28  Phillip & Keith  Attached is the first draw re...   \n",
              "2         allen-p_inbox_63  Your Internet Banking accounts are now setup a...   \n",
              "3         allen-p_inbox_64  To our IBS Customers that are still hanging in...   \n",
              "4         allen-p_inbox_65  Phillip Good Morning!\\nI hope you had a wonder...   \n",
              "...                    ...                                                ...   \n",
              "14431  zufferli-j_inbox_43  This email is acknowledgement from the Power P...   \n",
              "14432  zufferli-j_inbox_44  This email is acknowledgement from the Power P...   \n",
              "14433  zufferli-j_inbox_46  John,  Further to the voice message that I lef...   \n",
              "14434   zufferli-j_inbox_8  Make sure that all curves are downloaded by th...   \n",
              "14435   zufferli-j_inbox_9  John:  Do you need Accumap day one?\\nCarmen sa...   \n",
              "\n",
              "                       subject  ann0  ann1  ann2  body_wcount  subj_wcount  \\\n",
              "0            Service Agreement   NaN   NaN   NaN           65            2   \n",
              "1               Bishops Corner   NaN   NaN   NaN          145            2   \n",
              "2             Internet Banking   NaN   NaN   NaN          250            2   \n",
              "3             Internet Banking   NaN   NaN   NaN          458            2   \n",
              "4      SMEs for expert stories   NaN   NaN   NaN           68            4   \n",
              "...                        ...   ...   ...   ...          ...          ...   \n",
              "14431               Power Pool   NaN   NaN   NaN          227            2   \n",
              "14432    Power Pool of Alberta   NaN   NaN   NaN          277            4   \n",
              "14433           Enron Security   NaN   NaN   NaN          148            2   \n",
              "14434        Simulation Curves   NaN   NaN   NaN           66            2   \n",
              "14435              IHS Accumap   NaN   NaN   NaN           35            2   \n",
              "\n",
              "                                          cleaned_emails  \n",
              "0      gregphillip attached is the grande communicati...  \n",
              "1      phillip keith attached is the first draw reque...  \n",
              "2      your internet banking accounts are now setup a...  \n",
              "3      to our ibs customers that are still hanging in...  \n",
              "4      phillip good morning i hope you had a wonderfu...  \n",
              "...                                                  ...  \n",
              "14431  this email is acknowledgement from the power p...  \n",
              "14432  this email is acknowledgement from the power p...  \n",
              "14433  john further to the voice message that i left ...  \n",
              "14434  make sure that all curves are downloaded by th...  \n",
              "14435  john do you need accumap day one carmen said t...  \n",
              "\n",
              "[14436 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-98005fc2-6aa2-4139-89a2-0046eaad8ad6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>body</th>\n",
              "      <th>subject</th>\n",
              "      <th>ann0</th>\n",
              "      <th>ann1</th>\n",
              "      <th>ann2</th>\n",
              "      <th>body_wcount</th>\n",
              "      <th>subj_wcount</th>\n",
              "      <th>cleaned_emails</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>allen-p_inbox_20</td>\n",
              "      <td>Greg/Phillip,  Attached is the Grande Communic...</td>\n",
              "      <td>Service Agreement</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>65</td>\n",
              "      <td>2</td>\n",
              "      <td>gregphillip attached is the grande communicati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>allen-p_inbox_28</td>\n",
              "      <td>Phillip &amp; Keith  Attached is the first draw re...</td>\n",
              "      <td>Bishops Corner</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>145</td>\n",
              "      <td>2</td>\n",
              "      <td>phillip keith attached is the first draw reque...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>allen-p_inbox_63</td>\n",
              "      <td>Your Internet Banking accounts are now setup a...</td>\n",
              "      <td>Internet Banking</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>250</td>\n",
              "      <td>2</td>\n",
              "      <td>your internet banking accounts are now setup a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>allen-p_inbox_64</td>\n",
              "      <td>To our IBS Customers that are still hanging in...</td>\n",
              "      <td>Internet Banking</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>458</td>\n",
              "      <td>2</td>\n",
              "      <td>to our ibs customers that are still hanging in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>allen-p_inbox_65</td>\n",
              "      <td>Phillip Good Morning!\\nI hope you had a wonder...</td>\n",
              "      <td>SMEs for expert stories</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>68</td>\n",
              "      <td>4</td>\n",
              "      <td>phillip good morning i hope you had a wonderfu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14431</th>\n",
              "      <td>zufferli-j_inbox_43</td>\n",
              "      <td>This email is acknowledgement from the Power P...</td>\n",
              "      <td>Power Pool</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>227</td>\n",
              "      <td>2</td>\n",
              "      <td>this email is acknowledgement from the power p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14432</th>\n",
              "      <td>zufferli-j_inbox_44</td>\n",
              "      <td>This email is acknowledgement from the Power P...</td>\n",
              "      <td>Power Pool of Alberta</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>277</td>\n",
              "      <td>4</td>\n",
              "      <td>this email is acknowledgement from the power p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14433</th>\n",
              "      <td>zufferli-j_inbox_46</td>\n",
              "      <td>John,  Further to the voice message that I lef...</td>\n",
              "      <td>Enron Security</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>148</td>\n",
              "      <td>2</td>\n",
              "      <td>john further to the voice message that i left ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14434</th>\n",
              "      <td>zufferli-j_inbox_8</td>\n",
              "      <td>Make sure that all curves are downloaded by th...</td>\n",
              "      <td>Simulation Curves</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>66</td>\n",
              "      <td>2</td>\n",
              "      <td>make sure that all curves are downloaded by th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14435</th>\n",
              "      <td>zufferli-j_inbox_9</td>\n",
              "      <td>John:  Do you need Accumap day one?\\nCarmen sa...</td>\n",
              "      <td>IHS Accumap</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35</td>\n",
              "      <td>2</td>\n",
              "      <td>john do you need accumap day one carmen said t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14436 rows × 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-98005fc2-6aa2-4139-89a2-0046eaad8ad6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-98005fc2-6aa2-4139-89a2-0046eaad8ad6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-98005fc2-6aa2-4139-89a2-0046eaad8ad6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7fff7b56-59c9-46b2-a8a9-3b49b3580e02\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7fff7b56-59c9-46b2-a8a9-3b49b3580e02')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7fff7b56-59c9-46b2-a8a9-3b49b3580e02 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_d8476b72-aec0-43b8-9943-15018a086a38\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dataset_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_d8476b72-aec0-43b8-9943-15018a086a38 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dataset_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset_df",
              "summary": "{\n  \"name\": \"dataset_df\",\n  \"rows\": 14436,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14436,\n        \"samples\": [\n          \"sanders-r_sent_2329\",\n          \"buy-r_inbox_344\",\n          \"dorland-c_sent_301\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"body\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13523,\n        \"samples\": [\n          \"Sara: Here are drafts of the Certificates and Sight Drafts for Niagara Mohawk, El Paso electric, LG&E Energy Marketing and Energy Production Corp. (just in case it wasn't renewed).\\nI have put together files for each of these that include copies of the LC, the underlying trading docs and any correspondence that has been sent.\\nExcept for Niagara Mohawk which requires copies of the invoices and that the certificate be on \\\"letterhead\\\" there don't appear to be any other special requirements.\\nLeslie Reeves will be faxing the invoices to us.\\nI will be in tomorrow until around 11.\\nThanks for your help on this.\\nAll of these docs are in a file called Enron Restructuring.\",\n          \"Good afternoon,  When you get a chance, please let me know if you would like for me to order you a 2002 calendar.\\nIf so, which kind?\\nYou may want more than one type.\\nJust let me know and I will be more than happy to order what you want.\\nThanks!\",\n          \"Hey Hunter, A quick update.\\nKevin Brady is up to speed on working with Colin and Chris on consolidating critical and noncritical notices for all the regions.\\nI have also passed on the idea of an operations page on the website to the other logistics managers and the central desk schedulers to get their ideas and insights on how to make this page beneficial to both the traders and schedulers.\\nA couple of ideas that have already been passed my way are:  \\t1) Incorporating the Gas Daily and Index historicals against production/storage by pipe so that we could project cash outs (Mark Schrab) \\t2) A transportation rate matrix on the web site (Cora Pendergrass) \\t3) Enron operations personnel contact page with phone numbers and pictures.\\nMuch like the weather guys.\\nI laughed at first but Victor LaMadrid   \\t\\twhom suggested this said that the East Desk Traders don't even know all of his schedulers.\\n4) An easy access to the pipelines meters and dunns numbers.\\n(Kevin Brady) \\t5) Morning sheets updated with contract, constraint, and imbalance information from Sitara and Unify (Victor LaMadrid)  I have asked Kevin to consolidate all the ideas and then we can get together and decide which ones you want to place into production.\\nLisa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12032,\n        \"samples\": [\n          \"OMLX Application\",\n          \"Draft OF CA\",\n          \"Wharton trip\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ann0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ann1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ann2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"body_wcount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 148,\n        \"min\": 25,\n        \"max\": 3136,\n        \"num_unique_values\": 683,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subj_wcount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 15,\n        \"num_unique_values\": 15,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_emails\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13498,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test Train Split**"
      ],
      "metadata": {
        "id": "L6sRPzc6oU02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(dataset_df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "guCjiJJyRLzM"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape, test_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgDx01BreHk",
        "outputId": "25bd8ae0-9e32-42d1-f9ee-106083a23fb8"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11548, 9), (2888, 9))"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Train_dataset = Dataset.from_pandas(train_df)\n",
        "Test_dataset = Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "ZHaFrU6-oYQ5"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_dataset, Test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfLahVPWoa1X",
        "outputId": "1e819c51-9aa2-4aff-df15-1a27d417b8dd"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['source', 'body', 'subject', 'ann0', 'ann1', 'ann2', 'body_wcount', 'subj_wcount', 'cleaned_emails', '__index_level_0__'],\n",
              "     num_rows: 11548\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['source', 'body', 'subject', 'ann0', 'ann1', 'ann2', 'body_wcount', 'subj_wcount', 'cleaned_emails', '__index_level_0__'],\n",
              "     num_rows: 2888\n",
              " }))"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a subset of Dataset**"
      ],
      "metadata": {
        "id": "ofsPKKg_nxk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a subset of dataset\n",
        "train_dataset = Train_dataset.filter(lambda example, index: index % 3 == 0, with_indices=True)\n",
        "test_dataset = Train_dataset.filter(lambda example, index: index % 5 == 0, with_indices=True)\n",
        "\n",
        "train_dataset.shape, test_dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 99,
          "referenced_widgets": [
            "dd455412578c4d1d8f73454352f55dfe",
            "5b9825f5e02f4ee7b1193911a8b4302e",
            "db45f48e3f6a435f9c1033223c6583b3",
            "175c88a024e3442d873e1d32105c2a87",
            "70a0d4239ce54b7ca7b7be85e338ff6b",
            "ece5d0da870d4e9b9162aafe774e5cae",
            "a2d9e641e283465590b092012cde3416",
            "c763cb97ab6a47b9a79314d8f9d5f552",
            "79bd8be20fb94dde82bb1d229eb04079",
            "b65a430e82c7485c8285cd9cbfcef34a",
            "50f014f8e8844c2188dea3f6089c11f3",
            "724039c07d5543889f6dfbc9acba5963",
            "ac9fb8460d1c412d89ea717637d6f542",
            "ea78d56bce3045a2b1b29f62dda8b47c",
            "baa1e19cef524baaa1cec05b00dea6e6",
            "8cbb814d82ca400eb6a953d383c14a30",
            "92567892a44a471486080493c91ca457",
            "1e27f7c0f6a945f6bfe9c5f0b6967e60",
            "1d2cfedddba344699aba23b1d1e7815c",
            "f2a410fd369c4b2c824e731d8c632dab",
            "40d886f4a97d4dd18a12669b4f3cf034",
            "270698080fcc4436aa3f5e68fd5b9cd4"
          ]
        },
        "id": "-iDTgW2UtjIT",
        "outputId": "fb2251cc-0db4-4637-9119-54fe2b38d2ea"
      },
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/11548 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd455412578c4d1d8f73454352f55dfe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/11548 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "724039c07d5543889f6dfbc9acba5963"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((3850, 10), (2310, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "BP3r5bK_6H27",
        "outputId": "38382042-7804-4a7b-91c6-ecae6d418b92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['source', 'body', 'subject', 'ann0', 'ann1', 'ann2', 'body_wcount', 'subj_wcount', 'cleaned_emails', '__index_level_0__'],\n",
              "     num_rows: 3850\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['source', 'body', 'subject', 'ann0', 'ann1', 'ann2', 'body_wcount', 'subj_wcount', 'cleaned_emails', '__index_level_0__'],\n",
              "     num_rows: 2310\n",
              " }))"
            ]
          },
          "metadata": {},
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset['cleaned_emails'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "nghT9hdrfoYJ",
        "outputId": "d5b4c425-7c7a-41d7-c99e-e865ad6218ce"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'met early this morning with jack steward pres. of cma and keith mccrea lawyer for cma to follow up on the meeting that steve and i had with them regarding the proposal ken lay was pushing last week they like taking tx out of the mou they like getting dwr out of the power purchase business and they think that edisons mou needs a little haircut. mccrea pushed back initially on corenoncore why should it be mandatory to go the market but after much discussion they agreed that our idea for corenoncore has a lot of merit. our idea is that 1 the corenoncore would take effect in 1824 months when the markets settle and prices come down 2 everyone core and noncore should pay for dwrs past purchases since everyone consumed it 3 core customers should take iou generation qfs and dwr contracts which eliminates the cores net short position and 4 noncore would go to market and would not have to pay for any of dwrs going forward costs since core gets them in exchange for getting all the lowcost iou generation i told them we are still crunching the numbers to see what the rate effects would be and would share it with them when were done. while they are supportive of this approach and agreed that it would be useful to continue to flesh it out they are not convinced that it will get the necessary support to get passed. they are also withholding final judgment until they see all the numbers i.e. weve still got some work to do. they are very cynical about anything getting solved in california and are extremely upset about the pucs proposed rate decisions released by the judge and loretta lynch. they place very little trust in the governor or the legislature. interestingly the rate increase decisions coupled with calls in sacramento for an immediate move to corenoncore makes our proposal very attractive to cma. nonetheless they said that they will continue to engage and are willing to commit to continue meet to try to get californias mess resolved.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the pre-trained FLAN-T5 models and its tokenizer directly from HuggingFace. FLAN-T5 was released in the paper Scaling Instruction-Finetuned Language Models - it is an enhanced version of T5 that has been finetuned in a mixture of tasks. Setting torch_dtype=torch.bfloat16 specifies the memory type to be used by this model.**"
      ],
      "metadata": {
        "id": "qUwUyn2SowRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get size of models\n",
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"Trainable model parameters: {trainable_model_params}\\nAll model parameters: {all_model_params}\\nPercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n"
      ],
      "metadata": {
        "id": "0fsHEX6Bm3an"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Flan T5 Base Model**"
      ],
      "metadata": {
        "id": "GKeNgh0NpSaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flan_t5_base_original_model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)\n",
        "flan_t5_base_tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "print(\"For Model : FLAN-T5-Base \\n\",print_number_of_trainable_model_parameters(flan_t5_base_original_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ufBICyAosbw",
        "outputId": "c7f99706-e16c-4389-9fa4-4b3c66e6cd4d"
      },
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Model : FLAN-T5-Base \n",
            " Trainable model parameters: 247577856\n",
            "All model parameters: 247577856\n",
            "Percentage of trainable model parameters: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Flan T5 Small Model**"
      ],
      "metadata": {
        "id": "JU6tvjt2pbJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flan_t5_small_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", torch_dtype=torch.bfloat16)\n",
        "# flan_t5_small_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "print(\"For Model : Flan-t5-small \\n\",print_number_of_trainable_model_parameters(flan_t5_small_model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpTzBk9jEv_y",
        "outputId": "c3a3f77f-68ab-4bd9-aa70-ff0b6e59fccd"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Model : Flan-t5-small \n",
            " Trainable model parameters: 76961152\n",
            "All model parameters: 76961152\n",
            "Percentage of trainable model parameters: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# flan-t5-base vs. flan-t5-small parameters\n",
        "247577856-76961152"
      ],
      "metadata": {
        "id": "iLGrsgJ2E35P",
        "outputId": "0e37f4d1-2cdd-4faf-a9e1-63fe74880acd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "170616704"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test the Model with Zero Shot Inferencing**"
      ],
      "metadata": {
        "id": "ilaYbjowrNVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using a pre-trained model from Hugging Face/sources - FLAN-T5-Base .**"
      ],
      "metadata": {
        "id": "IV3wSp9HRSxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 3\n",
        "\n",
        "email = test_dataset['cleaned_emails'][index]\n",
        "subject = test_dataset['subject'][index]\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Generate the subject line for the following email.\n",
        "\n",
        "Email:\n",
        "{email}\n",
        "\n",
        "Subject:\n",
        "\"\"\"\n",
        "\n",
        "inputs = flan_t5_base_tokenizer(prompt, return_tensors='pt', padding=\"max_length\")\n",
        "\n",
        "# Move input tensors to the same device as the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inputs = {k: v.to(flan_t5_base_original_model.device) for k, v in inputs.items()}\n",
        "\n",
        "\n",
        "output = flan_t5_base_tokenizer.decode(\n",
        "    flan_t5_base_original_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE ORIGINAL HUMAN ANNOTED SUBJECT LINE :\\n{subject}\\n')\n",
        "print(dash_line)\n",
        "print(f'FLAN-T5 MODEL GENERATION - ZERO SHOT SUBJECT LINE:\\n{output}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpNTTyqsvQS-",
        "outputId": "2c306c30-a9e0-4cc6-80f9-5fb7dac55149"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "\n",
            "Generate the subject line for the following email.\n",
            "\n",
            "Email:\n",
            "thank you is nice. but cash is even better. happy holidays from sbc we wanted to do more than just say thanks to our customers who view\n",
            "\n",
            "Subject:\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE ORIGINAL HUMAN ANNOTED SUBJECT LINE :\n",
            "Thank you is nice. But cash is even better.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "FLAN-T5 MODEL GENERATION - ZERO SHOT SUBJECT LINE:\n",
            "Thank you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using a pre-trained model from Hugging Face etc- FLAN-T5-SMALL .**"
      ],
      "metadata": {
        "id": "slm6KkVEF_xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = flan_t5_base_tokenizer(prompt, return_tensors='pt', padding=\"max_length\")\n",
        "\n",
        "# Move input tensors to the same device as the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "\n",
        "output = flan_t5_base_tokenizer.decode(\n",
        "    flan_t5_small_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE ORIGINAL HUMAN ANNOTED SUBJECT LINE :\\n{subject}\\n')\n",
        "print(dash_line)\n",
        "print(f'FLAN-T5-SMALL MODEL GENERATION - ZERO SHOT SUBJECT LINE:\\n{output}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYLYJUzZG_pd",
        "outputId": "0e10ea10-bae4-4d16-c1b7-531e21cfac21"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "\n",
            "Generate the subject line for the following email.\n",
            "\n",
            "Email:\n",
            "thank you is nice. but cash is even better. happy holidays from sbc we wanted to do more than just say thanks to our customers who view\n",
            "\n",
            "Subject:\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE ORIGINAL HUMAN ANNOTED SUBJECT LINE :\n",
            "Thank you is nice. But cash is even better.\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "FLAN-T5-SMALL MODEL GENERATION - ZERO SHOT SUBJECT LINE:\n",
            "Thank You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**By testing with various models with the zero shot inferencing, we can see that the model struggles to extract the same subject line compared to the baseline subject, but it does pull out some important information from the email which indicates the models can be fine-tuned to the task at hand.**"
      ],
      "metadata": {
        "id": "C7S2UVFV8ZND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Performing Full Fine-Tuning using FLAN-T5 BASE MODEL**"
      ],
      "metadata": {
        "id": "3kTvPTm2wFVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "# # Define special tokens\n",
        "# special_tokens = {\n",
        "#     \"additional_special_tokens\": [\"<EMAIL>\", \"</EMAIL>\"]\n",
        "# }\n",
        "\n",
        "# # Add special tokens to the tokenizer\n",
        "# tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "\n",
        "# Load the model\n",
        "# flan_t5_base_original_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)\n",
        "model =T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "# # Resize the model's embeddings to accommodate the new tokens\n",
        "# model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    start_prompt = 'Generate a subject line for the following email.\\n\\nEmail:'\n",
        "    end_prompt = '\\nSubject:\\n'\n",
        "    # inputs = [start_prompt + email + end_prompt for email in examples['body']]\n",
        "    inputs = [f\"Generate a subject line for the following email:\\n{email}\" for email in examples[\"body\"]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
        "\n",
        "    labels = tokenizer(examples['subject'], max_length=128, truncation=True, padding='max_length')\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n"
      ],
      "metadata": {
        "id": "bKNmBevDVfmG",
        "outputId": "13a31909-db01-491d-f21e-ea2afdb81fca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "42257f6d0fd34b00bd0ab24f1d0fee29",
            "be1c9784782b42c09e994574fba6f876",
            "10399410ea694b0291bf69ae480f5aca",
            "ab6cf5b6b39248c198b8939cb9e7fbc8",
            "3e651810d6f94031bad1f0ecbe1cf329",
            "bb594b58b5084236a533ae611e1de180",
            "f9feca956227499e8092763e71706680",
            "1084a3b9d0c7496da45f143fe2803349",
            "a2ea38454dc84443b0dc2554fb48b205",
            "1884d3e40563472c860454264fbc8d09",
            "480fd7eff02f43389cbf5b771a0d57b7"
          ]
        }
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3850 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42257f6d0fd34b00bd0ab24f1d0fee29"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4gF6D6Qq0Se",
        "outputId": "b0a9d3f4-9560-4ba0-97ee-7515b16e06ac"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 3850\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and test\n",
        "split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "train_tokenised_dataset = split_dataset['train']\n",
        "test_tokenised_dataset = split_dataset['test']\n",
        "\n",
        "split_dataset"
      ],
      "metadata": {
        "id": "DCfVV097WbZL",
        "outputId": "d0ac3555-3132-439c-b203-3747b66774e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 3080\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 770\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1e-4"
      ],
      "metadata": {
        "id": "7tg1CbgGNoPm",
        "outputId": "ca2efad2-7294-4890-ea17-12ecd082ad98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0001"
            ]
          },
          "metadata": {},
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=1e-4,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    # warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=2\n",
        ")\n",
        "\n",
        "# # Tweeking training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=\"./results\",\n",
        "#     evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n",
        "#     learning_rate=3e-5,\n",
        "#     per_device_train_batch_size=4,\n",
        "#     per_device_eval_batch_size=4,\n",
        "#     num_train_epochs=3,\n",
        "#     weight_decay=0.01,\n",
        "#     logging_dir=\"./logs\",\n",
        "#     logging_steps=10,\n",
        "#     save_strategy=\"epoch\",  # Save a checkpoint at the end of each epoch\n",
        "#     save_total_limit=2,       # Keep only the two best checkpoints\n",
        "#     load_best_model_at_end=True\n",
        "# )"
      ],
      "metadata": {
        "id": "ckExw6k-Wq50"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenised_dataset,\n",
        "    eval_dataset=test_tokenised_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Uo6VC5P6Wv1X",
        "outputId": "1f0effdf-b038-42a1-e197-4dde821d2016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2310' max='2310' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2310/2310 34:03, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.188300</td>\n",
              "      <td>0.189362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.113200</td>\n",
              "      <td>0.166975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.064600</td>\n",
              "      <td>0.175215</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2310, training_loss=0.7421157444194282, metrics={'train_runtime': 2044.5971, 'train_samples_per_second': 4.519, 'train_steps_per_second': 1.13, 'total_flos': 6327157936619520.0, 'train_loss': 0.7421157444194282, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model and tokenizer\n",
        "save_directory = \"./finetuned_flan_t5_base3\"\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)\n",
        "generation_config.save_pretrained(save_directory)\n",
        "\n",
        "\n",
        "# Zip the saved model files\n",
        "\n",
        "shutil.make_archive(base_name=\"finetuned_flan_t5_base3\", format=\"zip\", root_dir=save_directory)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "S6gF7_J8uzEy",
        "outputId": "1bb5405b-a3f4-46d7-c727-805e258150b9"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/finetuned_flan_t5_base3.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YkHH6LjWzje2"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_filename = \"finetuned_flan_t5_base3.zip\"\n",
        "#Download the zipped checkpoint\n",
        "files.download(zip_filename)"
      ],
      "metadata": {
        "id": "0dWlJYa9schB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate the Model Qualitatively (Human Evaluation)**\n",
        "For many GenAI applications, start by qualitatively assessing whether our model behaves as expected. For example, as shown below, check if the fine-tuned model can create a reasonable subject lines of the emails, unlike the original model."
      ],
      "metadata": {
        "id": "2Oeva8pZUxsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the Saved Finetuned Model and Tokenizer**"
      ],
      "metadata": {
        "id": "xxDoUanxvRov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip model checkpoint as it is zipped\n",
        "# !unzip finetuned_flan_t5_base.zip -d ./finetuned_flan_t5_base\n",
        "\n",
        "\n",
        "\n",
        "finetuned_model_path = \"./finetuned_flan_t5_base3\"\n",
        "finetuned_tokenizer = AutoTokenizer.from_pretrained(finetuned_model_path)\n",
        "finetuned_model = AutoModelForSeq2SeqLM.from_pretrained(finetuned_model_path, torch_dtype=torch.bfloat16)\n",
        "generation_config = GenerationConfig.from_pretrained(finetuned_model_path)"
      ],
      "metadata": {
        "id": "XJd20rDqvIM4"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email = train_df['cleaned_emails'][index]\n",
        "subject = train_df['subject'][index]\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Generate a subject line for the following email.\n",
        "\n",
        "Email:\n",
        "{email}\n",
        "\n",
        "Subject:\n",
        "\"\"\"\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "id": "6PioIzXkwOSd",
        "outputId": "62579cc1-d15d-4c33-8c0d-f5ec900b899d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nGenerate a subject line for the following email.\\n\\nEmail:\\nwill here is a list of the top items we need to work on to improve the position and pl reporting for the west desk. my underlying goal is to create position managers and pl reports that represent all the risk held by the desk and estimate pl with great accuracy. lets try and schedule a meeting for this wednesday to go over the items above. phillip\\n\\nSubject:\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 239
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = 10\n",
        "\n",
        "email = test_df['cleaned_emails'][index]\n",
        "subject = test_df['subject'][index]\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Generate a subject line for the following email.\n",
        "\n",
        "Email:\n",
        "{email}\n",
        "\n",
        "Subject:\n",
        "\"\"\"\n",
        "\n",
        "inputs = flan_t5_base_tokenizer(prompt, return_tensors='pt', padding=\"max_length\")\n",
        "\n",
        "# Move input tensors to the same device as the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inputs = {k: v.to(flan_t5_base_original_model.device) for k, v in inputs.items()}\n",
        "\n",
        "\n",
        "\n",
        "flan_t5_base_output = flan_t5_base_tokenizer.decode(\n",
        "    flan_t5_base_original_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "# Move the finetuned model to the same device as the inputs\n",
        "finetuned_model = finetuned_model.to(device)\n",
        "\n",
        "finetuned_flan_t5_base_output = flan_t5_base_tokenizer.decode(\n",
        "    finetuned_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE ORIGINAL HUMAN ANNOTED SUBJECT LINE :\\n{subject}\\n')\n",
        "print(dash_line)\n",
        "print(f'FLAN-T5 MODEL - ZERO SHOT SUBJECT LINE:\\n{flan_t5_base_output}')\n",
        "print(dash_line)\n",
        "print(f'FINETUNED FLAN-T5 MODEL GENERATION:\\n{finetuned_flan_t5_base_output}')"
      ],
      "metadata": {
        "id": "2HNshhSYT6qD",
        "outputId": "cfe890f4-1edf-424c-efd4-46ed93d98ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        }
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "10",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3653\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 10",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-240-ed2e4b3bb55a>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0memail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_emails'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msubject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3654\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3655\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3656\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3657\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 10"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate the Flan T5-Base Model Quantitatively (with ROUGE Metric)**\n",
        "The ROUGE metric helps quantify the validity of subject lines produced by models. It compares subjects to a \"Annoted baseline\" subject which is usually created by a human. While not perfect, it does indicate the overall increase in subject line generatiion effectiveness that we have accomplished by fine-tuning."
      ],
      "metadata": {
        "id": "os-472avsQlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emails_for_score_df = train_df.sample(n=8, random_state=52)\n",
        "emails_for_score_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "pMYOOca77uGV",
        "outputId": "54f8f76e-e0c2-40bf-dd40-37cccc282458"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                        source  \\\n",
              "6241          lay-k_inbox_1729   \n",
              "7307          mann-k_sent_3102   \n",
              "12349         staab-t_inbox_27   \n",
              "9864          sager-e_sent_884   \n",
              "11184   shackleton-s_sent_1984   \n",
              "40           arnold-j_inbox_79   \n",
              "11362    shackleton-s_sent_469   \n",
              "9066   perlingiere-d_sent_2022   \n",
              "\n",
              "                                                    body  \\\n",
              "6241   Ken, I've left word with Rosalee, but I know y...   \n",
              "7307   www.dagger.com.... and look at the Products, R...   \n",
              "12349  logon : afellin password: htownbaby!\\nTheresa ...   \n",
              "9864   ATTORNEY WORK PRODUCT: NOT DISCOVERABLE   Purs...   \n",
              "11184  Are you still working every other day?\\nPlease...   \n",
              "40     Friends:  The NYMEX will be open for a normal ...   \n",
              "11362  Lucy Ortiz on the confirmation desk is awaitin...   \n",
              "9066   Veronica,  Relating to our previous conversati...   \n",
              "\n",
              "                                                 subject  ann0  ann1  ann2  \\\n",
              "6241                                 Spencer Stuart Help   NaN   NaN   NaN   \n",
              "7307                                          more kayak   NaN   NaN   NaN   \n",
              "12349                                           password   NaN   NaN   NaN   \n",
              "9864   TVA - Termination of MOPA and Related Dec. Action   NaN   NaN   NaN   \n",
              "11184                                               SITA   NaN   NaN   NaN   \n",
              "40                                   NYMEX Holiday Hours   NaN   NaN   NaN   \n",
              "11362                            Catalytica spark spread   NaN   NaN   NaN   \n",
              "9066           Duke Energy Marketing Limited Partnership   NaN   NaN   NaN   \n",
              "\n",
              "       body_wcount  subj_wcount  \\\n",
              "6241            74            3   \n",
              "7307           122            2   \n",
              "12349           32            1   \n",
              "9864           103            9   \n",
              "11184           27            1   \n",
              "40              59            3   \n",
              "11362           35            3   \n",
              "9066            33            5   \n",
              "\n",
              "                                          cleaned_emails  \n",
              "6241   ken ive left word with rosalee but i know you ...  \n",
              "7307   www.dagger.com.... and look at the products re...  \n",
              "12349  logon afellin password htownbaby theresa can y...  \n",
              "9864   attorney work product not discoverable pursuan...  \n",
              "11184  are you still working every other day please s...  \n",
              "40     friends the nymex will be open for a normal tr...  \n",
              "11362  lucy ortiz on the confirmation desk is awaitin...  \n",
              "9066   veronica relating to our previous conversation...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4aa2b57f-75d4-4b35-99c5-1e28bbea446d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>body</th>\n",
              "      <th>subject</th>\n",
              "      <th>ann0</th>\n",
              "      <th>ann1</th>\n",
              "      <th>ann2</th>\n",
              "      <th>body_wcount</th>\n",
              "      <th>subj_wcount</th>\n",
              "      <th>cleaned_emails</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6241</th>\n",
              "      <td>lay-k_inbox_1729</td>\n",
              "      <td>Ken, I've left word with Rosalee, but I know y...</td>\n",
              "      <td>Spencer Stuart Help</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>74</td>\n",
              "      <td>3</td>\n",
              "      <td>ken ive left word with rosalee but i know you ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7307</th>\n",
              "      <td>mann-k_sent_3102</td>\n",
              "      <td>www.dagger.com.... and look at the Products, R...</td>\n",
              "      <td>more kayak</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>122</td>\n",
              "      <td>2</td>\n",
              "      <td>www.dagger.com.... and look at the products re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12349</th>\n",
              "      <td>staab-t_inbox_27</td>\n",
              "      <td>logon : afellin password: htownbaby!\\nTheresa ...</td>\n",
              "      <td>password</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "      <td>logon afellin password htownbaby theresa can y...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9864</th>\n",
              "      <td>sager-e_sent_884</td>\n",
              "      <td>ATTORNEY WORK PRODUCT: NOT DISCOVERABLE   Purs...</td>\n",
              "      <td>TVA - Termination of MOPA and Related Dec. Action</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>103</td>\n",
              "      <td>9</td>\n",
              "      <td>attorney work product not discoverable pursuan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11184</th>\n",
              "      <td>shackleton-s_sent_1984</td>\n",
              "      <td>Are you still working every other day?\\nPlease...</td>\n",
              "      <td>SITA</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "      <td>are you still working every other day please s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>arnold-j_inbox_79</td>\n",
              "      <td>Friends:  The NYMEX will be open for a normal ...</td>\n",
              "      <td>NYMEX Holiday Hours</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>59</td>\n",
              "      <td>3</td>\n",
              "      <td>friends the nymex will be open for a normal tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11362</th>\n",
              "      <td>shackleton-s_sent_469</td>\n",
              "      <td>Lucy Ortiz on the confirmation desk is awaitin...</td>\n",
              "      <td>Catalytica spark spread</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35</td>\n",
              "      <td>3</td>\n",
              "      <td>lucy ortiz on the confirmation desk is awaitin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9066</th>\n",
              "      <td>perlingiere-d_sent_2022</td>\n",
              "      <td>Veronica,  Relating to our previous conversati...</td>\n",
              "      <td>Duke Energy Marketing Limited Partnership</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>33</td>\n",
              "      <td>5</td>\n",
              "      <td>veronica relating to our previous conversation...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4aa2b57f-75d4-4b35-99c5-1e28bbea446d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4aa2b57f-75d4-4b35-99c5-1e28bbea446d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4aa2b57f-75d4-4b35-99c5-1e28bbea446d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a483822a-ae9c-4bfc-b625-1159a04485e2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a483822a-ae9c-4bfc-b625-1159a04485e2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a483822a-ae9c-4bfc-b625-1159a04485e2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_5f3003eb-b690-4611-be97-12f430db4e10\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('emails_for_score_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5f3003eb-b690-4611-be97-12f430db4e10 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('emails_for_score_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "emails_for_score_df",
              "summary": "{\n  \"name\": \"emails_for_score_df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"mann-k_sent_3102\",\n          \"arnold-j_inbox_79\",\n          \"lay-k_inbox_1729\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"body\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"www.dagger.com.... and look at the Products, Recreational Kayaks section.\\nI am selling my Bayou II kayak by Dagger.\\nIt is a two person, enclosed  style, kayak perfect for enjoyable kayaking with a special friend or family  member.\\nEspecially good for fishing because one person can paddle and  position the boat while the other fishes.\\nFast and extremely quiet!!!\\n$749 Retail(camo $50 extra), will sell for $ 550 2 person, tandem polyethylene outer layer camoflage color wide cockpit and very stable bow seat slides and locks for single person use padded seats with adjustable back rests bow deck rigging with recessed fittings carry toggles(handles) includes bow and stern inflatable flotation inserts weighs 65 lbs 13 ' 6 \\\" long Max Load 450 lbs.\",\n          \"Friends:  The NYMEX will be open for a normal trading day today, closing at 2:30 p.m. EST.\\nThe market will remain closed Monday December 24 and Tuesday December 25.\\nNYMEX ACCESS will reopen the evening of the 25th at 7:00 p.m.\\nThe Exchange will reopen Wednesday morning the 26th at 10:00 a.m. EST.\\nHappy Holidays!!\\nBNP PARIBAS Commodity Futures\",\n          \"Ken, I've left word with Rosalee, but I know you are fighting a battle with no let up.\\nI want to communicate our best wishes and offer of support.\\nIf there is anything Spencer Stuart can do to help you in this period of crisis, please understand you have access to the entire Houston Office at no fee.\\nGood luck.\\nPlease call if there is anything we can do to support you.\\nWarmest regards,\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"more kayak\",\n          \"NYMEX Holiday Hours\",\n          \"Spencer Stuart Help\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ann0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ann1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ann2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"body_wcount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 36,\n        \"min\": 27,\n        \"max\": 122,\n        \"num_unique_values\": 8,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subj_wcount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 9,\n        \"num_unique_values\": 5,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_emails\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 8,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emails = emails_for_score_df['body']\n",
        "original_subjects = emails_for_score_df['subject']\n",
        "emails\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6FaMiCNtOgs",
        "outputId": "df44a582-43f4-4b58-fb06-615a0e49ef32"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10998    Carol:  I spoke with Richard Peterson (retail ...\n",
              "5904     The NYMEX Board of Directors met yesterday and...\n",
              "14352    Hello Paul - per our conversation yesterday, w...\n",
              "10067    We are please to announce the addition of John...\n",
              "11546    Anna:  I know that you have been negotiating w...\n",
              "11627    Terry and Dee:  I am resending the draft ISDA ...\n",
              "7296     Do not take this too seriously until we talk t...\n",
              "1025     eSource Presents Northern Light Training  Intr...\n",
              "14029    Mark, I have forwarded the invoice to the appr...\n",
              "8342     Gerald,  As discussed, insert the confidential...\n",
              "Name: body, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mn8SNYNUBLbF",
        "outputId": "11d4903c-98fd-4875-d330-7473c08ec9e3"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 770\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = 10\n",
        "\n",
        "email = train_df['body'][index]\n",
        "subject = train_df['subject'][index]\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Generate the subject line for the following email.\n",
        "\n",
        "Email:\n",
        "{email}\n",
        "\n",
        "Subject:\n",
        "\"\"\"\n",
        "\n",
        "inputs = flan_t5_base_tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "# Move input tensors to the same device as the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inputs = {k: v.to(flan_t5_base_original_model.device) for k, v in inputs.items()}\n",
        "\n",
        "\n",
        "output = flan_t5_base_tokenizer.decode(\n",
        "    finetuned_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE ORIGINAL HUMAN ANNOTED SUBJECT LINE :\\n{subject}\\n')\n",
        "print(dash_line)\n",
        "print(f'FLAN-T5 MODEL GENERATION - ZERO SHOT SUBJECT LINE:\\n{output}')"
      ],
      "metadata": {
        "id": "v-ntsCX41Iph",
        "outputId": "d3ba99d5-2577-4553-ee4e-d78c9b9c87c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------------------------------------------------------------------------\n",
            "INPUT PROMPT:\n",
            "\n",
            "Generate the subject line for the following email.\n",
            "\n",
            "Email:\n",
            "Will,   Here is a list of the top items we need to work on to improve the position  and p&l reporting for the west desk.\n",
            "My underlying goal is to create position managers and p&l reports that  represent all the risk held by the desk and estimate p&l with great accuracy.\n",
            "Let's try and schedule a meeting for this Wednesday to go over the items  above.\n",
            "Phillip\n",
            "\n",
            "Subject:\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "BASELINE ORIGINAL HUMAN ANNOTED SUBJECT LINE :\n",
            "Priority List\n",
            "\n",
            "---------------------------------------------------------------------------------------------------\n",
            "FLAN-T5 MODEL GENERATION - ZERO SHOT SUBJECT LINE:\n",
            ",   sur  est s a b c d e f r div pripriprilprixpriyprispriarchprileprimpricomprimontpricampriispripremmicommxstressisxyysx-y -x _x__sexyxs--xxxbyxxis-st-in-tempxxxy-ascxiesxi-stasenistrexpolixistxenxlyy_-_auxxilisexxtrestaisproxficxlixtylyxesuxXxoxz-by_procexitsxoxyxaxustaxprozxicyxnewxpozxicixinxforximxgraxitxfxsta\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "original_subjects = []\n",
        "flan_T5_subjects = []\n",
        "finetuned_flan_T5_subjects = []\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Determine if a GPU is available\n",
        "flan_t5_base_original_model.to(device) # Move the original model to the device\n",
        "finetuned_model.to(device) # Move the fine-tuned model to the device\n",
        "\n",
        "for index, row in emails_for_score_df.iterrows():\n",
        "    email = row['cleaned_emails']\n",
        "    original_subject = row['subject']\n",
        "    prompt = f\"\"\"\n",
        "    Generate a subject line for the following email.\n",
        "\n",
        "    Email:\n",
        "    {email}\n",
        "\n",
        "    Subject:\n",
        "    \"\"\"\n",
        "    original_model_input_ids = flan_t5_base_tokenizer(prompt, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
        "    finetuned_model_input_ids = finetuned_tokenizer(prompt, truncation=True, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "\n",
        "    original_model_outputs = flan_t5_base_original_model.generate(input_ids=original_model_input_ids, generation_config=generation_config)\n",
        "    original_model_text_output = flan_t5_base_tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "    flan_T5_subjects.append(original_model_text_output)\n",
        "\n",
        "    instruct_model_outputs = finetuned_model.generate(input_ids=finetuned_model_input_ids, generation_config=generation_config)\n",
        "    instruct_model_text_output = flan_t5_base_tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "    finetuned_flan_T5_subjects.append(instruct_model_text_output)\n",
        "\n",
        "    original_subjects.append(original_subject)\n",
        "\n",
        "zipped_subjects = list(zip(original_subjects, flan_T5_subjects, finetuned_flan_T5_subjects))\n",
        "\n",
        "Flan_T5_df = pd.DataFrame(zipped_subjects, columns = ['original_subjects', 'T5_subjects', 'finetuned_flan_T5_subjects'])\n",
        "Flan_T5_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "0SUG4spPuGPQ",
        "outputId": "5b059867-d46e-4f9d-82e6-8663b4886b4c"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                   original_subjects  \\\n",
              "0                                           Flying J   \n",
              "1                           Update on Brent Contract   \n",
              "2                                          ProCaribe   \n",
              "3       Risk Management addition: John Postlethwaite   \n",
              "4                         Enron Credit Inc.  (\"ECI\")   \n",
              "5                                 ISDA w/Enron Corp.   \n",
              "6  Secure Website for Enron/GE Turbine Contract N...   \n",
              "7   Northern Light and World Markets Energy Training   \n",
              "8                                 Update - Corestaff   \n",
              "9                                        Oakhill LOI   \n",
              "\n",
              "                                         T5_subjects  \\\n",
              "0                                                      \n",
              "1                                    and ). en e s o   \n",
              "2                                                in    \n",
              "3                                                      \n",
              "4              ---s-||s|/s//*/24>rab>bi>rb'ca nrr;rs   \n",
              "5                                   a... - y fro & c   \n",
              "6  -...the-to-d-----d---s-in-Da-f-*oo-oeedo---oh-...   \n",
              "7                       and a & ] - based s ).  b e    \n",
              "8                                                      \n",
              "9                                               'd).   \n",
              "\n",
              "                          finetuned_flan_T5_subjects  \n",
              "0  & %... / a b c d s m n r'  * _  ­ â - rap ) : — –  \n",
              "1  and... & - – — ) : a b c d m n o s r  â ­. _ _...  \n",
              "2  ... ,  ­ s & n r'- _. rap / en % f a b c ° fic...  \n",
              "3       ...   [ ], _ ) : s ­ â  ; n­­_­r &­â­­2-­12­  \n",
              "4  and resulting &... NE INTE LO RES IM HE haz DR...  \n",
              "5  and & NE ARE INTE COM EAR LO AP SEA ROW LAND R...  \n",
              "6  ...   [ ] r s,  & ; / ) re RC RR SU Y NE ART U...  \n",
              "7            re   de... ]  ­ – â­­­ââ2îi-àéislâîé éâ  \n",
              "8  resulting   [ ], ... n ­ s m f r c d a brrccfo...  \n",
              "9  re,  & % ]... ­ â _'s : f ; r - soci d a b c o...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bf3387eb-b788-471a-b383-46ddcefb9e78\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original_subjects</th>\n",
              "      <th>T5_subjects</th>\n",
              "      <th>finetuned_flan_T5_subjects</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Flying J</td>\n",
              "      <td></td>\n",
              "      <td>&amp; %... / a b c d s m n r'  * _  ­ â - rap ) : — –</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Update on Brent Contract</td>\n",
              "      <td>and ). en e s o</td>\n",
              "      <td>and... &amp; - – — ) : a b c d m n o s r  â ­. _ _...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ProCaribe</td>\n",
              "      <td>in</td>\n",
              "      <td>... ,  ­ s &amp; n r'- _. rap / en % f a b c ° fic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Risk Management addition: John Postlethwaite</td>\n",
              "      <td></td>\n",
              "      <td>...   [ ], _ ) : s ­ â  ; n­­_­r &amp;­â­­2-­12­</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Enron Credit Inc.  (\"ECI\")</td>\n",
              "      <td>---s-||s|/s//*/24&gt;rab&gt;bi&gt;rb'ca nrr;rs</td>\n",
              "      <td>and resulting &amp;... NE INTE LO RES IM HE haz DR...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ISDA w/Enron Corp.</td>\n",
              "      <td>a... - y fro &amp; c</td>\n",
              "      <td>and &amp; NE ARE INTE COM EAR LO AP SEA ROW LAND R...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Secure Website for Enron/GE Turbine Contract N...</td>\n",
              "      <td>-...the-to-d-----d---s-in-Da-f-*oo-oeedo---oh-...</td>\n",
              "      <td>...   [ ] r s,  &amp; ; / ) re RC RR SU Y NE ART U...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Northern Light and World Markets Energy Training</td>\n",
              "      <td>and a &amp; ] - based s ).  b e</td>\n",
              "      <td>re   de... ]  ­ – â­­­ââ2îi-àéislâîé éâ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Update - Corestaff</td>\n",
              "      <td></td>\n",
              "      <td>resulting   [ ], ... n ­ s m f r c d a brrccfo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Oakhill LOI</td>\n",
              "      <td>'d).</td>\n",
              "      <td>re,  &amp; % ]... ­ â _'s : f ; r - soci d a b c o...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf3387eb-b788-471a-b383-46ddcefb9e78')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bf3387eb-b788-471a-b383-46ddcefb9e78 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bf3387eb-b788-471a-b383-46ddcefb9e78');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d45ceea4-0107-4acd-ad79-ef5f5b0cd556\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d45ceea4-0107-4acd-ad79-ef5f5b0cd556')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d45ceea4-0107-4acd-ad79-ef5f5b0cd556 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_423b2b4a-2232-4b83-ba27-a587ed1f5f06\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('Flan_T5_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_423b2b4a-2232-4b83-ba27-a587ed1f5f06 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('Flan_T5_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "Flan_T5_df",
              "summary": "{\n  \"name\": \"Flan_T5_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"original_subjects\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Update - Corestaff\",\n          \"Update on Brent Contract\",\n          \"ISDA w/Enron Corp.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"T5_subjects\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"and a & ] - based s ).  b e \",\n          \"and ). en e s o\",\n          \"a... - y fro & c\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"finetuned_flan_T5_subjects\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"resulting   [ ], ... n \\u00ad s m f r c d a brrccforlc_byl _ \",\n          \"and... & - \\u2013 \\u2014 ) : a b c d m n o s r  \\u00e2 \\u00ad. _ _________\",\n          \"and & NE ARE INTE COM EAR LO AP SEA ROW LAND RI AW SF IR SU FERRM Arr X Y Pri PRI \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=flan_T5_subjects,\n",
        "    references=original_subjects,\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "finetuned_model_results = rouge.compute(\n",
        "    predictions=finetuned_flan_T5_subjects,\n",
        "    references=original_subjects,\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('FLAN T5 Base Model:')\n",
        "print(original_model_results)\n",
        "print('\\nFINETUNED FLAN T5 BASE MODEL:')\n",
        "print(finetuned_model_results)"
      ],
      "metadata": {
        "id": "LnuLg7e_1_Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the 'punkt' resource\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('wordnet') # Download the WordNet corpus"
      ],
      "metadata": {
        "id": "vTP9c9DuNXhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "# Evaluate the generated subject lines\n",
        "def evaluate_metrics(true_subject, generated_subject):\n",
        "    # ROUGE-L\n",
        "    rougeL = scorer.score(true_subject, generated_subject)['rougeL'].fmeasure\n",
        "\n",
        "    # METEOR\n",
        "    meteor = meteor_score([word_tokenize(true_subject)], word_tokenize(generated_subject))\n",
        "\n",
        "    # SACREBLEU\n",
        "    sacre_bleu = sacrebleu.corpus_bleu([generated_subject], [[true_subject]]).score\n",
        "\n",
        "    return rougeL, meteor, sacre_bleu\n",
        "\n",
        "Flan_T5_df['rougeL'], Flan_T5_df['meteor'], Flan_T5_df['sacrebleu'] = zip(*Flan_T5_df.apply(\n",
        "    lambda row: evaluate_metrics(row['original_subjects'], row['T5_subjects']), axis=1))\n",
        "\n",
        "# Calculate average scores\n",
        "average_rougeL = Flan_T5_df['rougeL'].mean()\n",
        "average_meteor = Flan_T5_df['meteor'].mean()\n",
        "average_sacrebleu = Flan_T5_df['sacrebleu'].mean()\n",
        "\n",
        "print(f'Average ROUGE-L: {average_rougeL:.4f}')\n",
        "print(f'Average METEOR: {average_meteor:.4f}')\n",
        "print(f'Average SACREBLEU: {average_sacrebleu:.4f}')"
      ],
      "metadata": {
        "id": "xFBuePhiLDcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1_dWxlbZQCTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#T5 Flan Base\n",
        "# Load the ROUGE metric\n",
        "rouge_metric = load_metric('rouge')\n",
        "\n",
        "def calculate_rouge_scores(predictions, references):\n",
        "    rouge_metric.add_batch(predictions=predictions, references=references)\n",
        "    result = rouge_metric.compute()\n",
        "    return {\n",
        "        'rouge1': result['rouge1'].mid.fmeasure * 100,\n",
        "        'rouge2': result['rouge2'].mid.fmeasure * 100,\n",
        "        'rougeL': result['rougeL'].mid.fmeasure * 100\n",
        "    }\n",
        "\n",
        "# Generate predictions and calculate ROUGE scores\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "for example in test_dataset:\n",
        "    input_ids = example['input_ids']\n",
        "    input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)  # Add batch dimension\n",
        "    outputs = model.generate(input_ids)\n",
        "    pred = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    ref = tokenizer.decode(example['labels'], skip_special_tokens=True)\n",
        "\n",
        "    predictions.append(pred)\n",
        "    references.append(ref)\n",
        "\n",
        "# Calculate and print ROUGE scores for each test sample\n",
        "rouge_scores = []\n",
        "for pred, ref in zip(predictions, references):\n",
        "    score = calculate_rouge_scores([pred], [ref])\n",
        "    rouge_scores.append(score)\n",
        "    print(f\"Prediction: {pred}\\nReference: {ref}\\nROUGE Scores: {score}\\n\")\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "rouge_df = pd.DataFrame(rouge_scores)\n",
        "rouge_df"
      ],
      "metadata": {
        "id": "vQ-w7TxGZEV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flan_T5_subjects = []\n",
        "finetuned_flan_T5_subjects = []\n",
        "\n",
        "for _, email in enumerate(emails_for_score):\n",
        "    prompt = f\"\"\"\n",
        "    Generate the subject line for the following email.\n",
        "\n",
        "    Email:\n",
        "    {email}\n",
        "\n",
        "    Subject:\n",
        "    \"\"\"\n",
        "    # Tokenize the input and move to the same device as the model\n",
        "    inputs = tokenizer(prompt, return_tensors='pt').to(flan_T5_small_model_finetuned.device) # Move inputs to the same device as the fine-tuned model\n",
        "\n",
        "    # Add the decoder_start_token_id to the generation configuration\n",
        "    generation_config = GenerationConfig(max_new_tokens=200,\n",
        "        decoder_start_token_id=tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.eos_token_id)\n",
        "\n",
        "\n",
        "\n",
        "    # Generate subject using the original model\n",
        "    # Make sure flan_t5_small_model is on the same device as flan_T5_small_model_finetuned\n",
        "    flan_t5_small_model = model.to(flan_T5_small_model_finetuned.device)\n",
        "    original_model_outputs = flan_t5_small_model.generate(**inputs, generation_config=generation_config)\n",
        "    original_model_subject_output = flan_t5_small_tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "    flan_T5small_subjects.append(original_model_subject_output)\n",
        "\n",
        "    # Generate subject using the finetuned model\n",
        "    trained_flan_t5_outputs = flan_T5_small_model_finetuned.generate(**inputs, generation_config=generation_config)\n",
        "    instruct_model_text_output = flan_T5_small_tokenizer.decode(trained_flan_t5_outputs[0], skip_special_tokens=True)\n",
        "    finetuned_flan_T5small_subjects.append(instruct_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(original_subjects, flan_T5small_subjects, finetuned_flan_T5small_subjects))\n"
      ],
      "metadata": {
        "id": "Nx2ALVXjr_cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)"
      ],
      "metadata": {
        "id": "rubayi3ZdWSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_test_df = test_df.sample(n=10, random_state=42)"
      ],
      "metadata": {
        "id": "SHpw9jHsefXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_test_df"
      ],
      "metadata": {
        "id": "258hkqQlevCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_subject_line(email_body):\n",
        "    inputs = tokenizer(\"Generate the subject line for the following email.: \" + email_body, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    # Move inputs to the same device as the model\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    # Access input_ids as a key in the dictionary\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "Hw-_IgKYdlkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_test_df['generated_subject_line'] = small_test_df['cleaned_emails'].apply(generate_subject_line)\n",
        "small_test_df"
      ],
      "metadata": {
        "id": "HPLoCJgffW7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "\n",
        "# Evaluate the generated subject lines\n",
        "def evaluate_metrics(true_subject, generated_subject):\n",
        "    # Tokenize the input for METEOR\n",
        "    true_subject_tokens = nltk.word_tokenize(true_subject)\n",
        "    generated_subject_tokens = nltk.word_tokenize(generated_subject)\n",
        "\n",
        "    # ROUGE-L\n",
        "    rougeL = scorer.score(true_subject, generated_subject)['rougeL'].fmeasure\n",
        "\n",
        "    # METEOR\n",
        "    meteor = meteor_score([true_subject_tokens], generated_subject_tokens) # Pass tokenized input\n",
        "\n",
        "    # SACREBLEU - Use import sacrebleu directly\n",
        "    sacre_bleu_score = sacrebleu.corpus_bleu([generated_subject], [[true_subject]]).score\n",
        "\n",
        "    return rougeL, meteor, sacre_bleu_score # Return the score, not the module\n",
        "\n",
        "test_df['rougeL'], test_df['meteor'], test_df['sacrebleu'] = zip(*test_df.apply(\n",
        "    lambda row: evaluate_metrics(row['subject'], row['generated_subject_line']), axis=1))"
      ],
      "metadata": {
        "id": "fjUdAhycdLvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")"
      ],
      "metadata": {
        "id": "tAUYMLeiWoQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_prompt = 'Generate the subject line for the following email.\\n\\nEmail:'\n",
        "end_prompt = '\\nSubject:\\n'\n",
        "start_prompt + email + end_prompt\n",
        "print(start_prompt)\n",
        "print(email)\n",
        "print(end_prompt)"
      ],
      "metadata": {
        "id": "hcU_iws1LLSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization for FLAN-T5 model\n",
        "def flan_t5_base_tokenize_function(example):\n",
        "    start_prompt = 'Generate the subject line for the following email.\\n\\nEmail:'\n",
        "    end_prompt = '\\nSubject:\\n'\n",
        "    prompt = [start_prompt + email + end_prompt for email in example[\"body\"]]\n",
        "    example['input_ids'] = flan_t5_base_tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    example['labels'] = flan_t5_base_tokenizer(example[\"subject\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    return example\n",
        "\n",
        "# The dataset actually contains 3 diff splits: train, validation, test.\n",
        "# The tokenize_function code is handling all data across all splits in batches.\n",
        "tokenized_train_datasets = train_dataset.map(flan_t5_base_tokenize_function, batched=True)\n",
        "tokenized_test_datasets = test_dataset.map(flan_t5_base_tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "id": "PNZhgan9fJ6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_train_datasets = tokenized_train_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
        "# tokenized_test_datasets = tokenized_test_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
      ],
      "metadata": {
        "id": "EkFTC1Mvr81h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_datasets, tokenized_test_datasets"
      ],
      "metadata": {
        "id": "lV8NjE0YrWaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FLAN-T5 BASE - Fine-Tune the Model with the Preprocessed Tokenised Dataset utiliing the built-in Hugging Face Trainer class. Then compare it with reference to the original model.**\n",
        "\n",
        "flan_t5_base_original_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)\n",
        "flan_t5_base_tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')"
      ],
      "metadata": {
        "id": "Q0tc44JYPNW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    start_prompt = 'Generate the subject line for the following email.\\n\\nEmail:'\n",
        "    end_prompt = '\\nSubject:\\n'\n",
        "    inputs = [start_prompt + email + end_prompt for email in examples['email_body']]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
        "\n",
        "    labels = tokenizer(examples['subject'], max_length=50, truncation=True, padding='max_length')\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n"
      ],
      "metadata": {
        "id": "H1GVZxyPU56L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Tokenization\n",
        "# def preprocess_data(examples):\n",
        "#     inputs = [\"generate subject line: \" + email for email in examples['cleaned_emails']]\n",
        "#     model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "#     with tokenizer.as_target_tokenizer():\n",
        "#         labels = tokenizer(examples['subject'], max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "#     return model_inputs\n",
        "\n",
        "# train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
        "# test_dataset = test_dataset.map(preprocess_data, batched=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=flan_t5_base_original_model ,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_datasets,\n",
        "    eval_dataset=tokenized_test_datasets,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "RV5TsUrfR1zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FLAN-T5 SMALL - Fine-Tune the Model with the Preprocessed Tokenised Dataset utiliing the built-in Hugging Face Trainer class. Then compare it with reference to the original model.\n",
        "\n",
        "flan_t5_small_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\", torch_dtype=torch.bfloat16)\n",
        "flan_t5_small_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")"
      ],
      "metadata": {
        "id": "YrHuhrEfsmdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization for FLAN-T5 SMALL model\n",
        "def flan_t5_small_tokenize_function(example):\n",
        "    start_prompt = 'Generate the subject line for the following email.\\n\\nEmail:'\n",
        "    end_prompt = '\\nSubject:\\n'\n",
        "    prompt = [start_prompt + email + end_prompt for email in example[\"cleaned_emails\"]]\n",
        "    example['input_ids'] = flan_t5_small_tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    example['labels'] = flan_t5_small_tokenizer(example[\"subject\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    return example\n",
        "\n",
        "# The dataset actually contains 3 diff splits: train, validation, test.\n",
        "# The tokenize_function code is handling all data across all splits in batches.\n",
        "tokenized_train_datasets = train_dataset.map(flan_t5_small_tokenize_function, batched=True)\n",
        "tokenized_test_datasets = test_dataset.map(flan_t5_small_tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "n15RWBkAXxzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=flan_t5_small_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_datasets,\n",
        "    eval_dataset=tokenized_test_datasets,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "GjJMAmUcYSh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model checkpoint\n",
        "flan_t5_small_model.save_pretrained('./results/Flan_T5_small_model')\n",
        "flan_t5_small_tokenizer.save_pretrained('./results/Flan_T5_small_model')\n",
        "\n",
        "# Define and save the generation configuration\n",
        "generation_config = GenerationConfig(\n",
        "    early_stopping=True,\n",
        "    num_beams=4,\n",
        "    no_repeat_ngram_size=3,\n",
        "    forced_bos_token_id=0,\n",
        "    forced_eos_token_id=2\n",
        ")\n",
        "\n",
        "generation_config.save_pretrained('./results/Flan_T5_small_model')"
      ],
      "metadata": {
        "id": "CcOfhXaGdmyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AutoModelForSeq2SeqLM\n",
        "AutoTokenizer"
      ],
      "metadata": {
        "id": "tESux8htfc_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained Flan T5 Small fine tuned model\n",
        "flan_T5_small_model_directory = '/content/results/Flan_T5_small_model'\n",
        "flan_T5_small_tokenizer = AutoTokenizer.from_pretrained(flan_T5_small_model_directory)\n",
        "flan_T5_small_model_finetuned = AutoModelForSeq2SeqLM.from_pretrained(flan_T5_small_model_directory)"
      ],
      "metadata": {
        "id": "PM4itxVveS3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emails_for_score = tokenized_test_datasets[0:10]['cleaned_emails']\n",
        "original_subjects = tokenized_test_datasets[0:10]['subject']"
      ],
      "metadata": {
        "id": "b9GGjgy6b0cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flan_T5small_subjects = []\n",
        "finetuned_flan_T5small_subjects = []\n",
        "\n",
        "for _, cleaned_emails in enumerate(emails_for_score):\n",
        "    prompt = f\"\"\"\n",
        "    Generate the subject line for the following email.\n",
        "\n",
        "    Email:\n",
        "    {email}\n",
        "\n",
        "    Subject:\n",
        "    \"\"\"\n",
        "    # Tokenize the input and move to the same device as the model\n",
        "    inputs = flan_t5_small_tokenizer(prompt, return_tensors='pt').to(flan_T5_small_model_finetuned.device) # Move inputs to the same device as the fine-tuned model\n",
        "\n",
        "    # Add the decoder_start_token_id to the generation configuration\n",
        "    generation_config = GenerationConfig(max_new_tokens=200,\n",
        "        decoder_start_token_id=flan_T5_small_tokenizer.bos_token_id if flan_T5_small_tokenizer.bos_token_id is not None else flan_T5_small_tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Generate subject using the original model\n",
        "    # Make sure flan_t5_small_model is on the same device as flan_T5_small_model_finetuned\n",
        "    flan_t5_small_model = flan_t5_small_model.to(flan_T5_small_model_finetuned.device)\n",
        "    original_model_outputs = flan_t5_small_model.generate(**inputs, generation_config=generation_config)\n",
        "    original_model_subject_output = flan_t5_small_tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "    flan_T5small_subjects.append(original_model_subject_output)\n",
        "\n",
        "    # Generate subject using the finetuned model\n",
        "    trained_flan_t5_outputs = flan_T5_small_model_finetuned.generate(**inputs, generation_config=generation_config)\n",
        "    instruct_model_text_output = flan_T5_small_tokenizer.decode(trained_flan_t5_outputs[0], skip_special_tokens=True)\n",
        "    finetuned_flan_T5small_subjects.append(instruct_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(original_subjects, flan_T5small_subjects, finetuned_flan_T5small_subjects))\n",
        "\n",
        "Flan_T5_Small_Subjects_df = pd.DataFrame(zipped_summaries, columns = ['Original_subjects', 'Flan_T5s_subjects', 'Finetuned_Flan_T5s_subjects'])\n",
        "Flan_T5_Small_Subjects_df"
      ],
      "metadata": {
        "id": "LXGOGtXUbh5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "# from datasets import Dataset\n",
        "\n",
        "\n",
        "# # Initialize tokenizer and model\n",
        "# tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "# T5_model = T5ForConditionalGeneration.from_pretrained('t5-small')"
      ],
      "metadata": {
        "id": "exqW5Kv0mzyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    start_prompt = 'Generate the subject line for the following email.\\n\\n'\n",
        "    end_prompt = '\\n\\Subject: '\n",
        "    prompt = [start_prompt + email + end_prompt for email in example[\"cleaned_emails\"]]\n",
        "    # Tokenize the input_ids - do not flatten\n",
        "    example['input_ids'] = [tokenizer(p, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids[0] for p in prompt]\n",
        "    # Tokenize the labels - do not flatten\n",
        "    example['labels'] = [tokenizer(s, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids[0] for s in example[\"subject\"]]\n",
        "\n",
        "    return example"
      ],
      "metadata": {
        "id": "aJmS7TCD4cQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output_dir = f'./email-Subject-training-{str(int(time.time()))}'\n",
        "\n",
        "# # Define training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=output_dir,\n",
        "#     learning_rate=1e-5,\n",
        "#     num_train_epochs=3,\n",
        "#     weight_decay=0.01,\n",
        "#     logging_steps=30,\n",
        "#     evaluation_strategy='epoch',\n",
        "#     per_device_train_batch_size=4,  # Reduced batch size\n",
        "#     # max_steps=1\n",
        "# )\n",
        "\n",
        "# # training_args = TrainingArguments(\n",
        "# #     output_dir='./results',\n",
        "# #     evaluation_strategy='epoch',\n",
        "# #     learning_rate=1e-5,\n",
        "# #     per_device_train_batch_size=4,\n",
        "# #     per_device_eval_batch_size=4,\n",
        "# #     num_train_epochs=3,\n",
        "# #     weight_decay=0.01,\n",
        "# # )\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=flan_t5_base_original_model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=tokenized_train_datasets,\n",
        "#     eval_dataset=tokenized_test_datasets\n",
        "# )\n",
        "\n",
        "# trainer.train()"
      ],
      "metadata": {
        "id": "ymjhoRaoshwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Generate subject lines for the test set\n",
        "# def generate_subject_line(email_body):\n",
        "#     inputs = tokenizer(\"generate subject line: \" + email_body, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "#     # Move inputs to the same device as the model\n",
        "#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "#     outputs = model.generate(inputs.input_ids, max_length=128, num_beams=4, early_stopping=True)\n",
        "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# test_df['generated_subject_line'] = test_df['cleaned_emails'].apply(generate_subject_line)\n",
        "\n",
        "# Generate subject lines for the test set\n",
        "def generate_subject_line(email_body):\n",
        "    inputs = tokenizer(\"generate subject line: \" + email_body, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    # Move inputs to the same device as the model\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    # Access input_ids as a key in the dictionary\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "test_df['generated_subject_line'] = test_df['cleaned_emails'].apply(generate_subject_line)"
      ],
      "metadata": {
        "id": "H00VaamVakII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **t5-small Evaluation**"
      ],
      "metadata": {
        "id": "cQ4bUC74cjcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)"
      ],
      "metadata": {
        "id": "XJ3D8o9yegTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the 'punkt' resource\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet') # Download WordNet\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "\n",
        "# Evaluate the generated subject lines\n",
        "def evaluate_metrics(true_subject, generated_subject):\n",
        "    # Tokenize the input for METEOR\n",
        "    true_subject_tokens = nltk.word_tokenize(true_subject)\n",
        "    generated_subject_tokens = nltk.word_tokenize(generated_subject)\n",
        "\n",
        "    # ROUGE-L\n",
        "    rougeL = scorer.score(true_subject, generated_subject)['rougeL'].fmeasure\n",
        "\n",
        "    # METEOR\n",
        "    meteor = meteor_score([true_subject_tokens], generated_subject_tokens) # Pass tokenized input\n",
        "\n",
        "    # SACREBLEU - Use import sacrebleu directly\n",
        "    sacre_bleu_score = sacrebleu.corpus_bleu([generated_subject], [[true_subject]]).score\n",
        "\n",
        "    return rougeL, meteor, sacre_bleu_score # Return the score, not the module\n",
        "\n",
        "test_df['rougeL'], test_df['meteor'], test_df['sacrebleu'] = zip(*test_df.apply(\n",
        "    lambda row: evaluate_metrics(row['subject'], row['generated_subject_line']), axis=1))\n",
        "\n",
        "# Calculate average scores\n",
        "average_rougeL = test_df['rougeL'].mean()\n",
        "average_meteor = test_df['meteor'].mean()\n",
        "average_sacrebleu = test_df['sacrebleu'].mean() # Now you should be able to calculate the mean\n",
        "\n",
        "print(f'Average ROUGE-L: {average_rougeL:.4f}')\n",
        "print(f'Average METEOR: {average_meteor:.4f}')\n",
        "print(f'Average SACREBLEU: {average_sacrebleu:.4f}')"
      ],
      "metadata": {
        "id": "t5C7gSX3ceB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pretrained('facebook/bart-base')**"
      ],
      "metadata": {
        "id": "nP7-BBpHezyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "K5lsH75xikZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert DataFrame to Dataset\n",
        "# train_dataset = Dataset.from_pandas(train_df)\n",
        "# test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "facebook_bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "facebook_bart_base_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
      ],
      "metadata": {
        "id": "W_UUmQ-O5tso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For Model : facebook_bart_base_model \\n\",print_number_of_trainable_model_parameters(facebook_bart_base_model))"
      ],
      "metadata": {
        "id": "HGVkLIIf52zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Generate the subject line for the following email.\n",
        "\n",
        "Email:\n",
        "{email}\n",
        "\n",
        "Subject:\n",
        "\"\"\"\n",
        "\n",
        "inputs = facebook_bart_tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "# Move input tensors to the same device as the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inputs = {k: v.to(facebook_bart_base_model.device) for k, v in inputs.items()}\n",
        "\n",
        "outputs = facebook_bart_base_model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE ORIGINAL HUMAN ANNOTED SUBJECT LINE :\\n{subject}\\n')\n",
        "print(dash_line)\n",
        "print(f'FB BART MODEL GENERATION - ZERO SHOT SUBJECT LINE:\\n{output}')"
      ],
      "metadata": {
        "id": "3p1txC8YLwi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenization\n",
        "def Tokenization_preprocess_data(examples):\n",
        "    inputs = [prompt + email for email in examples['cleaned_emails']]\n",
        "    model_inputs = facebook_bart_tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Use facebook_bart_tokenizer as the target tokenizer\n",
        "    with facebook_bart_tokenizer.as_target_tokenizer():\n",
        "        labels = facebook_bart_tokenizer(examples['subject'], max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "train_dataset = train_dataset.map(Tokenization_preprocess_data, batched=True)\n",
        "test_dataset = test_dataset.map(Tokenization_preprocess_data, batched=True)\n",
        "\n",
        "# # Define training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir='./results',\n",
        "#     evaluation_strategy='epoch',\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=4,\n",
        "#     per_device_eval_batch_size=4,\n",
        "#     num_train_epochs=3,\n",
        "#     weight_decay=0.01,\n",
        "# )\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',        # Directory to save the model checkpoints\n",
        "    evaluation_strategy='steps',\n",
        "    save_steps=500,               # Save checkpoint every 1000 steps\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=200,\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "facebook_bart_base_model_trainer = Trainer(\n",
        "    model=facebook_bart_base_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "facebook_bart_base_model_trainer.train()\n"
      ],
      "metadata": {
        "id": "UHfFS01Feyu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final model checkpoint\n",
        "facebook_bart_base_model.save_pretrained('./results/FB_Bart_model')\n",
        "facebook_bart_tokenizer.save_pretrained('./results/FB_Bart_model')\n",
        "\n",
        "# Define and save the generation configuration\n",
        "generation_config = GenerationConfig(\n",
        "    early_stopping=True,\n",
        "    num_beams=4,\n",
        "    no_repeat_ngram_size=3,\n",
        "    forced_bos_token_id=0,\n",
        "    forced_eos_token_id=2\n",
        ")\n",
        "\n",
        "generation_config.save_pretrained('./results/FB_Bart_model')"
      ],
      "metadata": {
        "id": "8Pj219LPCVmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content"
      ],
      "metadata": {
        "id": "wfBgXge85Fxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "logs_folder = '/content/logs'  # Adjust to your source folder\n",
        "logs_git_folder = './content/qM-AI-L/email-subject/model-tuning/logs'  # Adjust to your repository folder\n",
        "\n",
        "# Copy the folder\n",
        "shutil.copytree(logs_folder, logs_git_folder)"
      ],
      "metadata": {
        "id": "62ihDbDA_Dgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "results_folder = '/content/results'  # Adjust to your source folder\n",
        "results_git_folder = './content/qM-AI-L/email-subject/model-tuning/results'  # Adjust to your repository folder\n",
        "\n",
        "# Copy the folder\n",
        "shutil.copytree(results_folder, results_git_folder)"
      ],
      "metadata": {
        "id": "KA6-ZTV1AoJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to the cloned repository\n",
        "os.chdir('./content/qM-AI-L')  # Adjust to your repository folder\n",
        "\n",
        "# Configure Git (only needed once)\n",
        "!git config --global user.email \"mlreddy3082@gmail.com\"\n",
        "!git config --global user.name \"Mlr9459\"\n",
        "\n",
        "# Add, commit, and push changes\n",
        "!git add /content/qM-AI-L/email-subject/\n",
        "!git commit -m \"Add training outputs: model checkpoints, tokenizer, and generation configuration\"\n",
        "!git push origin master"
      ],
      "metadata": {
        "id": "qdXDrq7SCPm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the Model Quantitatively (with ROUGE Metric)\n",
        "The ROUGE metric) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning.**"
      ],
      "metadata": {
        "id": "BhjyD6Wyzxhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_datasets, tokenized_test_datasets"
      ],
      "metadata": {
        "id": "lKqFN4OX1yaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_datasets[0:10]['cleaned_emails']"
      ],
      "metadata": {
        "id": "m1tWpGFv2EcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emails_for_score = tokenized_test_datasets[0:10]['cleaned_emails']\n",
        "human_annoted_subjects = tokenized_test_datasets[0:10]['subject']\n"
      ],
      "metadata": {
        "id": "pqox7th015Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "facebook_bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "facebook_bart_base_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
      ],
      "metadata": {
        "id": "W1-e0Dgg4s71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_directory = '/content/results/FB_Bart_model'\n",
        "facebook_bart_tokenizer_trained = BartTokenizer.from_pretrained(model_directory)\n",
        "facebook_bart_base_model_trained = BartForConditionalGeneration.from_pretrained(model_directory)"
      ],
      "metadata": {
        "id": "gXnio11hELAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "facebook_bart_subjects = []\n",
        "trained_facebook_bart_subjects = []\n",
        "\n",
        "for _, email in enumerate(emails_for_score):\n",
        "    prompt = f\"\"\"\n",
        "    Generate the subject line for the following email.\n",
        "\n",
        "    Email:\n",
        "    {email}\n",
        "\n",
        "    Subject:\n",
        "    \"\"\"\n",
        "    # Tokenize the input and move to the same device as the model\n",
        "    inputs = facebook_bart_tokenizer(prompt, return_tensors='pt').to(facebook_bart_base_model.device)\n",
        "\n",
        "    # Add the decoder_start_token_id to the generation configuration\n",
        "    generation_config = GenerationConfig(\n",
        "    max_new_tokens=200,\n",
        "    decoder_start_token_id=facebook_bart_tokenizer.bos_token_id  # Add this line\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Generate subject using the original model\n",
        "    original_model_outputs = facebook_bart_base_model.generate(**inputs, generation_config=generation_config)\n",
        "    original_model_subject_output = facebook_bart_tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "    facebook_bart_subjects.append(original_model_subject_output)\n",
        "\n",
        "    trained_facebook_bart_outputs = facebook_bart_base_model_trained.generate(**inputs, generation_config=generation_config)\n",
        "    instruct_model_text_output = facebook_bart_tokenizer_trained.decode(trained_facebook_bart_outputs[0], skip_special_tokens=True)\n",
        "    trained_facebook_bart_subjects.append(instruct_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(human_annoted_subjects, facebook_bart_subjects, trained_facebook_bart_subjects))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns = ['human_annoted_subjects', 'facebook_bart_subjects', 'trained_facebook_bart_subjects'])\n",
        "df"
      ],
      "metadata": {
        "id": "Yy9Bz2uV1c-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "from rouge_score.scoring import BootstrapAggregator\n",
        "\n"
      ],
      "metadata": {
        "id": "eMHM2fPzsAxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate ROUGE scores for each pair of prediction and reference\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "aggregator = BootstrapAggregator() # Initialize aggregator\n",
        "\n",
        "for prediction, reference in zip(facebook_bart_subjects, human_annoted_subjects[0:len(facebook_bart_subjects)]):\n",
        "    score = scorer.score(reference, prediction)\n",
        "    aggregator.add_scores(score) # Add scores to aggregator\n",
        "\n",
        "# Aggregate the scores\n",
        "original_model_results = aggregator.aggregate()\n",
        "\n",
        "print(original_model_results)"
      ],
      "metadata": {
        "id": "eAzGwd9qrS0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model_results = rouge.compute(\n",
        "    predictions=facebook_bart_subjects,\n",
        "    references=human_annoted_subjects[0:len(facebook_bart_subjects)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=trained_facebook_bart_subjects,\n",
        "    references=human_annoted_subjects[0:len(trained_facebook_bart_subjects)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('INSTRUCT MODEL:')\n",
        "print(instruct_model_results)"
      ],
      "metadata": {
        "id": "4N2v_M6SJUBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model locally\n",
        "model_save_path = \"./facebook_bart_base\"\n",
        "facebook_bart_base_model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "WCpjkEa9rWxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate subject lines for the test set\n",
        "def generate_subject_line(email_body):\n",
        "    inputs = tokenizer(\"generate subject line: \" + email_body, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    # Move inputs to the GPU if available\n",
        "    inputs = {k: v.to(facebook_bart_base_model.device) for k, v in inputs.items()}\n",
        "    # Access input_ids from the dictionary\n",
        "    outputs = facebook_bart_base_model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "test_df['generated_subject_line'] = test_df['body'].apply(generate_subject_line)"
      ],
      "metadata": {
        "id": "HM0tVaa5i8n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "n5PT2dfT2OEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "from nltk.translate.meteor_score import meteor_score"
      ],
      "metadata": {
        "id": "T6KRD7Oo2Muk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "\n",
        "# Evaluate the generated subject lines\n",
        "def evaluate_metrics(true_subject, generated_subject):\n",
        "    # Tokenize the input for METEOR\n",
        "    true_subject_tokens = nltk.word_tokenize(true_subject)\n",
        "    generated_subject_tokens = nltk.word_tokenize(generated_subject)\n",
        "\n",
        "    # ROUGE-L\n",
        "    rougeL = scorer.score(true_subject, generated_subject)['rougeL'].fmeasure\n",
        "\n",
        "    # METEOR\n",
        "    meteor = meteor_score([true_subject_tokens], generated_subject_tokens) # Pass tokenized input\n",
        "\n",
        "    # SACREBLEU - Use import sacrebleu directly\n",
        "    sacre_bleu_score = sacrebleu.corpus_bleu([generated_subject], [[true_subject]]).score\n",
        "\n",
        "    return rougeL, meteor, sacre_bleu_score # Return the score, not the module\n",
        "\n",
        "test_df['rougeL'], test_df['meteor'], test_df['sacrebleu'] = zip(*test_df.apply(\n",
        "    lambda row: evaluate_metrics(row['subject'], row['generated_subject_line']), axis=1))"
      ],
      "metadata": {
        "id": "g5swqKxsfrQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate average scores\n",
        "average_rougeL = test_df['rougeL'].mean()\n",
        "average_meteor = test_df['meteor'].mean()\n",
        "average_sacrebleu = test_df['sacrebleu'].mean() # Now you should be able to calculate the mean\n",
        "\n",
        "print(f'Average facebook_bart_base_model ROUGE-L: {average_rougeL:.4f}')\n",
        "print(f'Average facebook_bart_base_model METEOR: {average_meteor:.4f}')\n",
        "print(f'Average facebook_bart_base_model SACREBLEU: {average_sacrebleu:.4f}')"
      ],
      "metadata": {
        "id": "TcCHg_OU29ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Replace 'directory_name' with the name of your directory\n",
        "shutil.make_archive('/content/facebook_bart_base_model', 'zip', 'facebook_bart_base_model')"
      ],
      "metadata": {
        "id": "Lzt-7qci4Niq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# files.download('facebook_bart_base_model.zip')"
      ],
      "metadata": {
        "id": "pY7QEHAw4bsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **t5-base**"
      ],
      "metadata": {
        "id": "Dx76AiWz5CEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokenizer and model t5-base\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n"
      ],
      "metadata": {
        "id": "qlr5gRXjGGw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset.from_pandas(pivot_df)"
      ],
      "metadata": {
        "id": "zd2OYT0bXhMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "_dQE0OytX7D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(pivot_df):\n",
        "    inputs = pivot_df['cleaned_emails']\n",
        "    targets = pivot_df['subject']\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "j2MxJu3bGWxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "id": "K0aZkei_T0V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Replace 'directory_name' with the name of your directory\n",
        "# shutil.make_archive('/content/fine_tuned_t5', 'zip', 'fine_tuned_t5')\n",
        "\n",
        "# Download the zipped directory\n"
      ],
      "metadata": {
        "id": "uGocveE7q2_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.make_archive('/content/results', 'zip', 'FB_Bart_model_results')\n",
        "shutil.make_archive('/content/logs', 'zip', 'FB_Bart_model_logs')\n"
      ],
      "metadata": {
        "id": "eT_AJo9PEjxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# files.download('fine_tuned_t5.zip')"
      ],
      "metadata": {
        "id": "ZO2hx72nrVx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "BDnsANB7Iz3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_dataset['attention_mask']\n",
        "# tokenized_dataset['input_ids']"
      ],
      "metadata": {
        "id": "6Mo5GbMDH8Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and test sets\n",
        "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "# Data collator for padding\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
      ],
      "metadata": {
        "id": "0KMgDPdkU753"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    # predict_with_generate=True,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "19V3PPTgVBQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('fine_tuned_t5')\n",
        "tokenizer.save_pretrained('fine_tuned_t5')"
      ],
      "metadata": {
        "id": "uvY_nd2_bise"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available. \\nUsing GPU\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"GPU is not available. \\nUsing CPU\")\n",
        "    device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "9shJp4Spb3Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(text):\n",
        "    inputs = tokenizer.encode_plus(text, return_tensors='pt', max_length=512, truncation=True)\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    summary_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=150, num_beams=2, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\n",
        "\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n"
      ],
      "metadata": {
        "id": "ohAtd3YQbn1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example inference\n",
        "example_text = pivot_df['body'][3]\n",
        "print(\"Email Body:\", example_text)\n",
        "print(\"\\n\\n Generated Subject:\", generate_summary(example_text))\n",
        "print(\"\\n\\n Actual Subject:\", pivot_df['subject'][3])"
      ],
      "metadata": {
        "id": "V_MzfdBqcJZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "# Evaluate the generated subject lines\n",
        "def evaluate_metrics(true_subject, generated_subject):\n",
        "    # ROUGE-L\n",
        "    rougeL = scorer.score(true_subject, generated_subject)['rougeL'].fmeasure\n",
        "\n",
        "    # METEOR\n",
        "    meteor = meteor_score([true_subject], generated_subject)\n",
        "\n",
        "    # SACREBLEU\n",
        "    sacre_bleu = sacrebleu.corpus_bleu([generated_subject], [[true_subject]]).score\n",
        "\n",
        "    return rougeL, meteor, sacre_bleu\n",
        "\n",
        "test_df['rougeL'], test_df['meteor'], test_df['sacrebleu'] = zip(*test_df.apply(\n",
        "    lambda row: evaluate_metrics(row['subject_line'], row['generated_subject_line']), axis=1))\n",
        "\n",
        "# Calculate average scores\n",
        "average_rougeL = test_df['rougeL'].mean()\n",
        "average_meteor = test_df['meteor'].mean()\n",
        "average_sacrebleu = test_df['sacrebleu'].mean()\n",
        "\n",
        "print(f'Average ROUGE-L: {average_rougeL:.4f}')\n",
        "print(f'Average METEOR: {average_meteor:.4f}')\n",
        "print(f'Average SACREBLEU: {average_sacrebleu:.4f}')"
      ],
      "metadata": {
        "id": "UvJijyqfN5k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_sentences(text):\n",
        "  \"\"\"\n",
        "  This function extracts sentences from text without introducing control characters.\n",
        "\n",
        "  Args:\n",
        "      text (str): The text to be processed.\n",
        "\n",
        "  Returns:\n",
        "      list: A list of extracted sentences.\n",
        "  \"\"\"\n",
        "  # Split text based on sentence boundaries (adjust pattern as needed)\n",
        "  sentences = re.split(r\"(?<!\\w)\\.(?!\\w)\", text)\n",
        "\n",
        "  # Remove empty elements and leading/trailing whitespace\n",
        "  sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "\n",
        "  return sentences\n",
        "\n",
        "# Example usage\n",
        "text = \"This is an example text. It contains multiple sentences.  \\nThere are also newlines and extra spaces.\"\n",
        "extracted_sentences = extract_sentences(pivot_df['body'][0])\n",
        "print(extracted_sentences)\n"
      ],
      "metadata": {
        "id": "R_UuPM3bLEEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df['body'][0]"
      ],
      "metadata": {
        "id": "gW-fbCCleshA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "summary = summarizer(pivot_df['body'][0], max_length=15, min_length=2, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "id": "8hmYeCH2flFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df['body'][0]"
      ],
      "metadata": {
        "id": "QSg88TJak4t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Chirayu/subject-generator-t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Chirayu/subject-generator-t5-base\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "def get_subject(content, num_beams=5,max_length=512, repetition_penalty=2.5, length_penalty=1, early_stopping=True,top_p=.95, top_k=50, num_return_sequences=3):\n",
        "\n",
        "  text =  \"title: \" + content + \" </s>\"\n",
        "\n",
        "  input_ids = tokenizer.encode(\n",
        "    text, return_tensors=\"pt\", add_special_tokens=True\n",
        "  )\n",
        "\n",
        "  input_ids = input_ids.to(device)\n",
        "  generated_ids = model.generate(\n",
        "      input_ids=input_ids,\n",
        "\n",
        "      num_beams=num_beams,\n",
        "      max_length=max_length,\n",
        "      repetition_penalty=repetition_penalty,\n",
        "      length_penalty=length_penalty,\n",
        "      early_stopping=early_stopping,\n",
        "      top_p=top_p,\n",
        "      top_k=top_k,\n",
        "      num_return_sequences=num_return_sequences,\n",
        "  )\n",
        "  subjects = [tokenizer.decode(generated_id,skip_special_tokens=True,clean_up_tokenization_spaces=True,) for generated_id in generated_ids]\n",
        "  return subjects"
      ],
      "metadata": {
        "id": "eI-YnG00leqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = get_subject(pivot_df['body'][0])\n",
        "x"
      ],
      "metadata": {
        "id": "JhMrjNf3mKOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df['body'][0]"
      ],
      "metadata": {
        "id": "fTcDIbDHq1wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df['body'][1]"
      ],
      "metadata": {
        "id": "X3-wKWGkmgXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap"
      ],
      "metadata": {
        "id": "XOWH5k3TjZqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_summarizer(email_text):\n",
        "\n",
        "    model_name = \"facebook/bart-large-cnn\"\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    inputs = tokenizer.encode(\"summarize: \" + email_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(inputs, max_length=100, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    formatted_summary = \"\\n\".join(textwrap.wrap(summary, width=80))\n",
        "    return formatted_summary\n",
        "\n",
        "summary = text_summarizer(pivot_df['body'][0])\n",
        "summary"
      ],
      "metadata": {
        "id": "Y85goaEkcOYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sentences(text):\n",
        "    \"\"\"\n",
        "    Extract sentences from the text and remove newline characters.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The text to process.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of sentences without newline characters.\n",
        "    \"\"\"\n",
        "    # Replace newline characters with spaces\n",
        "    text = text.replace('\\n', '')\n",
        "    text = text.replace('!', '.').replace('?', '.').replace(';', '.')\n",
        "\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "# Example usage\n",
        "sample_text = \"Hello, world!\\nThis is a test.\\nLet's replace, commas, and newlines.\"\n",
        "sentences = extract_sentences(sample_text)\n",
        "for sentence in sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "id": "zGazV9EgQkxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = text_summarizer(pivot_df['body'][0])\n",
        "summary"
      ],
      "metadata": {
        "id": "jezqQnA9Lck-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df['body'][0]"
      ],
      "metadata": {
        "id": "7ZrJIfk2MFn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_len = [len(pivot_df['dialogue'].split()) for x in samsum['train']]\n",
        "sub_len = [len(pivot_df['summary'].split()) for x in samsum['train']]\n",
        "\n",
        "dialogue_len = [len(x['dialogue'].split()) for x in samsum['train']]\n",
        "summary_len = [len(x['summary'].split()) for x in samsum['train']]\n",
        "\n"
      ],
      "metadata": {
        "id": "_AM_aMgKSjST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "describe_df(pivot_df)"
      ],
      "metadata": {
        "id": "0N8EfTwkRqF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_rows"
      ],
      "metadata": {
        "id": "FrRN8if0dyTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.preprocessing import Tokenizer, Lowercaser, PunctuationRemover, StopwordRemover\n",
        "from langchain.stemming import Stemmer\n",
        "from langchain import Pipeline"
      ],
      "metadata": {
        "id": "oKUEMAee8aSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(email_docs)*3"
      ],
      "metadata": {
        "id": "REA79efkSNQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emails_df['content'][0]"
      ],
      "metadata": {
        "id": "pwhf-gfhMfWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emails_df['source'][0]"
      ],
      "metadata": {
        "id": "FLk0SU2hTPr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_file_name(emails_df['source'][0])"
      ],
      "metadata": {
        "id": "XTKL_ao_T6jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8h-_DelG2pb9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dd455412578c4d1d8f73454352f55dfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5b9825f5e02f4ee7b1193911a8b4302e",
              "IPY_MODEL_db45f48e3f6a435f9c1033223c6583b3",
              "IPY_MODEL_175c88a024e3442d873e1d32105c2a87"
            ],
            "layout": "IPY_MODEL_70a0d4239ce54b7ca7b7be85e338ff6b"
          }
        },
        "5b9825f5e02f4ee7b1193911a8b4302e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ece5d0da870d4e9b9162aafe774e5cae",
            "placeholder": "​",
            "style": "IPY_MODEL_a2d9e641e283465590b092012cde3416",
            "value": "Filter: 100%"
          }
        },
        "db45f48e3f6a435f9c1033223c6583b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c763cb97ab6a47b9a79314d8f9d5f552",
            "max": 11548,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79bd8be20fb94dde82bb1d229eb04079",
            "value": 11548
          }
        },
        "175c88a024e3442d873e1d32105c2a87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b65a430e82c7485c8285cd9cbfcef34a",
            "placeholder": "​",
            "style": "IPY_MODEL_50f014f8e8844c2188dea3f6089c11f3",
            "value": " 11548/11548 [00:00&lt;00:00, 72190.43 examples/s]"
          }
        },
        "70a0d4239ce54b7ca7b7be85e338ff6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ece5d0da870d4e9b9162aafe774e5cae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2d9e641e283465590b092012cde3416": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c763cb97ab6a47b9a79314d8f9d5f552": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79bd8be20fb94dde82bb1d229eb04079": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b65a430e82c7485c8285cd9cbfcef34a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50f014f8e8844c2188dea3f6089c11f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "724039c07d5543889f6dfbc9acba5963": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ac9fb8460d1c412d89ea717637d6f542",
              "IPY_MODEL_ea78d56bce3045a2b1b29f62dda8b47c",
              "IPY_MODEL_baa1e19cef524baaa1cec05b00dea6e6"
            ],
            "layout": "IPY_MODEL_8cbb814d82ca400eb6a953d383c14a30"
          }
        },
        "ac9fb8460d1c412d89ea717637d6f542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92567892a44a471486080493c91ca457",
            "placeholder": "​",
            "style": "IPY_MODEL_1e27f7c0f6a945f6bfe9c5f0b6967e60",
            "value": "Filter: 100%"
          }
        },
        "ea78d56bce3045a2b1b29f62dda8b47c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d2cfedddba344699aba23b1d1e7815c",
            "max": 11548,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2a410fd369c4b2c824e731d8c632dab",
            "value": 11548
          }
        },
        "baa1e19cef524baaa1cec05b00dea6e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40d886f4a97d4dd18a12669b4f3cf034",
            "placeholder": "​",
            "style": "IPY_MODEL_270698080fcc4436aa3f5e68fd5b9cd4",
            "value": " 11548/11548 [00:00&lt;00:00, 68826.03 examples/s]"
          }
        },
        "8cbb814d82ca400eb6a953d383c14a30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92567892a44a471486080493c91ca457": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e27f7c0f6a945f6bfe9c5f0b6967e60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d2cfedddba344699aba23b1d1e7815c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2a410fd369c4b2c824e731d8c632dab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40d886f4a97d4dd18a12669b4f3cf034": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "270698080fcc4436aa3f5e68fd5b9cd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42257f6d0fd34b00bd0ab24f1d0fee29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be1c9784782b42c09e994574fba6f876",
              "IPY_MODEL_10399410ea694b0291bf69ae480f5aca",
              "IPY_MODEL_ab6cf5b6b39248c198b8939cb9e7fbc8"
            ],
            "layout": "IPY_MODEL_3e651810d6f94031bad1f0ecbe1cf329"
          }
        },
        "be1c9784782b42c09e994574fba6f876": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb594b58b5084236a533ae611e1de180",
            "placeholder": "​",
            "style": "IPY_MODEL_f9feca956227499e8092763e71706680",
            "value": "Map: 100%"
          }
        },
        "10399410ea694b0291bf69ae480f5aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1084a3b9d0c7496da45f143fe2803349",
            "max": 3850,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2ea38454dc84443b0dc2554fb48b205",
            "value": 3850
          }
        },
        "ab6cf5b6b39248c198b8939cb9e7fbc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1884d3e40563472c860454264fbc8d09",
            "placeholder": "​",
            "style": "IPY_MODEL_480fd7eff02f43389cbf5b771a0d57b7",
            "value": " 3850/3850 [00:06&lt;00:00, 748.34 examples/s]"
          }
        },
        "3e651810d6f94031bad1f0ecbe1cf329": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb594b58b5084236a533ae611e1de180": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9feca956227499e8092763e71706680": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1084a3b9d0c7496da45f143fe2803349": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2ea38454dc84443b0dc2554fb48b205": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1884d3e40563472c860454264fbc8d09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "480fd7eff02f43389cbf5b771a0d57b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}