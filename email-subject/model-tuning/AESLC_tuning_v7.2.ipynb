{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nutworker/qM-AI-L/blob/L_test/email-subject/model-tuning/AESLC_tuning_v7.2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repository_url = 'https://github.com/nutworker/qM-AI-L'\n",
        "!git clone {repository_url}"
      ],
      "metadata": {
        "id": "n0UfO9kv4-nq",
        "outputId": "544dcd20-3b0d-4a70-eb08-bb478822a21f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'qM-AI-L' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ID0koCUtX33Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ad1d2d3-5970-42d1-8a11-6695826a7ca6",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.5.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.15.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.4)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.4)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.32.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.3.1+cu121)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.10.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.5.15)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.25.2)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install evaluate\n",
        "!pip install rouge-score # Installing rouge-score library (https://pypi.org/project/rouge-score/)\n",
        "\n",
        "# !pip install accelerate -U\n",
        "\n",
        "!pip install transformers[torch]\n",
        "!pip install sacrebleu\n",
        "\n",
        "!pip install datasets nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Libraries\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import pandas as pd # Data Handling\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import seaborn as sns\n",
        "\n",
        "from datasets import load_metric\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "xuOoOaSEDLqg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformers\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration      # BERT Tokenizer and architecture\n",
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments         # These will help us to fine-tune our model\n",
        "from transformers import pipeline                                         # Pipeline\n",
        "from transformers import DataCollatorForSeq2Seq                           # DataCollator to batch the data\n",
        "\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "tbXvEcauCHyq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "from nltk.translate.meteor_score import meteor_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7T6r-6fLsBo",
        "outputId": "b14f942f-ddd0-4f04-8cb9-554b354de94b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Dataset**"
      ],
      "metadata": {
        "id": "As3cC5xSngQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df = pd.read_csv('/content/qM-AI-L/email-subject/model-tuning/train_emails.csv')"
      ],
      "metadata": {
        "id": "4nBEREYUQPjQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_df"
      ],
      "metadata": {
        "id": "L9skosJrQc0t",
        "outputId": "cd0499cf-012a-4ac4-a580-6d79fc3206a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    source                                               body  \\\n",
              "0         allen-p_inbox_20  Greg/Phillip,  Attached is the Grande Communic...   \n",
              "1         allen-p_inbox_28  Phillip & Keith  Attached is the first draw re...   \n",
              "2         allen-p_inbox_63  Your Internet Banking accounts are now setup a...   \n",
              "3         allen-p_inbox_64  To our IBS Customers that are still hanging in...   \n",
              "4         allen-p_inbox_65  Phillip Good Morning!\\nI hope you had a wonder...   \n",
              "...                    ...                                                ...   \n",
              "14431  zufferli-j_inbox_43  This email is acknowledgement from the Power P...   \n",
              "14432  zufferli-j_inbox_44  This email is acknowledgement from the Power P...   \n",
              "14433  zufferli-j_inbox_46  John,  Further to the voice message that I lef...   \n",
              "14434   zufferli-j_inbox_8  Make sure that all curves are downloaded by th...   \n",
              "14435   zufferli-j_inbox_9  John:  Do you need Accumap day one?\\nCarmen sa...   \n",
              "\n",
              "                       subject  ann0  ann1  ann2  body_wcount  subj_wcount  \\\n",
              "0            Service Agreement   NaN   NaN   NaN           65            2   \n",
              "1               Bishops Corner   NaN   NaN   NaN          145            2   \n",
              "2             Internet Banking   NaN   NaN   NaN          250            2   \n",
              "3             Internet Banking   NaN   NaN   NaN          458            2   \n",
              "4      SMEs for expert stories   NaN   NaN   NaN           68            4   \n",
              "...                        ...   ...   ...   ...          ...          ...   \n",
              "14431               Power Pool   NaN   NaN   NaN          227            2   \n",
              "14432    Power Pool of Alberta   NaN   NaN   NaN          277            4   \n",
              "14433           Enron Security   NaN   NaN   NaN          148            2   \n",
              "14434        Simulation Curves   NaN   NaN   NaN           66            2   \n",
              "14435              IHS Accumap   NaN   NaN   NaN           35            2   \n",
              "\n",
              "                                          cleaned_emails  \n",
              "0      gregphillip attached is the grande communicati...  \n",
              "1      phillip keith attached is the first draw reque...  \n",
              "2      your internet banking accounts are now setup a...  \n",
              "3      to our ibs customers that are still hanging in...  \n",
              "4      phillip good morning i hope you had a wonderfu...  \n",
              "...                                                  ...  \n",
              "14431  this email is acknowledgement from the power p...  \n",
              "14432  this email is acknowledgement from the power p...  \n",
              "14433  john further to the voice message that i left ...  \n",
              "14434  make sure that all curves are downloaded by th...  \n",
              "14435  john do you need accumap day one carmen said t...  \n",
              "\n",
              "[14436 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-149c0fd3-ee9f-4d64-abc7-3a14add7fc20\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>body</th>\n",
              "      <th>subject</th>\n",
              "      <th>ann0</th>\n",
              "      <th>ann1</th>\n",
              "      <th>ann2</th>\n",
              "      <th>body_wcount</th>\n",
              "      <th>subj_wcount</th>\n",
              "      <th>cleaned_emails</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>allen-p_inbox_20</td>\n",
              "      <td>Greg/Phillip,  Attached is the Grande Communic...</td>\n",
              "      <td>Service Agreement</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>65</td>\n",
              "      <td>2</td>\n",
              "      <td>gregphillip attached is the grande communicati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>allen-p_inbox_28</td>\n",
              "      <td>Phillip &amp; Keith  Attached is the first draw re...</td>\n",
              "      <td>Bishops Corner</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>145</td>\n",
              "      <td>2</td>\n",
              "      <td>phillip keith attached is the first draw reque...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>allen-p_inbox_63</td>\n",
              "      <td>Your Internet Banking accounts are now setup a...</td>\n",
              "      <td>Internet Banking</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>250</td>\n",
              "      <td>2</td>\n",
              "      <td>your internet banking accounts are now setup a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>allen-p_inbox_64</td>\n",
              "      <td>To our IBS Customers that are still hanging in...</td>\n",
              "      <td>Internet Banking</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>458</td>\n",
              "      <td>2</td>\n",
              "      <td>to our ibs customers that are still hanging in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>allen-p_inbox_65</td>\n",
              "      <td>Phillip Good Morning!\\nI hope you had a wonder...</td>\n",
              "      <td>SMEs for expert stories</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>68</td>\n",
              "      <td>4</td>\n",
              "      <td>phillip good morning i hope you had a wonderfu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14431</th>\n",
              "      <td>zufferli-j_inbox_43</td>\n",
              "      <td>This email is acknowledgement from the Power P...</td>\n",
              "      <td>Power Pool</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>227</td>\n",
              "      <td>2</td>\n",
              "      <td>this email is acknowledgement from the power p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14432</th>\n",
              "      <td>zufferli-j_inbox_44</td>\n",
              "      <td>This email is acknowledgement from the Power P...</td>\n",
              "      <td>Power Pool of Alberta</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>277</td>\n",
              "      <td>4</td>\n",
              "      <td>this email is acknowledgement from the power p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14433</th>\n",
              "      <td>zufferli-j_inbox_46</td>\n",
              "      <td>John,  Further to the voice message that I lef...</td>\n",
              "      <td>Enron Security</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>148</td>\n",
              "      <td>2</td>\n",
              "      <td>john further to the voice message that i left ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14434</th>\n",
              "      <td>zufferli-j_inbox_8</td>\n",
              "      <td>Make sure that all curves are downloaded by th...</td>\n",
              "      <td>Simulation Curves</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>66</td>\n",
              "      <td>2</td>\n",
              "      <td>make sure that all curves are downloaded by th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14435</th>\n",
              "      <td>zufferli-j_inbox_9</td>\n",
              "      <td>John:  Do you need Accumap day one?\\nCarmen sa...</td>\n",
              "      <td>IHS Accumap</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>35</td>\n",
              "      <td>2</td>\n",
              "      <td>john do you need accumap day one carmen said t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14436 rows Ã— 9 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-149c0fd3-ee9f-4d64-abc7-3a14add7fc20')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-149c0fd3-ee9f-4d64-abc7-3a14add7fc20 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-149c0fd3-ee9f-4d64-abc7-3a14add7fc20');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-745ec934-9160-43a1-964d-744d58215564\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-745ec934-9160-43a1-964d-744d58215564')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-745ec934-9160-43a1-964d-744d58215564 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_8716facb-c785-456a-b1ba-675c41e4aec0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('dataset_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8716facb-c785-456a-b1ba-675c41e4aec0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('dataset_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset_df",
              "summary": "{\n  \"name\": \"dataset_df\",\n  \"rows\": 14436,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14436,\n        \"samples\": [\n          \"sanders-r_sent_2329\",\n          \"buy-r_inbox_344\",\n          \"dorland-c_sent_301\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"body\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13523,\n        \"samples\": [\n          \"Sara: Here are drafts of the Certificates and Sight Drafts for Niagara Mohawk, El Paso electric, LG&E Energy Marketing and Energy Production Corp. (just in case it wasn't renewed).\\nI have put together files for each of these that include copies of the LC, the underlying trading docs and any correspondence that has been sent.\\nExcept for Niagara Mohawk which requires copies of the invoices and that the certificate be on \\\"letterhead\\\" there don't appear to be any other special requirements.\\nLeslie Reeves will be faxing the invoices to us.\\nI will be in tomorrow until around 11.\\nThanks for your help on this.\\nAll of these docs are in a file called Enron Restructuring.\",\n          \"Good afternoon,  When you get a chance, please let me know if you would like for me to order you a 2002 calendar.\\nIf so, which kind?\\nYou may want more than one type.\\nJust let me know and I will be more than happy to order what you want.\\nThanks!\",\n          \"Hey Hunter, A quick update.\\nKevin Brady is up to speed on working with Colin and Chris on consolidating critical and noncritical notices for all the regions.\\nI have also passed on the idea of an operations page on the website to the other logistics managers and the central desk schedulers to get their ideas and insights on how to make this page beneficial to both the traders and schedulers.\\nA couple of ideas that have already been passed my way are:  \\t1) Incorporating the Gas Daily and Index historicals against production/storage by pipe so that we could project cash outs (Mark Schrab) \\t2) A transportation rate matrix on the web site (Cora Pendergrass) \\t3) Enron operations personnel contact page with phone numbers and pictures.\\nMuch like the weather guys.\\nI laughed at first but Victor LaMadrid   \\t\\twhom suggested this said that the East Desk Traders don't even know all of his schedulers.\\n4) An easy access to the pipelines meters and dunns numbers.\\n(Kevin Brady) \\t5) Morning sheets updated with contract, constraint, and imbalance information from Sitara and Unify (Victor LaMadrid)  I have asked Kevin to consolidate all the ideas and then we can get together and decide which ones you want to place into production.\\nLisa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subject\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12032,\n        \"samples\": [\n          \"OMLX Application\",\n          \"Draft OF CA\",\n          \"Wharton trip\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ann0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ann1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ann2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"body_wcount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 148,\n        \"min\": 25,\n        \"max\": 3136,\n        \"num_unique_values\": 683,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subj_wcount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 1,\n        \"max\": 15,\n        \"num_unique_values\": 15,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cleaned_emails\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 13498,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Train_df, Test_df = train_test_split(dataset_df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "MoaINjzY76Bf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test Train Split**"
      ],
      "metadata": {
        "id": "L6sRPzc6oU02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subset_df = dataset_df.sample(frac=0.4, random_state=42)\n",
        "subset_df.shape"
      ],
      "metadata": {
        "id": "1lnDOKzv4QZb",
        "outputId": "0c386e40-e5d5-412c-8293-168ef754b600",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5774, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(subset_df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "guCjiJJyRLzM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.shape, test_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHgDx01BreHk",
        "outputId": "984b9296-5aed-45c9-9c27-7155691043a7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4619, 9), (1155, 9))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "3464+867"
      ],
      "metadata": {
        "id": "yWGKFsujF8B_",
        "outputId": "6dd9adfe-98a5-4c59-ddb1-26381c5c1758",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4331"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ],
      "metadata": {
        "id": "ZHaFrU6-oYQ5"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "ONvIXUMi96NJ",
        "outputId": "b99d8394-ffe0-482f-8fdd-c3e2248128dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['source', 'body', 'subject', 'ann0', 'ann1', 'ann2', 'body_wcount', 'subj_wcount', 'cleaned_emails', '__index_level_0__'],\n",
              "     num_rows: 4619\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['source', 'body', 'subject', 'ann0', 'ann1', 'ann2', 'body_wcount', 'subj_wcount', 'cleaned_emails', '__index_level_0__'],\n",
              "     num_rows: 1155\n",
              " }))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a subset of Dataset**"
      ],
      "metadata": {
        "id": "ofsPKKg_nxk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #create a subset of dataset\n",
        "# train_dataset = Train_dataset.filter(lambda example, index: index % 2 == 0, with_indices=True)\n",
        "# test_dataset = Test_dataset.filter(lambda example, index: index % 2 == 0, with_indices=True)\n",
        "\n",
        "train_dataset.shape, test_dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iDTgW2UtjIT",
        "outputId": "29bca63b-1a4e-4713-fcac-77e5d282b3e8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((4619, 10), (1155, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset['cleaned_emails'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "nghT9hdrfoYJ",
        "outputId": "6ddc2e54-87e6-4497-bb5b-145b7b6a3a77"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this follows up on my voicemail to you of just a few minutes ago. i reviewed a voicemail from theresa bushman this morning about the timing on the new mariner matter that mariner for which mariner would like to retain fj. she has spoken with scott josie apparently one of our business people and has been advised that the december 27 closing is critical due to market conditions. theresa also believes that the december 27 date will be aggressive even for fj given the late date. theresa please let me know if i have mischaracterized what you said. i see two options 1. if you agree with me that we cant sign the letter as is i do my best to get an acceptable letter negotiated with uriel dutton today. if i cant mariner retains someone else today. frankly i am not optimistic about getting an acceptable letter from fj based on where fj started. 2. if you believe that we need to go ahead and get the best deal we can from fj under the circumstances i will do my best to get that done by noon or so and if fj doesnt budge we may have to sign the letter as is. caveat i dont know if dutton is in the office this week so he and i may not be able to communicate quickly. also if we redraft a letter we will probably not have time to get comments back from all the other interested enron attorneys. i would also suggest telling kelly zelacovitz mariners gc this morning what our game plan is and if it is option one have her go ahead and contact her backup choices of law firms start running conflicts checks etc. and have them be ready to hit the ground running. if there is anyone else i need to send this email to please let me know. i am in all day today please let me know what you want me to do. britt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the pre-trained FLAN-T5 models and its tokenizer directly from HuggingFace. FLAN-T5 was released in the paper Scaling Instruction-Finetuned Language Models - it is an enhanced version of T5 that has been finetuned in a mixture of tasks. Setting torch_dtype=torch.bfloat16 specifies the memory type to be used by this model.**"
      ],
      "metadata": {
        "id": "qUwUyn2SowRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get size of models\n",
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"Trainable model parameters: {trainable_model_params}\\nAll model parameters: {all_model_params}\\nPercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
        "\n"
      ],
      "metadata": {
        "id": "0fsHEX6Bm3an"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Flan T5 Base Model**"
      ],
      "metadata": {
        "id": "GKeNgh0NpSaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# # Freeze the first 6 layers of the encoder and decoder\n",
        "# for param in model.encoder.block[:6].parameters(): # Use 'block' instead of 'layers'\n",
        "#     param.requires_grad = False\n",
        "# for param in model.decoder.block[:6].parameters(): # Use 'block' instead of 'layers'\n",
        "#     param.requires_grad = False\n",
        "\n",
        "print(\"For Model : FLAN-T5-Base \\n\",print_number_of_trainable_model_parameters(model))"
      ],
      "metadata": {
        "id": "Xkz4NmaW0KPZ",
        "outputId": "c94b6351-f224-45ca-bd4a-9296e72bbb57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Model : FLAN-T5-Base \n",
            " Trainable model parameters: 247577856\n",
            "All model parameters: 247577856\n",
            "Percentage of trainable model parameters: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    inputs = [\"generate subject line: \" + example for example in examples[\"cleaned_emails\"]]\n",
        "    targets = [example for example in examples[\"subject\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "    # Tokenize the targets and include decoder_input_ids\n",
        "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\", return_tensors='pt')\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    # Add decoder input ids\n",
        "    model_inputs['decoder_input_ids'] = model_inputs['labels']\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "2qUVhlTf07dU"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=train_dataset.column_names)\n",
        "tokenized_eval_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=test_dataset.column_names)\n"
      ],
      "metadata": {
        "id": "UPg_4Oau1Cem",
        "outputId": "b26cdf5a-71fb-4116-9c1c-da0c85163d29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "e8f82d68e8284be08d995d324f3efe89",
            "bc6d02ad8ecc4d758626a60d20f79c08",
            "bb61fb30810c4f42a73e97cff70001c9",
            "754a29233fc54d3a89391b44b66adc6d",
            "de75626e391e42a184ded4466d8597fe",
            "2497f6440b8543e4a2284e5740258fde",
            "8c7448d07c1c45458b50a67689f45e0d",
            "a04476077a764522b809fa7e8b88bfc4",
            "5eff43129ecd4ac19abcf369a067b3c9",
            "8072e26dd0bc4300a423294828a66302",
            "c022c4595f564d689813994ca597c4ad",
            "3229583154e641398c19dffaeab5dc32",
            "506bccd386da463a806c5681a76d0a6d",
            "43ad06ac4fc94e9bab870b96da8e4632",
            "fb32db86c0cd457cae5781f7496864f3",
            "b8593a0e7e294fc4b991eb1f5ba42b33",
            "0e6f0500d5c74155986d365d109f6aa2",
            "f494bd084335455bb8bfa6ea6f2ab618",
            "9085438dd7494c6795f08fe6c9973703",
            "8c93e7021e244a8fbf4c5ceb2de77795",
            "13ed88532658406e979eff57b6ea4717",
            "b9efae73e2244af58edd30ff9e722e30"
          ]
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4619 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8f82d68e8284be08d995d324f3efe89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1155 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3229583154e641398c19dffaeab5dc32"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_dataset, tokenized_eval_dataset"
      ],
      "metadata": {
        "id": "yFSryaOrEKQm",
        "outputId": "be81b21c-177f-4366-b6db-0c8c3bdb3f89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'],\n",
              "     num_rows: 4619\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['input_ids', 'attention_mask', 'labels', 'decoder_input_ids'],\n",
              "     num_rows: 1155\n",
              " }))"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',           # Output directory\n",
        "    evaluation_strategy='epoch',      # Evaluate after each epoch\n",
        "    learning_rate=3e-5,               # Learning rate\n",
        "    per_device_train_batch_size=4,    # Reduced batch size for training\n",
        "    per_device_eval_batch_size=4,     # Reduced batch size for evaluation\n",
        "    num_train_epochs=1,               # Number of epochs\n",
        "    weight_decay=0.01,                # Weight decay to prevent overfitting\n",
        "    save_total_limit=3,               # Limit the total amount of checkpoints\n",
        "    save_strategy='epoch',            # Save checkpoint after each epoch\n",
        "    load_best_model_at_end=True,      # Load the best model at the end of training\n",
        "    metric_for_best_model='eval_loss',# Metric to compare best models\n",
        "    logging_dir='./logs',             # Directory for storing logs\n",
        "    logging_steps=500,                # Log every 500 steps\n",
        "    gradient_accumulation_steps=1,    # Reduced gradient accumulation steps\n",
        "    warmup_steps=500,                 # Number of warmup steps for learning rate scheduler\n",
        "    fp16=True,                        # Use 16-bit floating point precision (mixed precision)\n",
        "    remove_unused_columns=False,      # Ensure all columns are used\n",
        "    label_smoothing_factor=0.1,       # Label smoothing\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, padding=True),\n",
        "\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "8prqmLk74DC2",
        "outputId": "cb8152b9-e1a7-43fb-875b-2682c5a8db4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1156' max='1155' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1155/1155 06:59, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "model_save_path = \"fine_tuned_flan_t5\"\n",
        "model.save_pretrained(model_save_path)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained(model_save_path)"
      ],
      "metadata": {
        "id": "FgN3Xw66_dmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model and tokenizer\n",
        "Flan_T5_FT_model = T5ForConditionalGeneration.from_pretrained(model_save_path)\n",
        "\n",
        "Flan_T5_model_FT_tokenizer = T5Tokenizer.from_pretrained(model_save_path)\n"
      ],
      "metadata": {
        "id": "Jg1XJBOZAbma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emails_for_score_df = Test_df.sample(n=20, random_state=32)\n",
        "emails_for_score_df"
      ],
      "metadata": {
        "id": "wtlJg_-YB2Fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_subjects = []\n",
        "flan_T5_subjects = []\n",
        "finetuned_flan_T5_subjects = []\n",
        "\n",
        "\n",
        "for index, row in emails_for_score_df.iterrows():\n",
        "    email = row['body']\n",
        "    original_subject = row['subject']\n",
        "    prompt = f\"\"\"\n",
        "    Generate the subject line for the following email.\n",
        "\n",
        "    Email:\n",
        "    {email}\n",
        "\n",
        "    Subject:\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the device (GPU if available, otherwise CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move both models to the same device\n",
        "    model.to(device)\n",
        "    Flan_T5_FT_LP_model.to(device)\n",
        "\n",
        "    original_model_input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    finetuned_model_input_ids = Flan_T5_model_FT_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "\n",
        "    original_model_outputs = model.generate(input_ids=original_model_input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "    flan_T5_subjects.append(original_model_text_output)\n",
        "\n",
        "    instruct_model_outputs = Flan_T5_FT_LP_model.generate(input_ids=finetuned_model_input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    instruct_model_text_output = Flan_T5_model_FT_tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "    finetuned_flan_T5_subjects.append(instruct_model_text_output)\n",
        "\n",
        "    original_subjects.append(original_subject)\n",
        "\n",
        "zipped_subjects = list(zip(original_subjects, flan_T5_subjects, finetuned_flan_T5_subjects))\n",
        "\n",
        "Flan_T5_LP_df = pd.DataFrame(zipped_subjects, columns = ['original_subjects', 'T5_subjects', 'finetuned_flan_T5_subjects'])\n",
        "Flan_T5_LP_df"
      ],
      "metadata": {
        "id": "m45B3VOkBAvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=flan_T5_subjects,\n",
        "    references=original_subjects,\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "finetuned_model_results = rouge.compute(\n",
        "    predictions=finetuned_flan_T5_subjects,\n",
        "    references=original_subjects,\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('FLAN T5 Base Model:')\n",
        "print(original_model_results)\n",
        "print('\\nFINETUNED FLAN T5 BASE MODEL:')\n",
        "print(finetuned_model_results)"
      ],
      "metadata": {
        "id": "Lufz_SVBDS5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**flan_t5_base_original_model**"
      ],
      "metadata": {
        "id": "OnDeQ_KnAWai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flan_t5_base_original_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)\n",
        "flan_t5_base_tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "print(\"For Model : FLAN-T5-Base \\n\",print_number_of_trainable_model_parameters(flan_t5_base_original_model))"
      ],
      "metadata": {
        "id": "6ufBICyAosbw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flan_t5_base_tokenizer.special_tokens_map"
      ],
      "metadata": {
        "id": "2-a_H6f06elE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Flan T5 Small Model**"
      ],
      "metadata": {
        "id": "JU6tvjt2pbJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flan_t5_small_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\", torch_dtype=torch.bfloat16)\n",
        "# flan_t5_small_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
        "\n",
        "print(\"For Model : Flan-t5-small \\n\",print_number_of_trainable_model_parameters(flan_t5_small_model))"
      ],
      "metadata": {
        "id": "SpTzBk9jEv_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flan-t5-base vs. flan-t5-small parameters\n",
        "247577856-76961152"
      ],
      "metadata": {
        "id": "iLGrsgJ2E35P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test the Model with Zero Shot Inferencing**"
      ],
      "metadata": {
        "id": "ilaYbjowrNVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using a pre-trained model from Hugging Face/sources - FLAN-T5-Base .**"
      ],
      "metadata": {
        "id": "IV3wSp9HRSxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = 9\n",
        "\n",
        "email = Train_dataset['cleaned_emails'][index]\n",
        "subject = Train_dataset['subject'][index]\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Generate the subject line for the following email.\n",
        "\n",
        "Email:\n",
        "{email}\n",
        "\n",
        "Subject:\n",
        "\"\"\"\n",
        "\n",
        "inputs = flan_t5_base_tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "# Move input tensors to the same device as the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inputs = {k: v.to(flan_t5_base_original_model.device) for k, v in inputs.items()}\n",
        "\n",
        "\n",
        "output = flan_t5_base_tokenizer.decode(\n",
        "    flan_t5_base_original_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE ORIGINAL HUMAN ANNOTED SUBJECT LINE :\\n{subject}\\n')\n",
        "print(dash_line)\n",
        "print(f'FLAN-T5 MODEL GENERATION - ZERO SHOT SUBJECT LINE:\\n{output}')"
      ],
      "metadata": {
        "id": "YpNTTyqsvQS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using a pre-trained model from Hugging Face etc- FLAN-T5-SMALL .**"
      ],
      "metadata": {
        "id": "slm6KkVEF_xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = flan_t5_base_tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "# Move input tensors to the same device as the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "\n",
        "output = flan_t5_base_tokenizer.decode(\n",
        "    flan_t5_small_model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=200,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE ORIGINAL HUMAN ANNOTED SUBJECT LINE :\\n{subject}\\n')\n",
        "print(dash_line)\n",
        "print(f'FLAN-T5-SMALL MODEL GENERATION - ZERO SHOT SUBJECT LINE:\\n{output}')"
      ],
      "metadata": {
        "id": "LYLYJUzZG_pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**By testing with various models with the zero shot inferencing, we can see that the model struggles to extract the same subject line compared to the baseline subject, but it does pull out some important information from the email which indicates the models can be fine-tuned to the task at hand.**"
      ],
      "metadata": {
        "id": "C7S2UVFV8ZND"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Performing Full Fine-Tuning using FLAN-T5 BASE MODEL**"
      ],
      "metadata": {
        "id": "3kTvPTm2wFVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "# # # Define special tokens\n",
        "# # special_tokens = {\n",
        "# #     \"additional_special_tokens\": [\"<EMAIL>\", \"</EMAIL>\"]\n",
        "# # }\n",
        "\n",
        "# # # Add special tokens to the tokenizer\n",
        "# # tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "\n",
        "# # Load the model\n",
        "# model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "# Resize the model's embeddings to accommodate the new tokens\n",
        "flan_t5_base_original_model.resize_token_embeddings(len(flan_t5_base_tokenizer))\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    start_prompt = 'Generate subject line for the email.\\n\\nEmail:'\n",
        "    end_prompt = '\\nSubject:\\n'\n",
        "    inputs = [start_prompt + email + end_prompt for email in examples['cleaned_emails']]\n",
        "    model_inputs = flan_t5_base_tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
        "\n",
        "    labels = flan_t5_base_tokenizer(examples['subject'], max_length=15, truncation=True, padding='max_length')\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = Train_dataset.map(tokenize_function, batched=True, remove_columns=Train_dataset.column_names)\n"
      ],
      "metadata": {
        "id": "bKNmBevDVfmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_dataset = Dataset.from_pandas(dataset_df)"
      ],
      "metadata": {
        "id": "Dhgp_1YuGFLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_full_dataset = full_dataset.map(tokenize_function, batched=True, remove_columns=full_dataset.column_names)"
      ],
      "metadata": {
        "id": "3_-CBdF6GavI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_full_dataset"
      ],
      "metadata": {
        "id": "t4gF6D6Qq0Se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and test\n",
        "split_dataset = tokenized_full_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = split_dataset['train']\n",
        "test_dataset = split_dataset['test']"
      ],
      "metadata": {
        "id": "DCfVV097WbZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "2pDOF9rv8eRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    learning_rate=0.0001,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        ")"
      ],
      "metadata": {
        "id": "ckExw6k-Wq50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=flan_t5_base_original_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=flan_t5_base_tokenizer,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True)\n",
        "\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Uo6VC5P6Wv1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model and tokenizer\n",
        "save_directory = \"./finetuned_flan_t5_base\"\n",
        "flan_t5_base_original_model.save_pretrained(save_directory)\n",
        "flan_t5_base_tokenizer.save_pretrained(save_directory)\n",
        "\n",
        "# Zip the saved model files\n",
        "zip_filename = \"finetuned_flan_t5_base.zip\"\n",
        "shutil.make_archive(base_name=\"finetuned_flan_t5_base\", format=\"zip\", root_dir=save_directory)"
      ],
      "metadata": {
        "id": "S6gF7_J8uzEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the zipped checkpoint\n",
        "files.download(zip_filename)"
      ],
      "metadata": {
        "id": "qWhDAJVuCFmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Unzip model checkpoint as it is zipped\n",
        "# # !unzip finetuned_flan_t5_base.zip -d ./finetuned_flan_t5_base\n",
        "\n",
        "# # Load the tokenizer\n",
        "# tokenizer = T5Tokenizer.from_pretrained('t5-base', legacy=False)\n",
        "\n",
        "# # Add special tokens to the tokenizer\n",
        "# num_added_tokens = tokenizer.add_special_tokens(special_tokens_map)\n",
        "\n",
        "\n",
        "# model_path = \"./finetuned_flan_t5_base\"\n",
        "# finetuned_tokenizer = T5Tokenizer.from_pretrained(model_path,  legacy=False)\n",
        "# finetuned_model = T5ForConditionalGeneration.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "XJd20rDqvIM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"./finetuned_flan_t5_base\"\n",
        "finetuned_tokenizer = T5Tokenizer.from_pretrained(model_path,  legacy=False)\n",
        "finetuned_model = T5ForConditionalGeneration.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "j0l1kvYvGKIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For Finetuned Model : Flan-t5 \\n\",print_number_of_trainable_model_parameters(finetuned_model))"
      ],
      "metadata": {
        "id": "qOAE40aKGtR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load special tokens from JSON file\n",
        "with open('/content/finetuned_flan_t5_base/special_tokens_map.json', 'r') as file:\n",
        "    special_tokens_map = json.load(file)\n",
        "\n",
        "# Print special tokens to verify\n",
        "print(special_tokens_map)"
      ],
      "metadata": {
        "id": "mjIfz8qkD7vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluate the Flan T5-Base Model Quantitatively (with ROUGE Metric)**\n",
        "The ROUGE metric helps quantify the validity of subject lines produced by models. It compares subjects to a \"Annoted baseline\" subject which is usually created by a human. While not perfect, it does indicate the overall increase in subject line generatiion effectiveness that we have accomplished by fine-tuning."
      ],
      "metadata": {
        "id": "os-472avsQlh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the Saved Finetuned Model and Tokenizer**"
      ],
      "metadata": {
        "id": "xxDoUanxvRov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emails_for_score_df = Train_df.sample(n=10, random_state=32)\n",
        "emails_for_score_df"
      ],
      "metadata": {
        "id": "pMYOOca77uGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emails = emails_for_score_df['body']\n",
        "original_subjects = emails_for_score_df['subject']\n",
        "original_subjects\n"
      ],
      "metadata": {
        "id": "a6FaMiCNtOgs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "id": "Mn8SNYNUBLbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_subjects = []\n",
        "flan_T5_subjects = []\n",
        "finetuned_flan_T5_subjects = []\n",
        "\n",
        "\n",
        "for index, row in emails_for_score_df.iterrows():\n",
        "    email = row['body']\n",
        "    original_subject = row['subject']\n",
        "    prompt = f\"\"\"\n",
        "    Generate the subject line for the following email.\n",
        "\n",
        "    Email:\n",
        "    {email}\n",
        "\n",
        "    Subject:\n",
        "    \"\"\"\n",
        "\n",
        "    # Define the device (GPU if available, otherwise CPU)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Move both models to the same device\n",
        "    flan_t5_base_original_model.to(device)\n",
        "    finetuned_model.to(device)\n",
        "\n",
        "    original_model_input_ids = flan_t5_base_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "    finetuned_model_input_ids = finetuned_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "\n",
        "    original_model_outputs = flan_t5_base_original_model.generate(input_ids=original_model_input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    original_model_text_output = flan_t5_base_tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "    flan_T5_subjects.append(original_model_text_output)\n",
        "\n",
        "    instruct_model_outputs = finetuned_model.generate(input_ids=finetuned_model_input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
        "    instruct_model_text_output = finetuned_tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "    finetuned_flan_T5_subjects.append(instruct_model_text_output)\n",
        "\n",
        "    original_subjects.append(original_subject)\n",
        "\n",
        "zipped_subjects = list(zip(original_subjects, flan_T5_subjects, finetuned_flan_T5_subjects))\n",
        "\n",
        "Flan_T5_df = pd.DataFrame(zipped_subjects, columns = ['original_subjects', 'T5_subjects', 'finetuned_flan_T5_subjects'])\n",
        "Flan_T5_df"
      ],
      "metadata": {
        "id": "0SUG4spPuGPQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=flan_T5_subjects,\n",
        "    references=original_subjects,\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "finetuned_model_results = rouge.compute(\n",
        "    predictions=finetuned_flan_T5_subjects,\n",
        "    references=original_subjects,\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('FLAN T5 Base Model:')\n",
        "print(original_model_results)\n",
        "print('\\nFINETUNED FLAN T5 BASE MODEL:')\n",
        "print(finetuned_model_results)"
      ],
      "metadata": {
        "id": "LnuLg7e_1_Kt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Finetuning results should show  improvement in all ROUGE metrics:**\n",
        "\n"
      ],
      "metadata": {
        "id": "-2t11zBHN8nM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Absolute percentage improvement of FINETUNED MODEL over HUMAN BASELINE\")\n",
        "\n",
        "improvement = (np.array(list(finetuned_model_results.values())) - np.array(list(original_model_results.values())))\n",
        "for key, value in zip(finetuned_model_results.keys(), improvement):\n",
        "    print(f'{key}: {value*100:.2f}%')"
      ],
      "metadata": {
        "id": "cKtHUDqyNf4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Perform Parameter Efficient Fine-Tuning (PEFT)**\n",
        "Parameter Efficient Fine-Tuning (PEFT), which is more efficient than full fine-tuning and yields comparable results. PEFT, often referring to Low-Rank Adaptation (LoRA), enables fine-tuning with fewer compute resources, often a single GPU.\n",
        "\n",
        "LoRA produces a small adapter (a few MBs) while keeping the original LLM unchanged. During inference, this adapter is combined with the original LLM, allowing multiple adapters to reuse the same LLM and reducing memory requirements for various tasks."
      ],
      "metadata": {
        "id": "FoDFsTogOMTn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup the PEFT/LoRA model for Fine-Tuning**"
      ],
      "metadata": {
        "id": "O0TgYeKuO-4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \\\n",
        "    loralib==0.1.1 \\\n",
        "    peft==0.3.0 --quiet"
      ],
      "metadata": {
        "id": "Od5iuL0YPKPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32, # Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
        ")"
      ],
      "metadata": {
        "id": "eDAkW_4VO6qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(flan_t5_base_original_model, lora_config)\n",
        "print(print_number_of_trainable_model_parameters(peft_model))"
      ],
      "metadata": {
        "id": "esd41UNmO6il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "uHDODVO9Q4Pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_dataset"
      ],
      "metadata": {
        "id": "QPI-afIzahgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "# # Preprocess function\n",
        "# def preprocess_function(examples):\n",
        "#     inputs = tokenizer(examples['body'], max_length=512, truncation=True, padding=\"max_length\")\n",
        "#     targets = tokenizer(examples['lable'], max_length=128, truncation=True, padding=\"max_length\")\n",
        "#     return {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'labels': targets['input_ids']} # Added attention_mask to the output\n",
        "\n",
        "# # Apply preprocessing\n",
        "# train_dataset = split_dataset['train'].map(preprocess_function, batched=True)\n",
        "# val_dataset = split_dataset['validation'].map(preprocess_function, batched=True)\n",
        "\n",
        "# Load base model\n",
        "model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "# Define LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1\n",
        ")"
      ],
      "metadata": {
        "id": "FIVCe35mZtlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "UwNKgwWScPvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# kill -9 <pid>"
      ],
      "metadata": {
        "id": "hbedsL4PcaRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f'./peft-subjectline-training-{str(int(time.time()))}'\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=1,\n",
        "    max_steps=1,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "xMBzuOTjiaiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "id": "m-IZqqfui1FA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_trainer.train()"
      ],
      "metadata": {
        "id": "gNK4EmpSinor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=1,\n",
        "    gradient_accumulation_steps=8,  # Accumulate gradients over multiple steps\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=200,\n",
        "    fp16=True,  # Enable mixed precision training\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "PS2BCdU1YxUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained('path/to/save/finetuned_model')\n",
        "tokenizer.save_pretrained('path/to/save/finetuned_model')"
      ],
      "metadata": {
        "id": "rByaLl7oZZBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f'./peft-subjectline-training-{str(int(time.time()))}'\n",
        "\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    # learning_rate=1e-4, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    # logging_steps=10,\n",
        "    # max_steps=1\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=split_dataset['test'],\n",
        ")\n",
        "\n",
        "\n",
        "peft_trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "Td3BbnnYPrhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_path=\"./peft-subjectline-checkpoint-local\"\n",
        "\n",
        "peft_trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)"
      ],
      "metadata": {
        "id": "mQIvcDX7YaEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "\n",
        "peft_model = PeftModel.from_pretrained(peft_model_base,\n",
        "                                       './peft-dialogue-summary-checkpoint-from-s3/',\n",
        "                                       torch_dtype=torch.bfloat16,\n",
        "                                       is_trainable=False)"
      ],
      "metadata": {
        "id": "swBbx2xffokn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wz0o7_vWSgON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download the 'punkt' resource\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('wordnet') # Download the WordNet corpus"
      ],
      "metadata": {
        "id": "vTP9c9DuNXhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "# Evaluate the generated subject lines\n",
        "def evaluate_metrics(true_subject, generated_subject):\n",
        "    # ROUGE-L\n",
        "    rougeL = scorer.score(true_subject, generated_subject)['rougeL'].fmeasure\n",
        "\n",
        "    # METEOR\n",
        "    meteor = meteor_score([word_tokenize(true_subject)], word_tokenize(generated_subject))\n",
        "\n",
        "    # SACREBLEU\n",
        "    sacre_bleu = sacrebleu.corpus_bleu([generated_subject], [[true_subject]]).score\n",
        "\n",
        "    return rougeL, meteor, sacre_bleu\n",
        "\n",
        "Flan_T5_df['rougeL'], Flan_T5_df['meteor'], Flan_T5_df['sacrebleu'] = zip(*Flan_T5_df.apply(\n",
        "    lambda row: evaluate_metrics(row['original_subjects'], row['T5_subjects']), axis=1))\n",
        "\n",
        "# Calculate average scores\n",
        "average_rougeL = Flan_T5_df['rougeL'].mean()\n",
        "average_meteor = Flan_T5_df['meteor'].mean()\n",
        "average_sacrebleu = Flan_T5_df['sacrebleu'].mean()\n",
        "\n",
        "print(f'Average ROUGE-L: {average_rougeL:.4f}')\n",
        "print(f'Average METEOR: {average_meteor:.4f}')\n",
        "print(f'Average SACREBLEU: {average_sacrebleu:.4f}')"
      ],
      "metadata": {
        "id": "xFBuePhiLDcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1_dWxlbZQCTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#T5 Flan Base\n",
        "# Load the ROUGE metric\n",
        "rouge_metric = load_metric('rouge')\n",
        "\n",
        "def calculate_rouge_scores(predictions, references):\n",
        "    rouge_metric.add_batch(predictions=predictions, references=references)\n",
        "    result = rouge_metric.compute()\n",
        "    return {\n",
        "        'rouge1': result['rouge1'].mid.fmeasure * 100,\n",
        "        'rouge2': result['rouge2'].mid.fmeasure * 100,\n",
        "        'rougeL': result['rougeL'].mid.fmeasure * 100\n",
        "    }\n",
        "\n",
        "# Generate predictions and calculate ROUGE scores\n",
        "predictions = []\n",
        "references = []\n",
        "\n",
        "for example in test_dataset.shuffle(seed=42).select(range(20)):\n",
        "    input_ids = example['input_ids']\n",
        "    input_ids = torch.tensor(input_ids).unsqueeze(0).to(device)  # Add batch dimension\n",
        "    outputs = finetuned_model.generate(input_ids)\n",
        "    pred = finetuned_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    ref = finetuned_tokenizer.decode(example['labels'], skip_special_tokens=True)\n",
        "\n",
        "    predictions.append(pred)\n",
        "    references.append(ref)\n",
        "\n",
        "# Calculate and print ROUGE scores for each test sample\n",
        "rouge_scores = []\n",
        "for pred, ref in zip(predictions, references):\n",
        "    score = calculate_rouge_scores([pred], [ref])\n",
        "    rouge_scores.append(score)\n",
        "    print(f\"Prediction: {pred}\\nReference: {ref}\\nROUGE Scores: {score}\\n\")\n",
        "\n",
        "# Convert to DataFrame for better visualization\n",
        "rouge_df = pd.DataFrame(rouge_scores)\n",
        "rouge_df"
      ],
      "metadata": {
        "id": "vQ-w7TxGZEV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)"
      ],
      "metadata": {
        "id": "rubayi3ZdWSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_test_df = test_df.sample(n=10, random_state=42)"
      ],
      "metadata": {
        "id": "SHpw9jHsefXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_test_df"
      ],
      "metadata": {
        "id": "258hkqQlevCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_subject_line(email_body):\n",
        "    inputs = finetuned_tokenizer(\"Generate a subject line for the following email.: \" + email_body, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    # Move inputs to the same device as the model\n",
        "    inputs = {k: v.to(finetuned_model.device) for k, v in inputs.items()}\n",
        "    # Access input_ids as a key in the dictionary\n",
        "    outputs = finetuned_model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n",
        "    return finetuned_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "Hw-_IgKYdlkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_test_df['generated_subject_line'] = small_test_df['cleaned_emails'].apply(generate_subject_line)\n",
        "small_test_df"
      ],
      "metadata": {
        "id": "HPLoCJgffW7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "\n",
        "# Evaluate the generated subject lines\n",
        "def evaluate_metrics(true_subject, generated_subject):\n",
        "    # Tokenize the input for METEOR\n",
        "    true_subject_tokens = nltk.word_tokenize(true_subject)\n",
        "    generated_subject_tokens = nltk.word_tokenize(generated_subject)\n",
        "\n",
        "    # ROUGE-L\n",
        "    rougeL = scorer.score(true_subject, generated_subject)['rougeL'].fmeasure\n",
        "\n",
        "    # METEOR\n",
        "    meteor = meteor_score([true_subject_tokens], generated_subject_tokens) # Pass tokenized input\n",
        "\n",
        "    # SACREBLEU - Use import sacrebleu directly\n",
        "    sacre_bleu_score = sacrebleu.corpus_bleu([generated_subject], [[true_subject]]).score\n",
        "\n",
        "    return rougeL, meteor, sacre_bleu_score # Return the score, not the module\n",
        "\n",
        "small_test_df['rougeL'], small_test_df['meteor'], small_test_df['sacrebleu'] = zip(*small_test_df.apply(\n",
        "    lambda row: evaluate_metrics(row['subject'], row['generated_subject_line']), axis=1))"
      ],
      "metadata": {
        "id": "fjUdAhycdLvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small_test_df"
      ],
      "metadata": {
        "id": "E4bo2fd6NQQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\"\n",
        ")"
      ],
      "metadata": {
        "id": "tAUYMLeiWoQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_prompt = 'Generate the subject line for the following email.\\n\\nEmail:'\n",
        "end_prompt = '\\nSubject:\\n'\n",
        "start_prompt + email + end_prompt\n",
        "print(start_prompt)\n",
        "print(email)\n",
        "print(end_prompt)"
      ],
      "metadata": {
        "id": "hcU_iws1LLSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization for FLAN-T5 model\n",
        "def flan_t5_base_tokenize_function(example):\n",
        "    start_prompt = 'Generate the subject line for the following email.\\n\\nEmail:'\n",
        "    end_prompt = '\\nSubject:\\n'\n",
        "    prompt = [start_prompt + email + end_prompt for email in example[\"body\"]]\n",
        "    example['input_ids'] = flan_t5_base_tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    example['labels'] = flan_t5_base_tokenizer(example[\"subject\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    return example\n",
        "\n",
        "# The dataset actually contains 3 diff splits: train, validation, test.\n",
        "# The tokenize_function code is handling all data across all splits in batches.\n",
        "tokenized_train_datasets = train_dataset.map(flan_t5_base_tokenize_function, batched=True)\n",
        "tokenized_test_datasets = test_dataset.map(flan_t5_base_tokenize_function, batched=True)\n"
      ],
      "metadata": {
        "id": "PNZhgan9fJ6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_train_datasets = tokenized_train_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
        "# tokenized_test_datasets = tokenized_test_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)"
      ],
      "metadata": {
        "id": "EkFTC1Mvr81h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_datasets, tokenized_test_datasets"
      ],
      "metadata": {
        "id": "lV8NjE0YrWaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FLAN-T5 BASE - Fine-Tune the Model with the Preprocessed Tokenised Dataset utiliing the built-in Hugging Face Trainer class. Then compare it with reference to the original model.**\n",
        "\n",
        "flan_t5_base_original_model = AutoModelForSeq2SeqLM.from_pretrained('google/flan-t5-base', torch_dtype=torch.bfloat16)\n",
        "flan_t5_base_tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')"
      ],
      "metadata": {
        "id": "Q0tc44JYPNW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    start_prompt = 'Generate the subject line for the following email.\\n\\nEmail:'\n",
        "    end_prompt = '\\nSubject:\\n'\n",
        "    inputs = [start_prompt + email + end_prompt for email in examples['email_body']]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding='max_length')\n",
        "\n",
        "    labels = tokenizer(examples['subject'], max_length=50, truncation=True, padding='max_length')\n",
        "    model_inputs['labels'] = labels['input_ids']\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=dataset.column_names)\n"
      ],
      "metadata": {
        "id": "H1GVZxyPU56L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Tokenization\n",
        "# def preprocess_data(examples):\n",
        "#     inputs = [\"generate subject line: \" + email for email in examples['cleaned_emails']]\n",
        "#     model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "#     with tokenizer.as_target_tokenizer():\n",
        "#         labels = tokenizer(examples['subject'], max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "#     return model_inputs\n",
        "\n",
        "# train_dataset = train_dataset.map(preprocess_data, batched=True)\n",
        "# test_dataset = test_dataset.map(preprocess_data, batched=True)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=flan_t5_base_original_model ,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_datasets,\n",
        "    eval_dataset=tokenized_test_datasets,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n"
      ],
      "metadata": {
        "id": "RV5TsUrfR1zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FLAN-T5 SMALL - Fine-Tune the Model with the Preprocessed Tokenised Dataset utiliing the built-in Hugging Face Trainer class. Then compare it with reference to the original model.\n",
        "\n",
        "flan_t5_small_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\", torch_dtype=torch.bfloat16)\n",
        "flan_t5_small_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")"
      ],
      "metadata": {
        "id": "YrHuhrEfsmdv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization for FLAN-T5 SMALL model\n",
        "def flan_t5_small_tokenize_function(example):\n",
        "    start_prompt = 'Generate the subject line for the following email.\\n\\nEmail:'\n",
        "    end_prompt = '\\nSubject:\\n'\n",
        "    prompt = [start_prompt + email + end_prompt for email in example[\"cleaned_emails\"]]\n",
        "    example['input_ids'] = flan_t5_small_tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "    example['labels'] = flan_t5_small_tokenizer(example[\"subject\"], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
        "\n",
        "    return example\n",
        "\n",
        "# The dataset actually contains 3 diff splits: train, validation, test.\n",
        "# The tokenize_function code is handling all data across all splits in batches.\n",
        "tokenized_train_datasets = train_dataset.map(flan_t5_small_tokenize_function, batched=True)\n",
        "tokenized_test_datasets = test_dataset.map(flan_t5_small_tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "n15RWBkAXxzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=flan_t5_small_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_datasets,\n",
        "    eval_dataset=tokenized_test_datasets,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "GjJMAmUcYSh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model checkpoint\n",
        "flan_t5_small_model.save_pretrained('./results/Flan_T5_small_model')\n",
        "flan_t5_small_tokenizer.save_pretrained('./results/Flan_T5_small_model')\n",
        "\n",
        "# Define and save the generation configuration\n",
        "generation_config = GenerationConfig(\n",
        "    early_stopping=True,\n",
        "    num_beams=4,\n",
        "    no_repeat_ngram_size=3,\n",
        "    forced_bos_token_id=0,\n",
        "    forced_eos_token_id=2\n",
        ")\n",
        "\n",
        "generation_config.save_pretrained('./results/Flan_T5_small_model')"
      ],
      "metadata": {
        "id": "CcOfhXaGdmyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AutoModelForSeq2SeqLM\n",
        "AutoTokenizer"
      ],
      "metadata": {
        "id": "tESux8htfc_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained Flan T5 Small fine tuned model\n",
        "flan_T5_small_model_directory = '/content/results/Flan_T5_small_model'\n",
        "flan_T5_small_tokenizer = AutoTokenizer.from_pretrained(flan_T5_small_model_directory)\n",
        "flan_T5_small_model_finetuned = AutoModelForSeq2SeqLM.from_pretrained(flan_T5_small_model_directory)"
      ],
      "metadata": {
        "id": "PM4itxVveS3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emails_for_score = tokenized_test_datasets[0:10]['cleaned_emails']\n",
        "original_subjects = tokenized_test_datasets[0:10]['subject']"
      ],
      "metadata": {
        "id": "b9GGjgy6b0cK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flan_T5small_subjects = []\n",
        "finetuned_flan_T5small_subjects = []\n",
        "\n",
        "for _, cleaned_emails in enumerate(emails_for_score):\n",
        "    prompt = f\"\"\"\n",
        "    Generate the subject line for the following email.\n",
        "\n",
        "    Email:\n",
        "    {email}\n",
        "\n",
        "    Subject:\n",
        "    \"\"\"\n",
        "    # Tokenize the input and move to the same device as the model\n",
        "    inputs = flan_t5_small_tokenizer(prompt, return_tensors='pt').to(flan_T5_small_model_finetuned.device) # Move inputs to the same device as the fine-tuned model\n",
        "\n",
        "    # Add the decoder_start_token_id to the generation configuration\n",
        "    generation_config = GenerationConfig(max_new_tokens=200,\n",
        "        decoder_start_token_id=flan_T5_small_tokenizer.bos_token_id if flan_T5_small_tokenizer.bos_token_id is not None else flan_T5_small_tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Generate subject using the original model\n",
        "    # Make sure flan_t5_small_model is on the same device as flan_T5_small_model_finetuned\n",
        "    flan_t5_small_model = flan_t5_small_model.to(flan_T5_small_model_finetuned.device)\n",
        "    original_model_outputs = flan_t5_small_model.generate(**inputs, generation_config=generation_config)\n",
        "    original_model_subject_output = flan_t5_small_tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "    flan_T5small_subjects.append(original_model_subject_output)\n",
        "\n",
        "    # Generate subject using the finetuned model\n",
        "    trained_flan_t5_outputs = flan_T5_small_model_finetuned.generate(**inputs, generation_config=generation_config)\n",
        "    instruct_model_text_output = flan_T5_small_tokenizer.decode(trained_flan_t5_outputs[0], skip_special_tokens=True)\n",
        "    finetuned_flan_T5small_subjects.append(instruct_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(original_subjects, flan_T5small_subjects, finetuned_flan_T5small_subjects))\n",
        "\n",
        "Flan_T5_Small_Subjects_df = pd.DataFrame(zipped_summaries, columns = ['Original_subjects', 'Flan_T5s_subjects', 'Finetuned_Flan_T5s_subjects'])\n",
        "Flan_T5_Small_Subjects_df"
      ],
      "metadata": {
        "id": "LXGOGtXUbh5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "# from datasets import Dataset\n",
        "\n",
        "\n",
        "# # Initialize tokenizer and model\n",
        "# tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "# T5_model = T5ForConditionalGeneration.from_pretrained('t5-small')"
      ],
      "metadata": {
        "id": "exqW5Kv0mzyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    start_prompt = 'Generate the subject line for the following email.\\n\\n'\n",
        "    end_prompt = '\\n\\Subject: '\n",
        "    prompt = [start_prompt + email + end_prompt for email in example[\"cleaned_emails\"]]\n",
        "    # Tokenize the input_ids - do not flatten\n",
        "    example['input_ids'] = [tokenizer(p, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids[0] for p in prompt]\n",
        "    # Tokenize the labels - do not flatten\n",
        "    example['labels'] = [tokenizer(s, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids[0] for s in example[\"subject\"]]\n",
        "\n",
        "    return example"
      ],
      "metadata": {
        "id": "aJmS7TCD4cQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output_dir = f'./email-Subject-training-{str(int(time.time()))}'\n",
        "\n",
        "# # Define training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir=output_dir,\n",
        "#     learning_rate=1e-5,\n",
        "#     num_train_epochs=3,\n",
        "#     weight_decay=0.01,\n",
        "#     logging_steps=30,\n",
        "#     evaluation_strategy='epoch',\n",
        "#     per_device_train_batch_size=4,  # Reduced batch size\n",
        "#     # max_steps=1\n",
        "# )\n",
        "\n",
        "# # training_args = TrainingArguments(\n",
        "# #     output_dir='./results',\n",
        "# #     evaluation_strategy='epoch',\n",
        "# #     learning_rate=1e-5,\n",
        "# #     per_device_train_batch_size=4,\n",
        "# #     per_device_eval_batch_size=4,\n",
        "# #     num_train_epochs=3,\n",
        "# #     weight_decay=0.01,\n",
        "# # )\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=flan_t5_base_original_model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=tokenized_train_datasets,\n",
        "#     eval_dataset=tokenized_test_datasets\n",
        "# )\n",
        "\n",
        "# trainer.train()"
      ],
      "metadata": {
        "id": "ymjhoRaoshwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Generate subject lines for the test set\n",
        "# def generate_subject_line(email_body):\n",
        "#     inputs = tokenizer(\"generate subject line: \" + email_body, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "#     # Move inputs to the same device as the model\n",
        "#     inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "#     outputs = model.generate(inputs.input_ids, max_length=128, num_beams=4, early_stopping=True)\n",
        "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# test_df['generated_subject_line'] = test_df['cleaned_emails'].apply(generate_subject_line)\n",
        "\n",
        "# Generate subject lines for the test set\n",
        "def generate_subject_line(email_body):\n",
        "    inputs = tokenizer(\"generate subject line: \" + email_body, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    # Move inputs to the same device as the model\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    # Access input_ids as a key in the dictionary\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "test_df['generated_subject_line'] = test_df['cleaned_emails'].apply(generate_subject_line)"
      ],
      "metadata": {
        "id": "H00VaamVakII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **t5-small Evaluation**"
      ],
      "metadata": {
        "id": "cQ4bUC74cjcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)"
      ],
      "metadata": {
        "id": "XJ3D8o9yegTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the 'punkt' resource\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet') # Download WordNet\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "\n",
        "# Evaluate the generated subject lines\n",
        "def evaluate_metrics(true_subject, generated_subject):\n",
        "    # Tokenize the input for METEOR\n",
        "    true_subject_tokens = nltk.word_tokenize(true_subject)\n",
        "    generated_subject_tokens = nltk.word_tokenize(generated_subject)\n",
        "\n",
        "    # ROUGE-L\n",
        "    rougeL = scorer.score(true_subject, generated_subject)['rougeL'].fmeasure\n",
        "\n",
        "    # METEOR\n",
        "    meteor = meteor_score([true_subject_tokens], generated_subject_tokens) # Pass tokenized input\n",
        "\n",
        "    # SACREBLEU - Use import sacrebleu directly\n",
        "    sacre_bleu_score = sacrebleu.corpus_bleu([generated_subject], [[true_subject]]).score\n",
        "\n",
        "    return rougeL, meteor, sacre_bleu_score # Return the score, not the module\n",
        "\n",
        "test_df['rougeL'], test_df['meteor'], test_df['sacrebleu'] = zip(*test_df.apply(\n",
        "    lambda row: evaluate_metrics(row['subject'], row['generated_subject_line']), axis=1))\n",
        "\n",
        "# Calculate average scores\n",
        "average_rougeL = test_df['rougeL'].mean()\n",
        "average_meteor = test_df['meteor'].mean()\n",
        "average_sacrebleu = test_df['sacrebleu'].mean() # Now you should be able to calculate the mean\n",
        "\n",
        "print(f'Average ROUGE-L: {average_rougeL:.4f}')\n",
        "print(f'Average METEOR: {average_meteor:.4f}')\n",
        "print(f'Average SACREBLEU: {average_sacrebleu:.4f}')"
      ],
      "metadata": {
        "id": "t5C7gSX3ceB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pretrained('facebook/bart-base')**"
      ],
      "metadata": {
        "id": "nP7-BBpHezyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "K5lsH75xikZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Convert DataFrame to Dataset\n",
        "# train_dataset = Dataset.from_pandas(train_df)\n",
        "# test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "facebook_bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "facebook_bart_base_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
      ],
      "metadata": {
        "id": "W_UUmQ-O5tso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"For Model : facebook_bart_base_model \\n\",print_number_of_trainable_model_parameters(facebook_bart_base_model))"
      ],
      "metadata": {
        "id": "HGVkLIIf52zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Generate the subject line for the following email.\n",
        "\n",
        "Email:\n",
        "{email}\n",
        "\n",
        "Subject:\n",
        "\"\"\"\n",
        "\n",
        "inputs = facebook_bart_tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "# Move input tensors to the same device as the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "inputs = {k: v.to(facebook_bart_base_model.device) for k, v in inputs.items()}\n",
        "\n",
        "outputs = facebook_bart_base_model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE ORIGINAL HUMAN ANNOTED SUBJECT LINE :\\n{subject}\\n')\n",
        "print(dash_line)\n",
        "print(f'FB BART MODEL GENERATION - ZERO SHOT SUBJECT LINE:\\n{output}')"
      ],
      "metadata": {
        "id": "3p1txC8YLwi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenization\n",
        "def Tokenization_preprocess_data(examples):\n",
        "    inputs = [prompt + email for email in examples['cleaned_emails']]\n",
        "    model_inputs = facebook_bart_tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Use facebook_bart_tokenizer as the target tokenizer\n",
        "    with facebook_bart_tokenizer.as_target_tokenizer():\n",
        "        labels = facebook_bart_tokenizer(examples['subject'], max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "train_dataset = train_dataset.map(Tokenization_preprocess_data, batched=True)\n",
        "test_dataset = test_dataset.map(Tokenization_preprocess_data, batched=True)\n",
        "\n",
        "# # Define training arguments\n",
        "# training_args = TrainingArguments(\n",
        "#     output_dir='./results',\n",
        "#     evaluation_strategy='epoch',\n",
        "#     learning_rate=2e-5,\n",
        "#     per_device_train_batch_size=4,\n",
        "#     per_device_eval_batch_size=4,\n",
        "#     num_train_epochs=3,\n",
        "#     weight_decay=0.01,\n",
        "# )\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',        # Directory to save the model checkpoints\n",
        "    evaluation_strategy='steps',\n",
        "    save_steps=500,               # Save checkpoint every 1000 steps\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=200,\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "facebook_bart_base_model_trainer = Trainer(\n",
        "    model=facebook_bart_base_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "facebook_bart_base_model_trainer.train()\n"
      ],
      "metadata": {
        "id": "UHfFS01Feyu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the final model checkpoint\n",
        "facebook_bart_base_model.save_pretrained('./results/FB_Bart_model')\n",
        "facebook_bart_tokenizer.save_pretrained('./results/FB_Bart_model')\n",
        "\n",
        "# Define and save the generation configuration\n",
        "generation_config = GenerationConfig(\n",
        "    early_stopping=True,\n",
        "    num_beams=4,\n",
        "    no_repeat_ngram_size=3,\n",
        "    forced_bos_token_id=0,\n",
        "    forced_eos_token_id=2\n",
        ")\n",
        "\n",
        "generation_config.save_pretrained('./results/FB_Bart_model')"
      ],
      "metadata": {
        "id": "8Pj219LPCVmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content"
      ],
      "metadata": {
        "id": "wfBgXge85Fxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "logs_folder = '/content/logs'  # Adjust to your source folder\n",
        "logs_git_folder = './content/qM-AI-L/email-subject/model-tuning/logs'  # Adjust to your repository folder\n",
        "\n",
        "# Copy the folder\n",
        "shutil.copytree(logs_folder, logs_git_folder)"
      ],
      "metadata": {
        "id": "62ihDbDA_Dgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "results_folder = '/content/results'  # Adjust to your source folder\n",
        "results_git_folder = './content/qM-AI-L/email-subject/model-tuning/results'  # Adjust to your repository folder\n",
        "\n",
        "# Copy the folder\n",
        "shutil.copytree(results_folder, results_git_folder)"
      ],
      "metadata": {
        "id": "KA6-ZTV1AoJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to the cloned repository\n",
        "os.chdir('./content/qM-AI-L')  # Adjust to your repository folder\n",
        "\n",
        "# Configure Git (only needed once)\n",
        "!git config --global user.email \"mlreddy3082@gmail.com\"\n",
        "!git config --global user.name \"Mlr9459\"\n",
        "\n",
        "# Add, commit, and push changes\n",
        "!git add /content/qM-AI-L/email-subject/\n",
        "!git commit -m \"Add training outputs: model checkpoints, tokenizer, and generation configuration\"\n",
        "!git push origin master"
      ],
      "metadata": {
        "id": "qdXDrq7SCPm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the Model Quantitatively (with ROUGE Metric)\n",
        "The ROUGE metric) helps quantify the validity of summarizations produced by models. It compares summarizations to a \"baseline\" summary which is usually created by a human. While not perfect, it does indicate the overall increase in summarization effectiveness that we have accomplished by fine-tuning.**"
      ],
      "metadata": {
        "id": "BhjyD6Wyzxhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_datasets, tokenized_test_datasets"
      ],
      "metadata": {
        "id": "lKqFN4OX1yaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_datasets[0:10]['cleaned_emails']"
      ],
      "metadata": {
        "id": "m1tWpGFv2EcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emails_for_score = tokenized_test_datasets[0:10]['cleaned_emails']\n",
        "human_annoted_subjects = tokenized_test_datasets[0:10]['subject']\n"
      ],
      "metadata": {
        "id": "pqox7th015Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "facebook_bart_tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
        "facebook_bart_base_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')"
      ],
      "metadata": {
        "id": "W1-e0Dgg4s71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_directory = '/content/results/FB_Bart_model'\n",
        "facebook_bart_tokenizer_trained = BartTokenizer.from_pretrained(model_directory)\n",
        "facebook_bart_base_model_trained = BartForConditionalGeneration.from_pretrained(model_directory)"
      ],
      "metadata": {
        "id": "gXnio11hELAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "facebook_bart_subjects = []\n",
        "trained_facebook_bart_subjects = []\n",
        "\n",
        "for _, email in enumerate(emails_for_score):\n",
        "    prompt = f\"\"\"\n",
        "    Generate the subject line for the following email.\n",
        "\n",
        "    Email:\n",
        "    {email}\n",
        "\n",
        "    Subject:\n",
        "    \"\"\"\n",
        "    # Tokenize the input and move to the same device as the model\n",
        "    inputs = facebook_bart_tokenizer(prompt, return_tensors='pt').to(facebook_bart_base_model.device)\n",
        "\n",
        "    # Add the decoder_start_token_id to the generation configuration\n",
        "    generation_config = GenerationConfig(\n",
        "    max_new_tokens=200,\n",
        "    decoder_start_token_id=facebook_bart_tokenizer.bos_token_id  # Add this line\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    # Generate subject using the original model\n",
        "    original_model_outputs = facebook_bart_base_model.generate(**inputs, generation_config=generation_config)\n",
        "    original_model_subject_output = facebook_bart_tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "    facebook_bart_subjects.append(original_model_subject_output)\n",
        "\n",
        "    trained_facebook_bart_outputs = facebook_bart_base_model_trained.generate(**inputs, generation_config=generation_config)\n",
        "    instruct_model_text_output = facebook_bart_tokenizer_trained.decode(trained_facebook_bart_outputs[0], skip_special_tokens=True)\n",
        "    trained_facebook_bart_subjects.append(instruct_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(human_annoted_subjects, facebook_bart_subjects, trained_facebook_bart_subjects))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns = ['human_annoted_subjects', 'facebook_bart_subjects', 'trained_facebook_bart_subjects'])\n",
        "df"
      ],
      "metadata": {
        "id": "Yy9Bz2uV1c-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n",
        "from rouge_score import rouge_scorer\n",
        "from rouge_score.scoring import BootstrapAggregator\n",
        "\n"
      ],
      "metadata": {
        "id": "eMHM2fPzsAxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate ROUGE scores for each pair of prediction and reference\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "aggregator = BootstrapAggregator() # Initialize aggregator\n",
        "\n",
        "for prediction, reference in zip(facebook_bart_subjects, human_annoted_subjects[0:len(facebook_bart_subjects)]):\n",
        "    score = scorer.score(reference, prediction)\n",
        "    aggregator.add_scores(score) # Add scores to aggregator\n",
        "\n",
        "# Aggregate the scores\n",
        "original_model_results = aggregator.aggregate()\n",
        "\n",
        "print(original_model_results)"
      ],
      "metadata": {
        "id": "eAzGwd9qrS0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_model_results = rouge.compute(\n",
        "    predictions=facebook_bart_subjects,\n",
        "    references=human_annoted_subjects[0:len(facebook_bart_subjects)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=trained_facebook_bart_subjects,\n",
        "    references=human_annoted_subjects[0:len(trained_facebook_bart_subjects)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('INSTRUCT MODEL:')\n",
        "print(instruct_model_results)"
      ],
      "metadata": {
        "id": "4N2v_M6SJUBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model locally\n",
        "model_save_path = \"./facebook_bart_base\"\n",
        "facebook_bart_base_model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "\n",
        "print(f\"Model and tokenizer saved to {model_save_path}\")"
      ],
      "metadata": {
        "id": "WCpjkEa9rWxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate subject lines for the test set\n",
        "def generate_subject_line(email_body):\n",
        "    inputs = tokenizer(\"generate subject line: \" + email_body, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "    # Move inputs to the GPU if available\n",
        "    inputs = {k: v.to(facebook_bart_base_model.device) for k, v in inputs.items()}\n",
        "    # Access input_ids from the dictionary\n",
        "    outputs = facebook_bart_base_model.generate(inputs['input_ids'], max_length=128, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "test_df['generated_subject_line'] = test_df['body'].apply(generate_subject_line)"
      ],
      "metadata": {
        "id": "HM0tVaa5i8n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df"
      ],
      "metadata": {
        "id": "n5PT2dfT2OEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "from nltk.translate.meteor_score import meteor_score"
      ],
      "metadata": {
        "id": "T6KRD7Oo2Muk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "\n",
        "# Evaluate the generated subject lines\n",
        "def evaluate_metrics(true_subject, generated_subject):\n",
        "    # Tokenize the input for METEOR\n",
        "    true_subject_tokens = nltk.word_tokenize(true_subject)\n",
        "    generated_subject_tokens = nltk.word_tokenize(generated_subject)\n",
        "\n",
        "    # ROUGE-L\n",
        "    rougeL = scorer.score(true_subject, generated_subject)['rougeL'].fmeasure\n",
        "\n",
        "    # METEOR\n",
        "    meteor = meteor_score([true_subject_tokens], generated_subject_tokens) # Pass tokenized input\n",
        "\n",
        "    # SACREBLEU - Use import sacrebleu directly\n",
        "    sacre_bleu_score = sacrebleu.corpus_bleu([generated_subject], [[true_subject]]).score\n",
        "\n",
        "    return rougeL, meteor, sacre_bleu_score # Return the score, not the module\n",
        "\n",
        "test_df['rougeL'], test_df['meteor'], test_df['sacrebleu'] = zip(*test_df.apply(\n",
        "    lambda row: evaluate_metrics(row['subject'], row['generated_subject_line']), axis=1))"
      ],
      "metadata": {
        "id": "g5swqKxsfrQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate average scores\n",
        "average_rougeL = test_df['rougeL'].mean()\n",
        "average_meteor = test_df['meteor'].mean()\n",
        "average_sacrebleu = test_df['sacrebleu'].mean() # Now you should be able to calculate the mean\n",
        "\n",
        "print(f'Average facebook_bart_base_model ROUGE-L: {average_rougeL:.4f}')\n",
        "print(f'Average facebook_bart_base_model METEOR: {average_meteor:.4f}')\n",
        "print(f'Average facebook_bart_base_model SACREBLEU: {average_sacrebleu:.4f}')"
      ],
      "metadata": {
        "id": "TcCHg_OU29ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Replace 'directory_name' with the name of your directory\n",
        "shutil.make_archive('/content/facebook_bart_base_model', 'zip', 'facebook_bart_base_model')"
      ],
      "metadata": {
        "id": "Lzt-7qci4Niq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# files.download('facebook_bart_base_model.zip')"
      ],
      "metadata": {
        "id": "pY7QEHAw4bsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **t5-base**"
      ],
      "metadata": {
        "id": "Dx76AiWz5CEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokenizer and model t5-base\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n"
      ],
      "metadata": {
        "id": "qlr5gRXjGGw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset.from_pandas(pivot_df)"
      ],
      "metadata": {
        "id": "zd2OYT0bXhMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "_dQE0OytX7D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_function(pivot_df):\n",
        "    inputs = pivot_df['cleaned_emails']\n",
        "    targets = pivot_df['subject']\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "j2MxJu3bGWxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "id": "K0aZkei_T0V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Replace 'directory_name' with the name of your directory\n",
        "# shutil.make_archive('/content/fine_tuned_t5', 'zip', 'fine_tuned_t5')\n",
        "\n",
        "# Download the zipped directory\n"
      ],
      "metadata": {
        "id": "uGocveE7q2_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shutil.make_archive('/content/results', 'zip', 'FB_Bart_model_results')\n",
        "shutil.make_archive('/content/logs', 'zip', 'FB_Bart_model_logs')\n"
      ],
      "metadata": {
        "id": "eT_AJo9PEjxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# files.download('fine_tuned_t5.zip')"
      ],
      "metadata": {
        "id": "ZO2hx72nrVx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "BDnsANB7Iz3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_dataset['attention_mask']\n",
        "# tokenized_dataset['input_ids']"
      ],
      "metadata": {
        "id": "6Mo5GbMDH8Zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into train and test sets\n",
        "train_test_split = tokenized_dataset.train_test_split(test_size=0.2)\n",
        "train_dataset = train_test_split['train']\n",
        "eval_dataset = train_test_split['test']\n",
        "\n",
        "# Data collator for padding\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
      ],
      "metadata": {
        "id": "0KMgDPdkU753"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=3,\n",
        "    # predict_with_generate=True,\n",
        "    logging_dir=\"./logs\",\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "19V3PPTgVBQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('fine_tuned_t5')\n",
        "tokenizer.save_pretrained('fine_tuned_t5')"
      ],
      "metadata": {
        "id": "uvY_nd2_bise"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available. \\nUsing GPU\")\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"GPU is not available. \\nUsing CPU\")\n",
        "    device = torch.device('cpu')"
      ],
      "metadata": {
        "id": "9shJp4Spb3Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(text):\n",
        "    inputs = tokenizer.encode_plus(text, return_tensors='pt', max_length=512, truncation=True)\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    summary_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=150, num_beams=2, repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\n",
        "\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n"
      ],
      "metadata": {
        "id": "ohAtd3YQbn1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example inference\n",
        "example_text = pivot_df['body'][3]\n",
        "print(\"Email Body:\", example_text)\n",
        "print(\"\\n\\n Generated Subject:\", generate_summary(example_text))\n",
        "print(\"\\n\\n Actual Subject:\", pivot_df['subject'][3])"
      ],
      "metadata": {
        "id": "V_MzfdBqcJZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "import sacrebleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "\n",
        "# Evaluate the generated subject lines\n",
        "def evaluate_metrics(true_subject, generated_subject):\n",
        "    # ROUGE-L\n",
        "    rougeL = scorer.score(true_subject, generated_subject)['rougeL'].fmeasure\n",
        "\n",
        "    # METEOR\n",
        "    meteor = meteor_score([true_subject], generated_subject)\n",
        "\n",
        "    # SACREBLEU\n",
        "    sacre_bleu = sacrebleu.corpus_bleu([generated_subject], [[true_subject]]).score\n",
        "\n",
        "    return rougeL, meteor, sacre_bleu\n",
        "\n",
        "test_df['rougeL'], test_df['meteor'], test_df['sacrebleu'] = zip(*test_df.apply(\n",
        "    lambda row: evaluate_metrics(row['subject_line'], row['generated_subject_line']), axis=1))\n",
        "\n",
        "# Calculate average scores\n",
        "average_rougeL = test_df['rougeL'].mean()\n",
        "average_meteor = test_df['meteor'].mean()\n",
        "average_sacrebleu = test_df['sacrebleu'].mean()\n",
        "\n",
        "print(f'Average ROUGE-L: {average_rougeL:.4f}')\n",
        "print(f'Average METEOR: {average_meteor:.4f}')\n",
        "print(f'Average SACREBLEU: {average_sacrebleu:.4f}')"
      ],
      "metadata": {
        "id": "UvJijyqfN5k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_sentences(text):\n",
        "  \"\"\"\n",
        "  This function extracts sentences from text without introducing control characters.\n",
        "\n",
        "  Args:\n",
        "      text (str): The text to be processed.\n",
        "\n",
        "  Returns:\n",
        "      list: A list of extracted sentences.\n",
        "  \"\"\"\n",
        "  # Split text based on sentence boundaries (adjust pattern as needed)\n",
        "  sentences = re.split(r\"(?<!\\w)\\.(?!\\w)\", text)\n",
        "\n",
        "  # Remove empty elements and leading/trailing whitespace\n",
        "  sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "\n",
        "  return sentences\n",
        "\n",
        "# Example usage\n",
        "text = \"This is an example text. It contains multiple sentences.  \\nThere are also newlines and extra spaces.\"\n",
        "extracted_sentences = extract_sentences(pivot_df['body'][0])\n",
        "print(extracted_sentences)\n"
      ],
      "metadata": {
        "id": "R_UuPM3bLEEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df['body'][0]"
      ],
      "metadata": {
        "id": "gW-fbCCleshA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "summary = summarizer(pivot_df['body'][0], max_length=15, min_length=2, do_sample=False)\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "id": "8hmYeCH2flFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df['body'][0]"
      ],
      "metadata": {
        "id": "QSg88TJak4t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Chirayu/subject-generator-t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Chirayu/subject-generator-t5-base\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "def get_subject(content, num_beams=5,max_length=512, repetition_penalty=2.5, length_penalty=1, early_stopping=True,top_p=.95, top_k=50, num_return_sequences=3):\n",
        "\n",
        "  text =  \"title: \" + content + \" </s>\"\n",
        "\n",
        "  input_ids = tokenizer.encode(\n",
        "    text, return_tensors=\"pt\", add_special_tokens=True\n",
        "  )\n",
        "\n",
        "  input_ids = input_ids.to(device)\n",
        "  generated_ids = model.generate(\n",
        "      input_ids=input_ids,\n",
        "\n",
        "      num_beams=num_beams,\n",
        "      max_length=max_length,\n",
        "      repetition_penalty=repetition_penalty,\n",
        "      length_penalty=length_penalty,\n",
        "      early_stopping=early_stopping,\n",
        "      top_p=top_p,\n",
        "      top_k=top_k,\n",
        "      num_return_sequences=num_return_sequences,\n",
        "  )\n",
        "  subjects = [tokenizer.decode(generated_id,skip_special_tokens=True,clean_up_tokenization_spaces=True,) for generated_id in generated_ids]\n",
        "  return subjects"
      ],
      "metadata": {
        "id": "eI-YnG00leqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = get_subject(pivot_df['body'][0])\n",
        "x"
      ],
      "metadata": {
        "id": "JhMrjNf3mKOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df['body'][0]"
      ],
      "metadata": {
        "id": "fTcDIbDHq1wu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df['body'][1]"
      ],
      "metadata": {
        "id": "X3-wKWGkmgXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap"
      ],
      "metadata": {
        "id": "XOWH5k3TjZqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_summarizer(email_text):\n",
        "\n",
        "    model_name = \"facebook/bart-large-cnn\"\n",
        "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    inputs = tokenizer.encode(\"summarize: \" + email_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "    summary_ids = model.generate(inputs, max_length=100, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    formatted_summary = \"\\n\".join(textwrap.wrap(summary, width=80))\n",
        "    return formatted_summary\n",
        "\n",
        "summary = text_summarizer(pivot_df['body'][0])\n",
        "summary"
      ],
      "metadata": {
        "id": "Y85goaEkcOYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_sentences(text):\n",
        "    \"\"\"\n",
        "    Extract sentences from the text and remove newline characters.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The text to process.\n",
        "\n",
        "    Returns:\n",
        "    list: A list of sentences without newline characters.\n",
        "    \"\"\"\n",
        "    # Replace newline characters with spaces\n",
        "    text = text.replace('\\n', '')\n",
        "    text = text.replace('!', '.').replace('?', '.').replace(';', '.')\n",
        "\n",
        "    # Tokenize the text into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    return sentences\n",
        "\n",
        "# Example usage\n",
        "sample_text = \"Hello, world!\\nThis is a test.\\nLet's replace, commas, and newlines.\"\n",
        "sentences = extract_sentences(sample_text)\n",
        "for sentence in sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "id": "zGazV9EgQkxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = text_summarizer(pivot_df['body'][0])\n",
        "summary"
      ],
      "metadata": {
        "id": "jezqQnA9Lck-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pivot_df['body'][0]"
      ],
      "metadata": {
        "id": "7ZrJIfk2MFn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_len = [len(pivot_df['dialogue'].split()) for x in samsum['train']]\n",
        "sub_len = [len(pivot_df['summary'].split()) for x in samsum['train']]\n",
        "\n",
        "dialogue_len = [len(x['dialogue'].split()) for x in samsum['train']]\n",
        "summary_len = [len(x['summary'].split()) for x in samsum['train']]\n",
        "\n"
      ],
      "metadata": {
        "id": "_AM_aMgKSjST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "describe_df(pivot_df)"
      ],
      "metadata": {
        "id": "0N8EfTwkRqF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nan_rows"
      ],
      "metadata": {
        "id": "FrRN8if0dyTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.preprocessing import Tokenizer, Lowercaser, PunctuationRemover, StopwordRemover\n",
        "from langchain.stemming import Stemmer\n",
        "from langchain import Pipeline"
      ],
      "metadata": {
        "id": "oKUEMAee8aSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(email_docs)*3"
      ],
      "metadata": {
        "id": "REA79efkSNQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emails_df['content'][0]"
      ],
      "metadata": {
        "id": "pwhf-gfhMfWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emails_df['source'][0]"
      ],
      "metadata": {
        "id": "FLk0SU2hTPr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_file_name(emails_df['source'][0])"
      ],
      "metadata": {
        "id": "XTKL_ao_T6jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8h-_DelG2pb9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e8f82d68e8284be08d995d324f3efe89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc6d02ad8ecc4d758626a60d20f79c08",
              "IPY_MODEL_bb61fb30810c4f42a73e97cff70001c9",
              "IPY_MODEL_754a29233fc54d3a89391b44b66adc6d"
            ],
            "layout": "IPY_MODEL_de75626e391e42a184ded4466d8597fe"
          }
        },
        "bc6d02ad8ecc4d758626a60d20f79c08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2497f6440b8543e4a2284e5740258fde",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8c7448d07c1c45458b50a67689f45e0d",
            "value": "Map:â€‡100%"
          }
        },
        "bb61fb30810c4f42a73e97cff70001c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a04476077a764522b809fa7e8b88bfc4",
            "max": 4619,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5eff43129ecd4ac19abcf369a067b3c9",
            "value": 4619
          }
        },
        "754a29233fc54d3a89391b44b66adc6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8072e26dd0bc4300a423294828a66302",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c022c4595f564d689813994ca597c4ad",
            "value": "â€‡4619/4619â€‡[00:04&lt;00:00,â€‡1111.82â€‡examples/s]"
          }
        },
        "de75626e391e42a184ded4466d8597fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2497f6440b8543e4a2284e5740258fde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c7448d07c1c45458b50a67689f45e0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a04476077a764522b809fa7e8b88bfc4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5eff43129ecd4ac19abcf369a067b3c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8072e26dd0bc4300a423294828a66302": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c022c4595f564d689813994ca597c4ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3229583154e641398c19dffaeab5dc32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_506bccd386da463a806c5681a76d0a6d",
              "IPY_MODEL_43ad06ac4fc94e9bab870b96da8e4632",
              "IPY_MODEL_fb32db86c0cd457cae5781f7496864f3"
            ],
            "layout": "IPY_MODEL_b8593a0e7e294fc4b991eb1f5ba42b33"
          }
        },
        "506bccd386da463a806c5681a76d0a6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e6f0500d5c74155986d365d109f6aa2",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f494bd084335455bb8bfa6ea6f2ab618",
            "value": "Map:â€‡100%"
          }
        },
        "43ad06ac4fc94e9bab870b96da8e4632": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9085438dd7494c6795f08fe6c9973703",
            "max": 1155,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c93e7021e244a8fbf4c5ceb2de77795",
            "value": 1155
          }
        },
        "fb32db86c0cd457cae5781f7496864f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13ed88532658406e979eff57b6ea4717",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b9efae73e2244af58edd30ff9e722e30",
            "value": "â€‡1155/1155â€‡[00:00&lt;00:00,â€‡1208.07â€‡examples/s]"
          }
        },
        "b8593a0e7e294fc4b991eb1f5ba42b33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e6f0500d5c74155986d365d109f6aa2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f494bd084335455bb8bfa6ea6f2ab618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9085438dd7494c6795f08fe6c9973703": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c93e7021e244a8fbf4c5ceb2de77795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13ed88532658406e979eff57b6ea4717": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9efae73e2244af58edd30ff9e722e30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}